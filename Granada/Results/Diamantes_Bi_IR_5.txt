[1] "DATASET NAME: Diamantes_Bi_IR_5"
[1] "TRAIN INSTANCES: 460"
[1] "TEST INSTANCES: 131"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 2.24719285964966"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

460 samples
947 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 368, 368, 368, 368, 368 
Resampling results:

  ROC  Sens       Spec
  1    0.9888889  1   

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     19.1      0.0
  positive      0.2     80.7
                            
 Accuracy (average) : 0.9978

[1] "TEST accuracy: 0.997826086956522"
[1] "TEST +precision: 0.997311827956989"
[1] "TEST -precision: 1"
[1] "TEST specifity: 0.98876404494382"
[1] "TEST sensitivity: 1"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        0        0
            positive        6      125
[1] "TEST accuracy: 0.954198473282443"
[1] "TEST +precision: 0.954198473282443"
[1] "TEST -precision: NaN"
[1] "TEST specifity: 0"
[1] "TEST sensitivity: 1"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 1.36946671406428"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

460 samples
947 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 368, 368, 368, 368, 368 
Resampling results across tuning parameters:

  C      M  ROC        Sens        Spec     
  0.010  1  0.5647357  0.08888889  0.9972973
  0.010  2  0.5429640  0.06666667  1.0000000
  0.010  3  0.5429640  0.06666667  1.0000000
  0.255  1  0.7168986  0.26732026  0.9972973
  0.255  2  0.7171238  0.26732026  0.9972973
  0.255  3  0.7125119  0.26732026  0.9946306
  0.500  1  0.7418986  0.28954248  0.9972973
  0.500  2  0.7421238  0.28954248  0.9972973
  0.500  3  0.7174669  0.26732026  0.9946306

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.5 and M = 2.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative      5.7      0.2
  positive     13.7     80.4
                            
 Accuracy (average) : 0.8609

[1] "TEST accuracy: 0.860869565217391"
[1] "TEST +precision: 0.854503464203233"
[1] "TEST -precision: 0.962962962962963"
[1] "TEST specifity: 0.292134831460674"
[1] "TEST sensitivity: 0.997304582210243"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        0        0
            positive        6      125
[1] "TEST accuracy: 0.954198473282443"
[1] "TEST +precision: 0.954198473282443"
[1] "TEST -precision: NaN"
[1] "TEST specifity: 0"
[1] "TEST sensitivity: 1"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 2.42268261909485"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

460 samples
947 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 368, 368, 368, 369, 367 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens       Spec     
  0.3  1          0.6               0.50        50      0.8164242  0.3143791  1.0000000
  0.3  1          0.6               0.50       100      0.8442460  0.4941176  0.9918919
  0.3  1          0.6               0.50       150      0.8372167  0.4718954  0.9864865
  0.3  1          0.6               0.75        50      0.8442351  0.4705882  0.9973333
  0.3  1          0.6               0.75       100      0.8628522  0.5836601  0.9892252
  0.3  1          0.6               0.75       150      0.8699857  0.6058824  0.9892252
  0.3  1          0.6               1.00        50      0.8495626  0.4372549  0.9946306
  0.3  1          0.6               1.00       100      0.8856083  0.4941176  0.9946306
  0.3  1          0.6               1.00       150      0.8864623  0.5607843  0.9892252
  0.3  1          0.8               0.50        50      0.7998510  0.3581699  0.9891892
  0.3  1          0.8               0.50       100      0.8384281  0.4830065  0.9891892
  0.3  1          0.8               0.50       150      0.8461354  0.5385621  0.9810811
  0.3  1          0.8               0.75        50      0.8229366  0.4718954  0.9946306
  0.3  1          0.8               0.75       100      0.8631562  0.5274510  0.9919279
  0.3  1          0.8               0.75       150      0.8647479  0.5385621  0.9892252
  0.3  1          0.8               1.00        50      0.8400064  0.4372549  0.9946306
  0.3  1          0.8               1.00       100      0.8839037  0.4941176  0.9919279
  0.3  1          0.8               1.00       150      0.8912976  0.5718954  0.9892252
  0.3  2          0.6               0.50        50      0.8452886  0.4267974  0.9972973
  0.3  2          0.6               0.50       100      0.8601770  0.4941176  0.9892252
  0.3  2          0.6               0.50       150      0.8714337  0.4941176  0.9892252
  0.3  2          0.6               0.75        50      0.8931196  0.5385621  0.9919279
  0.3  2          0.6               0.75       100      0.9168069  0.5941176  0.9892252
  0.3  2          0.6               0.75       150      0.9284412  0.5941176  0.9892252
  0.3  2          0.6               1.00        50      0.8921915  0.5385621  0.9946306
  0.3  2          0.6               1.00       100      0.9044641  0.5941176  0.9892252
  0.3  2          0.6               1.00       150      0.9213283  0.5941176  0.9918919
  0.3  2          0.8               0.50        50      0.8401361  0.5052288  0.9946306
  0.3  2          0.8               0.50       100      0.8755409  0.5385621  0.9945946
  0.3  2          0.8               0.50       150      0.8892464  0.5385621  0.9945946
  0.3  2          0.8               0.75        50      0.8687860  0.5385621  0.9919279
  0.3  2          0.8               0.75       100      0.9129721  0.5496732  0.9945946
  0.3  2          0.8               0.75       150      0.9212453  0.5385621  0.9918919
  0.3  2          0.8               1.00        50      0.8993310  0.5496732  0.9946306
  0.3  2          0.8               1.00       100      0.9191690  0.5941176  0.9919279
  0.3  2          0.8               1.00       150      0.9122378  0.5941176  0.9945946
  0.3  3          0.6               0.50        50      0.8812804  0.5274510  0.9919279
  0.3  3          0.6               0.50       100      0.9005439  0.5274510  0.9918919
  0.3  3          0.6               0.50       150      0.8966970  0.5385621  0.9945946
  0.3  3          0.6               0.75        50      0.8973002  0.5947712  0.9918919
  0.3  3          0.6               0.75       100      0.9277847  0.5830065  0.9945946
  0.3  3          0.6               0.75       150      0.9300546  0.5947712  0.9945946
  0.3  3          0.6               1.00        50      0.9004568  0.5836601  0.9945946
  0.3  3          0.6               1.00       100      0.9297058  0.6058824  0.9918919
  0.3  3          0.6               1.00       150      0.9373967  0.5947712  0.9945946
  0.3  3          0.8               0.50        50      0.8826237  0.5385621  0.9918919
  0.3  3          0.8               0.50       100      0.9016007  0.5496732  0.9918919
  0.3  3          0.8               0.50       150      0.9127465  0.5732026  0.9918919
  0.3  3          0.8               0.75        50      0.8887255  0.5830065  0.9946306
  0.3  3          0.8               0.75       100      0.9180502  0.5718954  0.9865225
  0.3  3          0.8               0.75       150      0.9183832  0.5718954  0.9891892
  0.3  3          0.8               1.00        50      0.9182854  0.6058824  0.9973333
  0.3  3          0.8               1.00       100      0.9365820  0.6058824  0.9945946
  0.3  3          0.8               1.00       150      0.9372881  0.6058824  0.9945946
  0.4  1          0.6               0.50        50      0.8204488  0.4150327  0.9892252
  0.4  1          0.6               0.50       100      0.8226413  0.4607843  0.9892252
  0.4  1          0.6               0.50       150      0.8363127  0.4830065  0.9838198
  0.4  1          0.6               0.75        50      0.8338378  0.4718954  0.9946306
  0.4  1          0.6               0.75       100      0.8588066  0.5607843  0.9919279
  0.4  1          0.6               0.75       150      0.8582078  0.5607843  0.9891892
  0.4  1          0.6               1.00        50      0.8419953  0.4941176  0.9946306
  0.4  1          0.6               1.00       100      0.8757953  0.5385621  0.9946306
  0.4  1          0.6               1.00       150      0.8800434  0.5607843  0.9892252
  0.4  1          0.8               0.50        50      0.8175107  0.3928105  0.9892252
  0.4  1          0.8               0.50       100      0.8317301  0.4941176  0.9892252
  0.4  1          0.8               0.50       150      0.8483925  0.5163399  0.9838198
  0.4  1          0.8               0.75        50      0.8424212  0.4830065  0.9973333
  0.4  1          0.8               0.75       100      0.8523746  0.5607843  0.9919279
  0.4  1          0.8               0.75       150      0.8664211  0.5718954  0.9864865
  0.4  1          0.8               1.00        50      0.8494546  0.4941176  0.9946306
  0.4  1          0.8               1.00       100      0.8892924  0.5718954  0.9892252
  0.4  1          0.8               1.00       150      0.8963088  0.5941176  0.9918919
  0.4  2          0.6               0.50        50      0.8588279  0.4718954  0.9945946
  0.4  2          0.6               0.50       100      0.8696502  0.5385621  0.9864865
  0.4  2          0.6               0.50       150      0.8761687  0.5385621  0.9864865
  0.4  2          0.6               0.75        50      0.8715341  0.5274510  0.9918919
  0.4  2          0.6               0.75       100      0.9175377  0.5385621  0.9945946
  0.4  2          0.6               0.75       150      0.9252730  0.5385621  0.9945946
  0.4  2          0.6               1.00        50      0.8803741  0.5385621  0.9946306
  0.4  2          0.6               1.00       100      0.8902671  0.5496732  0.9919279
  0.4  2          0.6               1.00       150      0.9213496  0.5496732  0.9919279
  0.4  2          0.8               0.50        50      0.8552268  0.4830065  0.9892252
  0.4  2          0.8               0.50       100      0.8849147  0.4830065  0.9892252
  0.4  2          0.8               0.50       150      0.8899145  0.5052288  0.9892252
  0.4  2          0.8               0.75        50      0.8808957  0.5496732  0.9919279
  0.4  2          0.8               0.75       100      0.9187897  0.5725490  0.9918919
  0.4  2          0.8               0.75       150      0.9210360  0.5496732  0.9918919
  0.4  2          0.8               1.00        50      0.9058597  0.5718954  0.9892252
  0.4  2          0.8               1.00       100      0.9169952  0.5941176  0.9919279
  0.4  2          0.8               1.00       150      0.9345750  0.5941176  0.9945946
  0.4  3          0.6               0.50        50      0.8508345  0.4941176  0.9918919
  0.4  3          0.6               0.50       100      0.8720491  0.5392157  0.9918919
  0.4  3          0.6               0.50       150      0.8816034  0.5392157  0.9918919
  0.4  3          0.6               0.75        50      0.9006643  0.5385621  0.9918919
  0.4  3          0.6               0.75       100      0.9164851  0.5385621  0.9945946
  0.4  3          0.6               0.75       150      0.9211201  0.5385621  0.9945946
  0.4  3          0.6               1.00        50      0.9125702  0.6058824  0.9918919
  0.4  3          0.6               1.00       100      0.9397399  0.5947712  0.9919279
  0.4  3          0.6               1.00       150      0.9373326  0.5947712  0.9919279
  0.4  3          0.8               0.50        50      0.8655488  0.5052288  0.9891892
  0.4  3          0.8               0.50       100      0.8923559  0.5163399  0.9865225
  0.4  3          0.8               0.50       150      0.8951033  0.5163399  0.9892252
  0.4  3          0.8               0.75        50      0.9177602  0.5503268  0.9918919
  0.4  3          0.8               0.75       100      0.9213962  0.5503268  0.9945946
  0.4  3          0.8               0.75       150      0.9274119  0.5503268  0.9918919
  0.4  3          0.8               1.00        50      0.9148275  0.5607843  0.9946306
  0.4  3          0.8               1.00       100      0.9281437  0.5607843  0.9972973
  0.4  3          0.8               1.00       150      0.9291948  0.5607843  0.9972973

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 100, max_depth = 3, eta = 0.4, gamma = 0, colsample_bytree = 0.6, min_child_weight = 1 and subsample = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     11.5      0.7
  positive      7.8     80.0
                            
 Accuracy (average) : 0.9152

[1] "TEST accuracy: 0.915217391304348"
[1] "TEST +precision: 0.910891089108911"
[1] "TEST -precision: 0.946428571428571"
[1] "TEST specifity: 0.595505617977528"
[1] "TEST sensitivity: 0.991913746630728"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        0        1
            positive        6      124
[1] "TEST accuracy: 0.946564885496183"
[1] "TEST +precision: 0.953846153846154"
[1] "TEST -precision: 0"
[1] "TEST specifity: 0"
[1] "TEST sensitivity: 0.992"

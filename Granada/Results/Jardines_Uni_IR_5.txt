[1] "DATASET NAME: Jardines_Uni_IR_5"
[1] "TRAIN INSTANCES: 796"
[1] "TEST INSTANCES: 223"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 2.72869396209717"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

796 samples
736 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 638, 636, 637, 636, 637 
Resampling results:

  ROC  Sens  Spec
  1    1     1   

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative       18        0
  positive        0       82
                       
 Accuracy (average) : 1

[1] "TEST accuracy: 1"
[1] "TEST +precision: 1"
[1] "TEST -precision: 1"
[1] "TEST specifity: 1"
[1] "TEST sensitivity: 1"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        1        0
            positive        3      219
[1] "TEST accuracy: 0.986547085201794"
[1] "TEST +precision: 0.986486486486487"
[1] "TEST -precision: 1"
[1] "TEST specifity: 0.25"
[1] "TEST sensitivity: 1"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 1.25841773351034"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

796 samples
736 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 636, 637, 637, 637, 637 
Resampling results across tuning parameters:

  C      M  ROC        Sens  Spec     
  0.010  1  0.9915852  1     0.9770170
  0.010  2  0.9927082  1     0.9770170
  0.010  3  0.9943417  1     0.9785437
  0.255  1  0.9914535  1     0.9785437
  0.255  2  0.9924449  1     0.9785437
  0.255  3  0.9958875  1     0.9892777
  0.500  1  0.9914535  1     0.9785437
  0.500  2  0.9924449  1     0.9785437
  0.500  3  0.9958875  1     0.9892777

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.255 and M = 3.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     18.0      0.9
  positive      0.0     81.2
                            
 Accuracy (average) : 0.9912

[1] "TEST accuracy: 0.991206030150754"
[1] "TEST +precision: 1"
[1] "TEST -precision: 0.953333333333333"
[1] "TEST specifity: 1"
[1] "TEST sensitivity: 0.989280245022971"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        1        3
            positive        3      216
[1] "TEST accuracy: 0.973094170403587"
[1] "TEST +precision: 0.986301369863014"
[1] "TEST -precision: 0.25"
[1] "TEST specifity: 0.25"
[1] "TEST sensitivity: 0.986301369863014"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 2.96802346309026"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

796 samples
736 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 638, 636, 637, 636, 637 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens       Spec     
  0.3  1          0.6               0.50        50      0.9843567  0.7630542  0.9908162
  0.3  1          0.6               0.50       100      0.9970867  0.9226601  0.9908280
  0.3  1          0.6               0.50       150      0.9976247  0.9655172  0.9892777
  0.3  1          0.6               0.75        50      0.9898708  0.8738916  0.9954198
  0.3  1          0.6               0.75       100      0.9973671  0.9655172  0.9923547
  0.3  1          0.6               0.75       150      0.9987045  0.9655172  0.9938814
  0.3  1          0.6               1.00        50      0.9902388  0.8320197  0.9938814
  0.3  1          0.6               1.00       100      0.9977254  0.9655172  0.9938814
  0.3  1          0.6               1.00       150      0.9981522  0.9655172  0.9938814
  0.3  1          0.8               0.50        50      0.9875527  0.7768473  0.9923547
  0.3  1          0.8               0.50       100      0.9969459  0.9655172  0.9893012
  0.3  1          0.8               0.50       150      0.9972486  0.9655172  0.9908280
  0.3  1          0.8               0.75        50      0.9851755  0.8672414  0.9923547
  0.3  1          0.8               0.75       100      0.9966071  0.9655172  0.9923547
  0.3  1          0.8               0.75       150      0.9982604  0.9655172  0.9938814
  0.3  1          0.8               1.00        50      0.9851841  0.8044335  0.9923547
  0.3  1          0.8               1.00       100      0.9965626  0.9655172  0.9923547
  0.3  1          0.8               1.00       150      0.9983703  0.9655172  0.9923547
  0.3  2          0.6               0.50        50      0.9982383  0.9655172  0.9984733
  0.3  2          0.6               0.50       100      0.9991357  1.0000000  0.9923429
  0.3  2          0.6               0.50       150      0.9991489  1.0000000  0.9908280
  0.3  2          0.6               0.75        50      0.9985339  0.9655172  0.9969348
  0.3  2          0.6               0.75       100      0.9980308  1.0000000  0.9923429
  0.3  2          0.6               0.75       150      0.9986872  1.0000000  0.9923429
  0.3  2          0.6               1.00        50      0.9996307  0.9655172  0.9938696
  0.3  2          0.6               1.00       100      0.9993457  1.0000000  0.9892777
  0.3  2          0.6               1.00       150      0.9996183  1.0000000  0.9908045
  0.3  2          0.8               0.50        50      0.9978021  0.9655172  0.9954198
  0.3  2          0.8               0.50       100      0.9985454  1.0000000  0.9923547
  0.3  2          0.8               0.50       150      0.9988631  1.0000000  0.9923547
  0.3  2          0.8               0.75        50      0.9989734  0.9655172  0.9954198
  0.3  2          0.8               0.75       100      0.9992912  1.0000000  0.9938814
  0.3  2          0.8               0.75       150      0.9998909  1.0000000  0.9938814
  0.3  2          0.8               1.00        50      0.9985917  0.9655172  0.9953964
  0.3  2          0.8               1.00       100      0.9988550  1.0000000  0.9954081
  0.3  2          0.8               1.00       150      0.9987992  1.0000000  0.9953964
  0.3  3          0.6               0.50        50      0.9997291  1.0000000  0.9938696
  0.3  3          0.6               0.50       100      0.9997806  1.0000000  0.9908162
  0.3  3          0.6               0.50       150      0.9999455  1.0000000  0.9908162
  0.3  3          0.6               0.75        50      0.9991809  1.0000000  0.9969348
  0.3  3          0.6               0.75       100      0.9990705  1.0000000  0.9969348
  0.3  3          0.6               0.75       150      0.9992341  1.0000000  0.9954081
  0.3  3          0.6               1.00        50      1.0000000  1.0000000  0.9953964
  0.3  3          0.6               1.00       100      1.0000000  1.0000000  0.9938579
  0.3  3          0.6               1.00       150      1.0000000  1.0000000  0.9969348
  0.3  3          0.8               0.50        50      0.9989189  0.9655172  0.9938696
  0.3  3          0.8               0.50       100      0.9990825  1.0000000  0.9938696
  0.3  3          0.8               0.50       150      0.9993551  1.0000000  0.9938696
  0.3  3          0.8               0.75        50      0.9997368  0.9655172  0.9923547
  0.3  3          0.8               0.75       100      0.9997368  1.0000000  0.9892895
  0.3  3          0.8               0.75       150      1.0000000  1.0000000  0.9892895
  0.3  3          0.8               1.00        50      0.9994547  1.0000000  0.9938696
  0.3  3          0.8               1.00       100      0.9997819  1.0000000  0.9938696
  0.3  3          0.8               1.00       150      0.9999455  1.0000000  0.9969466
  0.4  1          0.6               0.50        50      0.9840005  0.9091133  0.9877510
  0.4  1          0.6               0.50       100      0.9959469  0.9655172  0.9892895
  0.4  1          0.6               0.50       150      0.9978547  0.9655172  0.9908162
  0.4  1          0.6               0.75        50      0.9936844  0.9019704  0.9908162
  0.4  1          0.6               0.75       100      0.9973408  0.9655172  0.9938814
  0.4  1          0.6               0.75       150      0.9971632  1.0000000  0.9938814
  0.4  1          0.6               1.00        50      0.9927635  0.9226601  0.9938814
  0.4  1          0.6               1.00       100      0.9978438  0.9655172  0.9923429
  0.4  1          0.6               1.00       150      0.9979324  1.0000000  0.9938814
  0.4  1          0.8               0.50        50      0.9951627  0.8945813  0.9908045
  0.4  1          0.8               0.50       100      0.9984908  0.9655172  0.9892895
  0.4  1          0.8               0.50       150      0.9984996  1.0000000  0.9877510
  0.4  1          0.8               0.75        50      0.9925409  0.9440887  0.9877510
  0.4  1          0.8               0.75       100      0.9976257  0.9655172  0.9923429
  0.4  1          0.8               0.75       150      0.9976589  1.0000000  0.9923429
  0.4  1          0.8               1.00        50      0.9924220  0.9226601  0.9938814
  0.4  1          0.8               1.00       100      0.9982621  0.9655172  0.9923547
  0.4  1          0.8               1.00       150      0.9981522  1.0000000  0.9938814
  0.4  2          0.6               0.50        50      0.9976287  0.9655172  0.9938696
  0.4  2          0.6               0.50       100      0.9986983  1.0000000  0.9938696
  0.4  2          0.6               0.50       150      0.9986983  1.0000000  0.9908045
  0.4  2          0.6               0.75        50      0.9990718  0.9655172  0.9953964
  0.4  2          0.6               0.75       100      0.9990718  1.0000000  0.9938696
  0.4  2          0.6               0.75       150      0.9990173  1.0000000  0.9923312
  0.4  2          0.6               1.00        50      0.9997368  1.0000000  0.9954081
  0.4  2          0.6               1.00       100      1.0000000  1.0000000  0.9938696
  0.4  2          0.6               1.00       150      1.0000000  1.0000000  0.9938696
  0.4  2          0.8               0.50        50      0.9975125  0.9655172  0.9892660
  0.4  2          0.8               0.50       100      0.9981062  1.0000000  0.9877393
  0.4  2          0.8               0.50       150      0.9977811  1.0000000  0.9877393
  0.4  2          0.8               0.75        50      0.9992912  0.9655172  0.9938696
  0.4  2          0.8               0.75       100      0.9990718  1.0000000  0.9908162
  0.4  2          0.8               0.75       150      0.9997274  1.0000000  0.9908162
  0.4  2          0.8               1.00        50      0.9984282  0.9655172  0.9938696
  0.4  2          0.8               1.00       100      0.9984269  1.0000000  0.9938696
  0.4  2          0.8               1.00       150      0.9985265  1.0000000  0.9938696
  0.4  3          0.6               0.50        50      0.9998909  1.0000000  0.9969348
  0.4  3          0.6               0.50       100      1.0000000  1.0000000  0.9923429
  0.4  3          0.6               0.50       150      1.0000000  1.0000000  0.9954198
  0.4  3          0.6               0.75        50      0.9997274  1.0000000  0.9923429
  0.4  3          0.6               0.75       100      0.9999455  1.0000000  0.9938814
  0.4  3          0.6               0.75       150      1.0000000  1.0000000  0.9938814
  0.4  3          0.6               1.00        50      0.9989640  1.0000000  0.9938814
  0.4  3          0.6               1.00       100      0.9994547  1.0000000  0.9938814
  0.4  3          0.6               1.00       150      0.9997819  1.0000000  0.9938814
  0.4  3          0.8               0.50        50      0.9993645  1.0000000  0.9923429
  0.4  3          0.8               0.50       100      0.9998352  1.0000000  0.9908162
  0.4  3          0.8               0.50       150      0.9998352  1.0000000  0.9908162
  0.4  3          0.8               0.75        50      1.0000000  1.0000000  0.9923429
  0.4  3          0.8               0.75       100      1.0000000  1.0000000  0.9923429
  0.4  3          0.8               0.75       150      1.0000000  1.0000000  0.9938696
  0.4  3          0.8               1.00        50      1.0000000  1.0000000  0.9953964
  0.4  3          0.8               1.00       100      1.0000000  1.0000000  0.9984615
  0.4  3          0.8               1.00       150      1.0000000  1.0000000  0.9969348

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 50, max_depth = 3, eta = 0.3, gamma = 0, colsample_bytree = 0.6, min_child_weight = 1 and subsample = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     18.0      0.4
  positive      0.0     81.7
                            
 Accuracy (average) : 0.9962

[1] "TEST accuracy: 0.996231155778894"
[1] "TEST +precision: 1"
[1] "TEST -precision: 0.979452054794521"
[1] "TEST specifity: 1"
[1] "TEST sensitivity: 0.995405819295559"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        1        0
            positive        3      219
[1] "TEST accuracy: 0.986547085201794"
[1] "TEST +precision: 0.986486486486487"
[1] "TEST -precision: 1"
[1] "TEST specifity: 0.25"
[1] "TEST sensitivity: 1"

[1] "DATASET NAME: Diamantes_Uni_IR_2"
[1] "TRAIN INSTANCES: 569"
[1] "TEST INSTANCES: 131"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 2.71101212501526"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

569 samples
797 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 456, 455, 455, 455, 455 
Resampling results:

  ROC  Sens  Spec     
  1    1     0.9946306

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     34.3      0.4
  positive      0.0     65.4
                            
 Accuracy (average) : 0.9965

[1] "TEST accuracy: 0.996485061511424"
[1] "TEST +precision: 1"
[1] "TEST -precision: 0.989847715736041"
[1] "TEST specifity: 1"
[1] "TEST sensitivity: 0.994652406417112"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        2        0
            positive        7      122
[1] "TEST accuracy: 0.946564885496183"
[1] "TEST +precision: 0.945736434108527"
[1] "TEST -precision: 1"
[1] "TEST specifity: 0.222222222222222"
[1] "TEST sensitivity: 1"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 1.36356411774953"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

569 samples
797 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 455, 455, 455, 456, 455 
Resampling results across tuning parameters:

  C      M  ROC        Sens  Spec     
  0.010  1  0.9846570  1     0.9518919
  0.010  2  0.9850875  1     0.9518919
  0.010  3  0.9812243  1     0.9518919
  0.255  1  0.9838365  1     0.9598919
  0.255  2  0.9851698  1     0.9598919
  0.255  3  0.9851264  1     0.9545586
  0.500  1  0.9838365  1     0.9598919
  0.500  2  0.9851698  1     0.9598919
  0.500  3  0.9851264  1     0.9545586

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.255 and M = 2.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     34.3      2.6
  positive      0.0     63.1
                            
 Accuracy (average) : 0.9736

[1] "TEST accuracy: 0.973637961335677"
[1] "TEST +precision: 1"
[1] "TEST -precision: 0.928571428571429"
[1] "TEST specifity: 1"
[1] "TEST sensitivity: 0.959893048128342"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        1        5
            positive        8      117
[1] "TEST accuracy: 0.900763358778626"
[1] "TEST +precision: 0.936"
[1] "TEST -precision: 0.166666666666667"
[1] "TEST specifity: 0.111111111111111"
[1] "TEST sensitivity: 0.959016393442623"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 2.77830196619034"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

569 samples
797 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 456, 455, 455, 455, 455 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens       Spec     
  0.3  1          0.6               0.50        50      0.9948164  0.9794872  0.9838919
  0.3  1          0.6               0.50       100      0.9963483  1.0000000  0.9838919
  0.3  1          0.6               0.50       150      0.9957339  1.0000000  0.9838919
  0.3  1          0.6               0.75        50      0.9959039  1.0000000  0.9812252
  0.3  1          0.6               0.75       100      0.9986260  1.0000000  0.9838919
  0.3  1          0.6               0.75       150      0.9971199  1.0000000  0.9838919
  0.3  1          0.6               1.00        50      0.9959104  1.0000000  0.9785586
  0.3  1          0.6               1.00       100      0.9968122  1.0000000  0.9811892
  0.3  1          0.6               1.00       150      0.9978120  1.0000000  0.9838919
  0.3  1          0.8               0.50        50      0.9964574  1.0000000  0.9812613
  0.3  1          0.8               0.50       100      0.9976013  1.0000000  0.9812252
  0.3  1          0.8               0.50       150      0.9965775  1.0000000  0.9838919
  0.3  1          0.8               0.75        50      0.9945983  1.0000000  0.9839279
  0.3  1          0.8               0.75       100      0.9972890  1.0000000  0.9838919
  0.3  1          0.8               0.75       150      0.9979432  1.0000000  0.9838919
  0.3  1          0.8               1.00        50      0.9968649  1.0000000  0.9785586
  0.3  1          0.8               1.00       100      0.9972225  1.0000000  0.9838919
  0.3  1          0.8               1.00       150      0.9980855  1.0000000  0.9811892
  0.3  2          0.6               0.50        50      0.9986935  1.0000000  0.9865946
  0.3  2          0.6               0.50       100      0.9977963  1.0000000  0.9812613
  0.3  2          0.6               0.50       150      0.9975967  1.0000000  0.9785946
  0.3  2          0.6               0.75        50      0.9979487  1.0000000  0.9839279
  0.3  2          0.6               0.75       100      0.9979487  1.0000000  0.9812252
  0.3  2          0.6               0.75       150      0.9969915  1.0000000  0.9785586
  0.3  2          0.6               1.00        50      0.9981797  1.0000000  0.9838919
  0.3  2          0.6               1.00       100      0.9965054  1.0000000  0.9812252
  0.3  2          0.6               1.00       150      0.9947240  1.0000000  0.9839279
  0.3  2          0.8               0.50        50      0.9970552  1.0000000  0.9812252
  0.3  2          0.8               0.50       100      0.9951398  1.0000000  0.9758919
  0.3  2          0.8               0.50       150      0.9954188  1.0000000  0.9785946
  0.3  2          0.8               0.75        50      0.9973472  1.0000000  0.9811892
  0.3  2          0.8               0.75       100      0.9990409  1.0000000  0.9838919
  0.3  2          0.8               0.75       150      0.9989744  1.0000000  0.9812252
  0.3  2          0.8               1.00        50      0.9968889  1.0000000  0.9865946
  0.3  2          0.8               1.00       100      0.9954872  1.0000000  0.9839279
  0.3  2          0.8               1.00       150      0.9952821  1.0000000  0.9839279
  0.3  3          0.6               0.50        50      0.9977279  1.0000000  0.9839279
  0.3  3          0.6               0.50       100      0.9970367  1.0000000  0.9812613
  0.3  3          0.6               0.50       150      0.9971033  1.0000000  0.9812613
  0.3  3          0.6               0.75        50      0.9991776  1.0000000  0.9812252
  0.3  3          0.6               0.75       100      0.9984957  1.0000000  0.9812252
  0.3  3          0.6               0.75       150      0.9982222  1.0000000  0.9812252
  0.3  3          0.6               1.00        50      0.9995214  1.0000000  0.9839279
  0.3  3          0.6               1.00       100      0.9980855  1.0000000  0.9839279
  0.3  3          0.6               1.00       150      0.9968547  1.0000000  0.9812252
  0.3  3          0.8               0.50        50      0.9969878  1.0000000  0.9812252
  0.3  3          0.8               0.50       100      0.9970598  1.0000000  0.9812252
  0.3  3          0.8               0.50       150      0.9967863  1.0000000  0.9758919
  0.3  3          0.8               0.75        50      0.9995214  1.0000000  0.9812252
  0.3  3          0.8               0.75       100      0.9972613  1.0000000  0.9812252
  0.3  3          0.8               0.75       150      0.9979469  1.0000000  0.9785586
  0.3  3          0.8               1.00        50      0.9992479  1.0000000  0.9785586
  0.3  3          0.8               1.00       100      0.9974701  1.0000000  0.9812252
  0.3  3          0.8               1.00       150      0.9974017  1.0000000  0.9812252
  0.4  1          0.6               0.50        50      0.9949614  1.0000000  0.9785586
  0.4  1          0.6               0.50       100      0.9962301  1.0000000  0.9811892
  0.4  1          0.6               0.50       150      0.9936170  1.0000000  0.9811892
  0.4  1          0.6               0.75        50      0.9976965  1.0000000  0.9838919
  0.4  1          0.6               0.75       100      0.9985595  1.0000000  0.9838919
  0.4  1          0.6               0.75       150      0.9971892  1.0000000  0.9838919
  0.4  1          0.6               1.00        50      0.9966736  1.0000000  0.9811892
  0.4  1          0.6               1.00       100      0.9977408  1.0000000  0.9838919
  0.4  1          0.6               1.00       150      0.9979487  1.0000000  0.9865946
  0.4  1          0.8               0.50        50      0.9978674  1.0000000  0.9812252
  0.4  1          0.8               0.50       100      0.9986851  1.0000000  0.9838919
  0.4  1          0.8               0.50       150      0.9975893  1.0000000  0.9839279
  0.4  1          0.8               0.75        50      0.9963336  1.0000000  0.9838919
  0.4  1          0.8               0.75       100      0.9979376  1.0000000  0.9838919
  0.4  1          0.8               0.75       150      0.9964371  1.0000000  0.9865946
  0.4  1          0.8               1.00        50      0.9980411  1.0000000  0.9785586
  0.4  1          0.8               1.00       100      0.9991111  1.0000000  0.9838919
  0.4  1          0.8               1.00       150      0.9986325  1.0000000  0.9812252
  0.4  2          0.6               0.50        50      0.9984837  1.0000000  0.9866306
  0.4  2          0.6               0.50       100      0.9980679  1.0000000  0.9839640
  0.4  2          0.6               0.50       150      0.9989023  1.0000000  0.9839640
  0.4  2          0.6               0.75        50      0.9964999  1.0000000  0.9838919
  0.4  2          0.6               0.75       100      0.9956840  1.0000000  0.9839279
  0.4  2          0.6               0.75       150      0.9958198  1.0000000  0.9812613
  0.4  2          0.6               1.00        50      1.0000000  1.0000000  0.9838919
  0.4  2          0.6               1.00       100      0.9987618  1.0000000  0.9839279
  0.4  2          0.6               1.00       150      0.9982185  1.0000000  0.9839279
  0.4  2          0.8               0.50        50      0.9982204  1.0000000  0.9812252
  0.4  2          0.8               0.50       100      0.9956803  1.0000000  0.9758919
  0.4  2          0.8               0.50       150      0.9953375  1.0000000  0.9758559
  0.4  2          0.8               0.75        50      0.9984218  1.0000000  0.9838919
  0.4  2          0.8               0.75       100      0.9955537  1.0000000  0.9785586
  0.4  2          0.8               0.75       150      0.9967143  1.0000000  0.9785586
  0.4  2          0.8               1.00        50      0.9988958  1.0000000  0.9812252
  0.4  2          0.8               1.00       100      0.9976678  1.0000000  0.9839279
  0.4  2          0.8               1.00       150      0.9976734  1.0000000  0.9812252
  0.4  3          0.6               0.50        50      0.9975348  1.0000000  0.9865946
  0.4  3          0.6               0.50       100      0.9969794  1.0000000  0.9839279
  0.4  3          0.6               0.50       150      0.9972529  1.0000000  0.9812252
  0.4  3          0.6               0.75        50      0.9984957  1.0000000  0.9812252
  0.4  3          0.6               0.75       100      0.9980152  1.0000000  0.9812252
  0.4  3          0.6               0.75       150      0.9978785  1.0000000  0.9812252
  0.4  3          0.6               1.00        50      0.9989744  1.0000000  0.9865946
  0.4  3          0.6               1.00       100      0.9984274  1.0000000  0.9839279
  0.4  3          0.6               1.00       150      0.9980855  1.0000000  0.9812613
  0.4  3          0.8               0.50        50      0.9961709  1.0000000  0.9785946
  0.4  3          0.8               0.50       100      0.9967179  1.0000000  0.9759279
  0.4  3          0.8               0.50       150      0.9965784  1.0000000  0.9758919
  0.4  3          0.8               0.75        50      0.9986925  1.0000000  0.9812613
  0.4  3          0.8               0.75       100      0.9985539  1.0000000  0.9812613
  0.4  3          0.8               0.75       150      0.9986205  1.0000000  0.9812613
  0.4  3          0.8               1.00        50      0.9984957  1.0000000  0.9785586
  0.4  3          0.8               1.00       100      0.9982222  1.0000000  0.9812252
  0.4  3          0.8               1.00       150      0.9983590  1.0000000  0.9812252

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 50, max_depth = 2, eta = 0.4, gamma = 0, colsample_bytree = 0.6, min_child_weight = 1 and subsample = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     34.3      1.1
  positive      0.0     64.7
                            
 Accuracy (average) : 0.9895

[1] "TEST accuracy: 0.989455184534271"
[1] "TEST +precision: 1"
[1] "TEST -precision: 0.970149253731343"
[1] "TEST specifity: 1"
[1] "TEST sensitivity: 0.983957219251337"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        2        1
            positive        7      121
[1] "TEST accuracy: 0.938931297709924"
[1] "TEST +precision: 0.9453125"
[1] "TEST -precision: 0.666666666666667"
[1] "TEST specifity: 0.222222222222222"
[1] "TEST sensitivity: 0.991803278688525"

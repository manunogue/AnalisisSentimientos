[1] "DATASET NAME: Jardines_Bi_IR_10"
[1] "TRAIN INSTANCES: 733"
[1] "TEST INSTANCES: 223"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 3.50399804115295"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

733 samples
959 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 586, 588, 586, 586, 586 
Resampling results:

  ROC  Sens  Spec
  1    1     1   

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     10.8      0.0
  positive      0.0     89.2
                       
 Accuracy (average) : 1

[1] "TEST accuracy: 1"
[1] "TEST +precision: 1"
[1] "TEST -precision: 1"
[1] "TEST specifity: 1"
[1] "TEST sensitivity: 1"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        0        0
            positive        5      218
[1] "TEST accuracy: 0.977578475336323"
[1] "TEST +precision: 0.977578475336323"
[1] "TEST -precision: NaN"
[1] "TEST specifity: 0"
[1] "TEST sensitivity: 1"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 1.98369015057882"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

733 samples
959 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 587, 587, 586, 586, 586 
Resampling results across tuning parameters:

  C      M  ROC        Sens       Spec     
  0.010  1  0.5710176  0.1008333  1.0000000
  0.010  2  0.5863865  0.1275000  1.0000000
  0.010  3  0.5630087  0.0900000  1.0000000
  0.255  1  0.8775881  0.5441667  0.9923664
  0.255  2  0.8642847  0.5191667  0.9923664
  0.255  3  0.8484832  0.4675000  0.9969466
  0.500  1  0.8844583  0.5441667  0.9938931
  0.500  2  0.8735404  0.5191667  0.9938931
  0.500  3  0.8545901  0.4675000  0.9984733

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.5 and M = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative      5.9      0.5
  positive      4.9     88.7
                            
 Accuracy (average) : 0.9454

[1] "TEST accuracy: 0.945429740791269"
[1] "TEST +precision: 0.947521865889213"
[1] "TEST -precision: 0.914893617021277"
[1] "TEST specifity: 0.544303797468354"
[1] "TEST sensitivity: 0.99388379204893"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        0        2
            positive        5      216
[1] "TEST accuracy: 0.968609865470852"
[1] "TEST +precision: 0.97737556561086"
[1] "TEST -precision: 0"
[1] "TEST specifity: 0"
[1] "TEST sensitivity: 0.990825688073395"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 3.83348023494085"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

733 samples
959 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 586, 586, 588, 586, 586 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens       Spec     
  0.3  1          0.6               0.50        50      0.9142223  0.5316667  1.0000000
  0.3  1          0.6               0.50       100      0.9358918  0.6566667  0.9984733
  0.3  1          0.6               0.50       150      0.9371142  0.6691667  0.9954198
  0.3  1          0.6               0.75        50      0.9109400  0.5583333  1.0000000
  0.3  1          0.6               0.75       100      0.9378622  0.7083333  1.0000000
  0.3  1          0.6               0.75       150      0.9461752  0.7716667  1.0000000
  0.3  1          0.6               1.00        50      0.9093163  0.5941667  1.0000000
  0.3  1          0.6               1.00       100      0.9332305  0.7841667  0.9984733
  0.3  1          0.6               1.00       150      0.9429529  0.8091667  0.9969466
  0.3  1          0.8               0.50        50      0.9104749  0.5175000  1.0000000
  0.3  1          0.8               0.50       100      0.9120358  0.6708333  1.0000000
  0.3  1          0.8               0.50       150      0.9278074  0.6833333  0.9954198
  0.3  1          0.8               0.75        50      0.9108836  0.5950000  0.9984615
  0.3  1          0.8               0.75       100      0.9421500  0.7841667  0.9954081
  0.3  1          0.8               0.75       150      0.9465212  0.8091667  0.9954081
  0.3  1          0.8               1.00        50      0.9064726  0.5925000  1.0000000
  0.3  1          0.8               1.00       100      0.9354976  0.7716667  0.9984733
  0.3  1          0.8               1.00       150      0.9419533  0.7966667  0.9969466
  0.3  2          0.6               0.50        50      0.9211017  0.6191667  1.0000000
  0.3  2          0.6               0.50       100      0.9369292  0.6441667  0.9984733
  0.3  2          0.6               0.50       150      0.9384128  0.6825000  0.9984733
  0.3  2          0.6               0.75        50      0.9459599  0.7341667  0.9984733
  0.3  2          0.6               0.75       100      0.9467174  0.7841667  0.9984733
  0.3  2          0.6               0.75       150      0.9473662  0.8091667  0.9984733
  0.3  2          0.6               1.00        50      0.9449725  0.7716667  0.9984733
  0.3  2          0.6               1.00       100      0.9520175  0.8216667  0.9984733
  0.3  2          0.6               1.00       150      0.9499757  0.8216667  0.9984733
  0.3  2          0.8               0.50        50      0.9380610  0.6708333  1.0000000
  0.3  2          0.8               0.50       100      0.9335811  0.7341667  1.0000000
  0.3  2          0.8               0.50       150      0.9400768  0.7341667  1.0000000
  0.3  2          0.8               0.75        50      0.9501290  0.7966667  0.9969348
  0.3  2          0.8               0.75       100      0.9519862  0.8216667  0.9984615
  0.3  2          0.8               0.75       150      0.9521413  0.8216667  0.9984615
  0.3  2          0.8               1.00        50      0.9431215  0.7966667  0.9984733
  0.3  2          0.8               1.00       100      0.9514713  0.8216667  1.0000000
  0.3  2          0.8               1.00       150      0.9510276  0.8216667  0.9984733
  0.3  3          0.6               0.50        50      0.9417231  0.6833333  0.9984615
  0.3  3          0.6               0.50       100      0.9452034  0.7083333  0.9969348
  0.3  3          0.6               0.50       150      0.9475078  0.7333333  0.9984615
  0.3  3          0.6               0.75        50      0.9523053  0.7966667  0.9984615
  0.3  3          0.6               0.75       100      0.9542708  0.8216667  0.9969348
  0.3  3          0.6               0.75       150      0.9549602  0.8216667  0.9984615
  0.3  3          0.6               1.00        50      0.9557532  0.8466667  0.9984733
  0.3  3          0.6               1.00       100      0.9579524  0.8466667  1.0000000
  0.3  3          0.6               1.00       150      0.9581218  0.8466667  1.0000000
  0.3  3          0.8               0.50        50      0.9330856  0.7208333  0.9984733
  0.3  3          0.8               0.50       100      0.9370930  0.7583333  0.9984733
  0.3  3          0.8               0.50       150      0.9388914  0.7716667  0.9969466
  0.3  3          0.8               0.75        50      0.9554957  0.7841667  0.9984733
  0.3  3          0.8               0.75       100      0.9558774  0.8091667  0.9984733
  0.3  3          0.8               0.75       150      0.9575399  0.8091667  0.9984733
  0.3  3          0.8               1.00        50      0.9565380  0.8466667  0.9984733
  0.3  3          0.8               1.00       100      0.9577570  0.8466667  1.0000000
  0.3  3          0.8               1.00       150      0.9564237  0.8466667  1.0000000
  0.4  1          0.6               0.50        50      0.9009362  0.5316667  1.0000000
  0.4  1          0.6               0.50       100      0.9133618  0.5941667  1.0000000
  0.4  1          0.6               0.50       150      0.9252987  0.7208333  0.9984733
  0.4  1          0.6               0.75        50      0.9278581  0.6591667  1.0000000
  0.4  1          0.6               0.75       100      0.9448011  0.7716667  0.9984615
  0.4  1          0.6               0.75       150      0.9455632  0.7966667  0.9969348
  0.4  1          0.6               1.00        50      0.9139153  0.7341667  1.0000000
  0.4  1          0.6               1.00       100      0.9453009  0.7966667  0.9969466
  0.4  1          0.6               1.00       150      0.9480061  0.7966667  0.9984733
  0.4  1          0.8               0.50        50      0.9009347  0.5550000  0.9969348
  0.4  1          0.8               0.50       100      0.9296916  0.6966667  0.9969348
  0.4  1          0.8               0.50       150      0.9357459  0.7341667  0.9984615
  0.4  1          0.8               0.75        50      0.9220600  0.6833333  1.0000000
  0.4  1          0.8               0.75       100      0.9412353  0.7958333  1.0000000
  0.4  1          0.8               0.75       150      0.9432583  0.7958333  1.0000000
  0.4  1          0.8               1.00        50      0.9157617  0.7091667  1.0000000
  0.4  1          0.8               1.00       100      0.9420773  0.7966667  0.9969466
  0.4  1          0.8               1.00       150      0.9458285  0.7966667  0.9969466
  0.4  2          0.6               0.50        50      0.9126754  0.6958333  0.9984615
  0.4  2          0.6               0.50       100      0.9244853  0.6691667  0.9969348
  0.4  2          0.6               0.50       150      0.9331364  0.6958333  0.9969231
  0.4  2          0.6               0.75        50      0.9518448  0.7841667  0.9984733
  0.4  2          0.6               0.75       100      0.9522420  0.7841667  0.9984733
  0.4  2          0.6               0.75       150      0.9541693  0.8091667  0.9984733
  0.4  2          0.6               1.00        50      0.9470274  0.8216667  0.9984733
  0.4  2          0.6               1.00       100      0.9476713  0.8216667  0.9984733
  0.4  2          0.6               1.00       150      0.9492409  0.8216667  1.0000000
  0.4  2          0.8               0.50        50      0.9286432  0.6708333  0.9954081
  0.4  2          0.8               0.50       100      0.9410215  0.6958333  1.0000000
  0.4  2          0.8               0.50       150      0.9424360  0.6958333  1.0000000
  0.4  2          0.8               0.75        50      0.9446944  0.7966667  1.0000000
  0.4  2          0.8               0.75       100      0.9437877  0.7966667  1.0000000
  0.4  2          0.8               0.75       150      0.9438117  0.7966667  1.0000000
  0.4  2          0.8               1.00        50      0.9518601  0.8216667  0.9984733
  0.4  2          0.8               1.00       100      0.9521308  0.8216667  0.9984733
  0.4  2          0.8               1.00       150      0.9527939  0.8216667  0.9984733
  0.4  3          0.6               0.50        50      0.9374147  0.6841667  0.9984733
  0.4  3          0.6               0.50       100      0.9435846  0.7466667  0.9984733
  0.4  3          0.6               0.50       150      0.9423097  0.7466667  0.9984733
  0.4  3          0.6               0.75        50      0.9454399  0.7700000  1.0000000
  0.4  3          0.6               0.75       100      0.9491110  0.7966667  1.0000000
  0.4  3          0.6               0.75       150      0.9499555  0.7966667  0.9984615
  0.4  3          0.6               1.00        50      0.9522954  0.8216667  1.0000000
  0.4  3          0.6               1.00       100      0.9525602  0.8216667  1.0000000
  0.4  3          0.6               1.00       150      0.9532281  0.8216667  1.0000000
  0.4  3          0.8               0.50        50      0.9381102  0.7591667  0.9984733
  0.4  3          0.8               0.50       100      0.9411705  0.7591667  0.9954081
  0.4  3          0.8               0.50       150      0.9433950  0.7591667  0.9984733
  0.4  3          0.8               0.75        50      0.9518545  0.8091667  1.0000000
  0.4  3          0.8               0.75       100      0.9565655  0.8341667  1.0000000
  0.4  3          0.8               0.75       150      0.9553276  0.8341667  1.0000000
  0.4  3          0.8               1.00        50      0.9514294  0.8216667  0.9984733
  0.4  3          0.8               1.00       100      0.9529919  0.8216667  0.9984733
  0.4  3          0.8               1.00       150      0.9535381  0.8216667  0.9984733

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 150, max_depth = 3, eta = 0.3, gamma = 0, colsample_bytree = 0.6, min_child_weight = 1 and subsample = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative      9.1      0.0
  positive      1.6     89.2
                            
 Accuracy (average) : 0.9836

[1] "TEST accuracy: 0.983628922237381"
[1] "TEST +precision: 0.981981981981982"
[1] "TEST -precision: 1"
[1] "TEST specifity: 0.848101265822785"
[1] "TEST sensitivity: 1"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        0        0
            positive        5      218
[1] "TEST accuracy: 0.977578475336323"
[1] "TEST +precision: 0.977578475336323"
[1] "TEST -precision: NaN"
[1] "TEST specifity: 0"
[1] "TEST sensitivity: 1"

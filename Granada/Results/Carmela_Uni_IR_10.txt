[1] "DATASET NAME: Carmela_Uni_IR_10"
[1] "TRAIN INSTANCES: 333"
[1] "TEST INSTANCES: 103"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 1.90789389610291"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

333 samples
746 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 266, 266, 267, 267, 266 
Resampling results:

  ROC        Sens       Spec     
  0.9821527  0.8772727  0.9783766

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     14.7      1.8
  positive      2.1     81.4
                           
 Accuracy (average) : 0.961

[1] "TEST accuracy: 0.960960960960961"
[1] "TEST +precision: 0.974820143884892"
[1] "TEST -precision: 0.890909090909091"
[1] "TEST specifity: 0.875"
[1] "TEST sensitivity: 0.978339350180505"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        4        2
            positive        4       93
[1] "TEST accuracy: 0.941747572815534"
[1] "TEST +precision: 0.958762886597938"
[1] "TEST -precision: 0.666666666666667"
[1] "TEST specifity: 0.5"
[1] "TEST sensitivity: 0.978947368421053"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 52.957494020462"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

333 samples
746 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 266, 267, 266, 267, 266 
Resampling results across tuning parameters:

  C      M  ROC        Sens       Spec     
  0.010  1  0.7074803  0.4303030  0.9783766
  0.010  2  0.6701427  0.3393939  0.9783766
  0.010  3  0.6684071  0.3393939  0.9820130
  0.255  1  0.8333323  0.6818182  0.9495455
  0.255  2  0.7990437  0.5742424  0.9531169
  0.255  3  0.8150020  0.5924242  0.9603247
  0.500  1  0.8590447  0.7151515  0.9495455
  0.500  2  0.8373849  0.6606061  0.9423377
  0.500  3  0.8152834  0.6090909  0.9531169

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.5 and M = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     12.0      4.2
  positive      4.8     79.0
                            
 Accuracy (average) : 0.9099

[1] "TEST accuracy: 0.90990990990991"
[1] "TEST +precision: 0.942652329749104"
[1] "TEST -precision: 0.740740740740741"
[1] "TEST specifity: 0.714285714285714"
[1] "TEST sensitivity: 0.949458483754513"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        0        3
            positive        8       92
[1] "TEST accuracy: 0.893203883495146"
[1] "TEST +precision: 0.92"
[1] "TEST -precision: 0"
[1] "TEST specifity: 0"
[1] "TEST sensitivity: 0.968421052631579"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 1.76322838068008"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

333 samples
746 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 267, 266, 266, 267, 266 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens       Spec     
  0.3  1          0.6               0.50        50      0.9552597  0.5515152  0.9964286
  0.3  1          0.6               0.50       100      0.9716667  0.6227273  1.0000000
  0.3  1          0.6               0.50       150      0.9694195  0.7136364  0.9818831
  0.3  1          0.6               0.75        50      0.9545297  0.5696970  0.9928571
  0.3  1          0.6               0.75       100      0.9725462  0.6787879  0.9855195
  0.3  1          0.6               0.75       150      0.9769953  0.6969697  0.9819481
  0.3  1          0.6               1.00        50      0.9647757  0.5712121  0.9891558
  0.3  1          0.6               1.00       100      0.9774577  0.6787879  0.9927922
  0.3  1          0.6               1.00       150      0.9823318  0.7500000  0.9891558
  0.3  1          0.8               0.50        50      0.9288213  0.5530303  0.9892208
  0.3  1          0.8               0.50       100      0.9606434  0.6424242  0.9819481
  0.3  1          0.8               0.50       150      0.9645120  0.7151515  0.9820130
  0.3  1          0.8               0.75        50      0.9648957  0.6060606  0.9927922
  0.3  1          0.8               0.75       100      0.9714915  0.6772727  0.9819481
  0.3  1          0.8               0.75       150      0.9764581  0.6969697  0.9855844
  0.3  1          0.8               1.00        50      0.9649606  0.5893939  0.9891558
  0.3  1          0.8               1.00       100      0.9773327  0.7318182  0.9963636
  0.3  1          0.8               1.00       150      0.9823534  0.7500000  0.9891558
  0.3  2          0.6               0.50        50      0.9486718  0.6590909  0.9855844
  0.3  2          0.6               0.50       100      0.9503483  0.7318182  0.9711688
  0.3  2          0.6               0.50       150      0.9509819  0.7500000  0.9783117
  0.3  2          0.6               0.75        50      0.9706966  0.6787879  0.9855195
  0.3  2          0.6               0.75       100      0.9697265  0.7681818  0.9819481
  0.3  2          0.6               0.75       150      0.9645514  0.7848485  0.9711039
  0.3  2          0.6               1.00        50      0.9813754  0.6954545  0.9927273
  0.3  2          0.6               1.00       100      0.9805647  0.7151515  0.9891558
  0.3  2          0.6               1.00       150      0.9760488  0.7515152  0.9819481
  0.3  2          0.8               0.50        50      0.9617887  0.6257576  0.9819481
  0.3  2          0.8               0.50       100      0.9604742  0.7333333  0.9783117
  0.3  2          0.8               0.50       150      0.9643920  0.7151515  0.9711688
  0.3  2          0.8               0.75        50      0.9815506  0.6772727  0.9927922
  0.3  2          0.8               0.75       100      0.9763302  0.7318182  0.9856494
  0.3  2          0.8               0.75       150      0.9714719  0.7666667  0.9783766
  0.3  2          0.8               1.00        50      0.9789709  0.6954545  0.9890909
  0.3  2          0.8               1.00       100      0.9757202  0.7151515  0.9890909
  0.3  2          0.8               1.00       150      0.9711039  0.7515152  0.9819481
  0.3  3          0.6               0.50        50      0.9371488  0.6242424  0.9782468
  0.3  3          0.6               0.50       100      0.9411196  0.7151515  0.9747403
  0.3  3          0.6               0.50       150      0.9403916  0.7151515  0.9711688
  0.3  3          0.6               0.75        50      0.9718024  0.7136364  0.9963636
  0.3  3          0.6               0.75       100      0.9687859  0.8030303  0.9855195
  0.3  3          0.6               0.75       150      0.9690594  0.8196970  0.9855195
  0.3  3          0.6               1.00        50      0.9823987  0.7303030  0.9890909
  0.3  3          0.6               1.00       100      0.9755037  0.7848485  0.9855195
  0.3  3          0.6               1.00       150      0.9767926  0.7848485  0.9819481
  0.3  3          0.8               0.50        50      0.9596497  0.6772727  0.9820130
  0.3  3          0.8               0.50       100      0.9650118  0.7318182  0.9820130
  0.3  3          0.8               0.50       150      0.9644215  0.7500000  0.9602597
  0.3  3          0.8               0.75        50      0.9735085  0.6787879  0.9783766
  0.3  3          0.8               0.75       100      0.9627765  0.7681818  0.9746753
  0.3  3          0.8               0.75       150      0.9594569  0.7863636  0.9783766
  0.3  3          0.8               1.00        50      0.9799744  0.7484848  0.9890909
  0.3  3          0.8               1.00       100      0.9731976  0.7833333  0.9855195
  0.3  3          0.8               1.00       150      0.9729319  0.7666667  0.9818831
  0.4  1          0.6               0.50        50      0.9615565  0.5893939  0.9964286
  0.4  1          0.6               0.50       100      0.9763813  0.7151515  0.9855844
  0.4  1          0.6               0.50       150      0.9748268  0.7515152  0.9927922
  0.4  1          0.6               0.75        50      0.9663538  0.6090909  0.9891558
  0.4  1          0.6               0.75       100      0.9716037  0.6954545  0.9819481
  0.4  1          0.6               0.75       150      0.9713026  0.7136364  0.9819481
  0.4  1          0.6               1.00        50      0.9684927  0.6621212  0.9891558
  0.4  1          0.6               1.00       100      0.9814010  0.7500000  0.9855844
  0.4  1          0.6               1.00       150      0.9826072  0.7318182  0.9855844
  0.4  1          0.8               0.50        50      0.9560153  0.6075758  0.9892857
  0.4  1          0.8               0.50       100      0.9646124  0.6424242  0.9927922
  0.4  1          0.8               0.50       150      0.9708363  0.7151515  0.9855844
  0.4  1          0.8               0.75        50      0.9710626  0.6787879  0.9819481
  0.4  1          0.8               0.75       100      0.9791755  0.6787879  0.9819481
  0.4  1          0.8               0.75       150      0.9758894  0.7333333  0.9819481
  0.4  1          0.8               1.00        50      0.9702981  0.6439394  0.9927922
  0.4  1          0.8               1.00       100      0.9797835  0.7136364  0.9891558
  0.4  1          0.8               1.00       150      0.9803030  0.7136364  0.9819481
  0.4  2          0.6               0.50        50      0.9730519  0.7318182  0.9819481
  0.4  2          0.6               0.50       100      0.9658501  0.7136364  0.9783117
  0.4  2          0.6               0.50       150      0.9690693  0.7303030  0.9711039
  0.4  2          0.6               0.75        50      0.9801338  0.7515152  0.9927273
  0.4  2          0.6               0.75       100      0.9712790  0.7318182  0.9783117
  0.4  2          0.6               0.75       150      0.9679201  0.7500000  0.9819481
  0.4  2          0.6               1.00        50      0.9818693  0.6969697  0.9927273
  0.4  2          0.6               1.00       100      0.9726131  0.7515152  0.9784416
  0.4  2          0.6               1.00       150      0.9684494  0.7681818  0.9783766
  0.4  2          0.8               0.50        50      0.9536088  0.6424242  0.9855195
  0.4  2          0.8               0.50       100      0.9540830  0.7151515  0.9746753
  0.4  2          0.8               0.50       150      0.9490122  0.7333333  0.9783117
  0.4  2          0.8               0.75        50      0.9748091  0.7333333  0.9855195
  0.4  2          0.8               0.75       100      0.9658422  0.8030303  0.9783117
  0.4  2          0.8               0.75       150      0.9695907  0.7863636  0.9783117
  0.4  2          0.8               1.00        50      0.9809721  0.6787879  0.9819481
  0.4  2          0.8               1.00       100      0.9738292  0.7681818  0.9819481
  0.4  2          0.8               1.00       150      0.9711196  0.7848485  0.9747403
  0.4  3          0.6               0.50        50      0.9545533  0.6954545  0.9783117
  0.4  3          0.6               0.50       100      0.9562180  0.7303030  0.9783766
  0.4  3          0.6               0.50       150      0.9550590  0.7666667  0.9638961
  0.4  3          0.6               0.75        50      0.9689630  0.7136364  0.9927273
  0.4  3          0.6               0.75       100      0.9633943  0.7848485  0.9855195
  0.4  3          0.6               0.75       150      0.9604014  0.7848485  0.9855195
  0.4  3          0.6               1.00        50      0.9721940  0.7848485  0.9890909
  0.4  3          0.6               1.00       100      0.9652479  0.7848485  0.9891558
  0.4  3          0.6               1.00       150      0.9643388  0.8030303  0.9820130
  0.4  3          0.8               0.50        50      0.9542739  0.7500000  0.9747403
  0.4  3          0.8               0.50       100      0.9505765  0.7666667  0.9675974
  0.4  3          0.8               0.50       150      0.9545376  0.7681818  0.9640260
  0.4  3          0.8               0.75        50      0.9686698  0.7863636  0.9784416
  0.4  3          0.8               0.75       100      0.9659032  0.7863636  0.9748052
  0.4  3          0.8               0.75       150      0.9655234  0.7863636  0.9784416
  0.4  3          0.8               1.00        50      0.9772314  0.7848485  0.9818831
  0.4  3          0.8               1.00       100      0.9706238  0.8030303  0.9783117
  0.4  3          0.8               1.00       150      0.9700118  0.8030303  0.9783766

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 150, max_depth = 1, eta = 0.4, gamma = 0, colsample_bytree = 0.6, min_child_weight = 1 and subsample = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     12.3      1.2
  positive      4.5     82.0
                            
 Accuracy (average) : 0.9429

[1] "TEST accuracy: 0.942942942942943"
[1] "TEST +precision: 0.947916666666667"
[1] "TEST -precision: 0.911111111111111"
[1] "TEST specifity: 0.732142857142857"
[1] "TEST sensitivity: 0.985559566787004"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        4        4
            positive        4       91
[1] "TEST accuracy: 0.922330097087379"
[1] "TEST +precision: 0.957894736842105"
[1] "TEST -precision: 0.5"
[1] "TEST specifity: 0.5"
[1] "TEST sensitivity: 0.957894736842105"

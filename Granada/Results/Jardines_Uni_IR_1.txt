[1] "DATASET NAME: Jardines_Uni_IR_1"
[1] "TRAIN INSTANCES: 1306"
[1] "TEST INSTANCES: 223"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 4.49692797660828"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

1306 samples
 736 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1046, 1044, 1044, 1045, 1045 
Resampling results:

  ROC  Sens  Spec
  1    1     1   

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative       50        0
  positive        0       50
                       
 Accuracy (average) : 1

[1] "TEST accuracy: 1"
[1] "TEST +precision: 1"
[1] "TEST -precision: 1"
[1] "TEST specifity: 1"
[1] "TEST sensitivity: 1"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        2        0
            positive        2      219
[1] "TEST accuracy: 0.991031390134529"
[1] "TEST +precision: 0.990950226244344"
[1] "TEST -precision: 1"
[1] "TEST specifity: 0.5"
[1] "TEST sensitivity: 1"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 1.98550151586533"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

1306 samples
 736 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1045, 1044, 1044, 1046, 1045 
Resampling results across tuning parameters:

  C      M  ROC        Sens  Spec     
  0.010  1  0.9811412  1     0.9525073
  0.010  2  0.9811412  1     0.9525073
  0.010  3  0.9811412  1     0.9525073
  0.255  1  0.9846958  1     0.9617029
  0.255  2  0.9846958  1     0.9617029
  0.255  3  0.9846958  1     0.9617029
  0.500  1  0.9864805  1     0.9663065
  0.500  2  0.9864805  1     0.9663065
  0.500  3  0.9854283  1     0.9663065

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.5 and M = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     50.0      1.7
  positive      0.0     48.3
                            
 Accuracy (average) : 0.9832

[1] "TEST accuracy: 0.983154670750383"
[1] "TEST +precision: 1"
[1] "TEST -precision: 0.967407407407407"
[1] "TEST specifity: 1"
[1] "TEST sensitivity: 0.966309341500766"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        1        1
            positive        3      218
[1] "TEST accuracy: 0.982062780269058"
[1] "TEST +precision: 0.986425339366516"
[1] "TEST -precision: 0.5"
[1] "TEST specifity: 0.25"
[1] "TEST sensitivity: 0.995433789954338"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 4.95593134959539"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

1306 samples
 736 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1045, 1044, 1045, 1044, 1046 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens  Spec     
  0.3  1          0.6               0.50        50      0.9947804  1     0.9571227
  0.3  1          0.6               0.50       100      0.9976304  1     0.9800705
  0.3  1          0.6               0.50       150      0.9984733  1     0.9831474
  0.3  1          0.6               0.75        50      0.9950038  1     0.9617264
  0.3  1          0.6               0.75       100      0.9981562  1     0.9846741
  0.3  1          0.6               0.75       150      0.9984733  1     0.9862126
  0.3  1          0.6               1.00        50      0.9946653  1     0.9617029
  0.3  1          0.6               1.00       100      0.9984733  1     0.9800940
  0.3  1          0.6               1.00       150      0.9986015  1     0.9846976
  0.3  1          0.8               0.50        50      0.9950190  1     0.9586494
  0.3  1          0.8               0.50       100      0.9980763  1     0.9754903
  0.3  1          0.8               0.50       150      0.9985665  1     0.9862008
  0.3  1          0.8               0.75        50      0.9950997  1     0.9601644
  0.3  1          0.8               0.75       100      0.9978391  1     0.9770170
  0.3  1          0.8               0.75       150      0.9984733  1     0.9846858
  0.3  1          0.8               1.00        50      0.9960382  1     0.9540341
  0.3  1          0.8               1.00       100      0.9981092  1     0.9831591
  0.3  1          0.8               1.00       150      0.9984733  1     0.9831591
  0.3  2          0.6               0.50        50      0.9988346  1     0.9816089
  0.3  2          0.6               0.50       100      0.9992774  1     0.9831356
  0.3  2          0.6               0.50       150      0.9995222  1     0.9877510
  0.3  2          0.6               0.75        50      0.9993357  1     0.9831356
  0.3  2          0.6               0.75       100      0.9988113  1     0.9877393
  0.3  2          0.6               0.75       150      0.9989628  1     0.9877393
  0.3  2          0.6               1.00        50      0.9989377  1     0.9877393
  0.3  2          0.6               1.00       100      1.0000000  1     0.9892777
  0.3  2          0.6               1.00       150      0.9994173  1     0.9862243
  0.3  2          0.8               0.50        50      0.9992174  1     0.9846741
  0.3  2          0.8               0.50       100      0.9986947  1     0.9892777
  0.3  2          0.8               0.50       150      0.9989628  1     0.9892777
  0.3  2          0.8               0.75        50      0.9997786  1     0.9862126
  0.3  2          0.8               0.75       100      0.9994406  1     0.9923547
  0.3  2          0.8               0.75       150      0.9995222  1     0.9892895
  0.3  2          0.8               1.00        50      1.0000000  1     0.9847093
  0.3  2          0.8               1.00       100      1.0000000  1     0.9923429
  0.3  2          0.8               1.00       150      1.0000000  1     0.9908162
  0.3  3          0.6               0.50        50      1.0000000  1     0.9862008
  0.3  3          0.6               0.50       100      1.0000000  1     0.9862008
  0.3  3          0.6               0.50       150      1.0000000  1     0.9862008
  0.3  3          0.6               0.75        50      0.9996038  1     0.9877393
  0.3  3          0.6               0.75       100      1.0000000  1     0.9892660
  0.3  3          0.6               0.75       150      1.0000000  1     0.9908045
  0.3  3          0.6               1.00        50      1.0000000  1     0.9938579
  0.3  3          0.6               1.00       100      1.0000000  1     0.9938579
  0.3  3          0.6               1.00       150      1.0000000  1     0.9938579
  0.3  3          0.8               0.50        50      1.0000000  1     0.9892543
  0.3  3          0.8               0.50       100      1.0000000  1     0.9892660
  0.3  3          0.8               0.50       150      1.0000000  1     0.9892660
  0.3  3          0.8               0.75        50      0.9998252  1     0.9923194
  0.3  3          0.8               0.75       100      0.9995222  1     0.9923312
  0.3  3          0.8               0.75       150      1.0000000  1     0.9892660
  0.3  3          0.8               1.00        50      0.9996038  1     0.9908045
  0.3  3          0.8               1.00       100      0.9995222  1     0.9892777
  0.3  3          0.8               1.00       150      1.0000000  1     0.9892777
  0.4  1          0.6               0.50        50      0.9975954  1     0.9739636
  0.4  1          0.6               0.50       100      0.9986015  1     0.9846741
  0.4  1          0.6               0.50       150      0.9986947  1     0.9846741
  0.4  1          0.6               0.75        50      0.9977070  1     0.9708984
  0.4  1          0.6               0.75       100      0.9986481  1     0.9831239
  0.4  1          0.6               0.75       150      0.9984733  1     0.9862008
  0.4  1          0.6               1.00        50      0.9971036  1     0.9724251
  0.4  1          0.6               1.00       100      0.9984733  1     0.9846858
  0.4  1          0.6               1.00       150      0.9986481  1     0.9877510
  0.4  1          0.8               0.50        50      0.9956222  1     0.9632179
  0.4  1          0.8               0.50       100      0.9982985  1     0.9785555
  0.4  1          0.8               0.50       150      0.9985199  1     0.9831474
  0.4  1          0.8               0.75        50      0.9967449  1     0.9678332
  0.4  1          0.8               0.75       100      0.9984733  1     0.9862243
  0.4  1          0.8               0.75       150      0.9984733  1     0.9846741
  0.4  1          0.8               1.00        50      0.9971597  1     0.9724251
  0.4  1          0.8               1.00       100      0.9985199  1     0.9816207
  0.4  1          0.8               1.00       150      0.9986947  1     0.9877510
  0.4  2          0.6               0.50        50      1.0000000  1     0.9754668
  0.4  2          0.6               0.50       100      1.0000000  1     0.9877393
  0.4  2          0.6               0.50       150      1.0000000  1     0.9877393
  0.4  2          0.6               0.75        50      0.9992308  1     0.9877393
  0.4  2          0.6               0.75       100      0.9989628  1     0.9877510
  0.4  2          0.6               0.75       150      0.9995222  1     0.9877510
  0.4  2          0.6               1.00        50      1.0000000  1     0.9862126
  0.4  2          0.6               1.00       100      1.0000000  1     0.9877510
  0.4  2          0.6               1.00       150      0.9999184  1     0.9862243
  0.4  2          0.8               0.50        50      0.9986947  1     0.9862126
  0.4  2          0.8               0.50       100      0.9991143  1     0.9892777
  0.4  2          0.8               0.50       150      0.9993357  1     0.9892777
  0.4  2          0.8               0.75        50      0.9987879  1     0.9862126
  0.4  2          0.8               0.75       100      0.9990560  1     0.9877510
  0.4  2          0.8               0.75       150      0.9995222  1     0.9877510
  0.4  2          0.8               1.00        50      0.9987879  1     0.9862126
  0.4  2          0.8               1.00       100      0.9986947  1     0.9908162
  0.4  2          0.8               1.00       150      0.9992192  1     0.9892777
  0.4  3          0.6               0.50        50      0.9995222  1     0.9877275
  0.4  3          0.6               0.50       100      1.0000000  1     0.9862008
  0.4  3          0.6               0.50       150      1.0000000  1     0.9877393
  0.4  3          0.6               0.75        50      0.9995222  1     0.9908280
  0.4  3          0.6               0.75       100      1.0000000  1     0.9923547
  0.4  3          0.6               0.75       150      1.0000000  1     0.9923547
  0.4  3          0.6               1.00        50      1.0000000  1     0.9908045
  0.4  3          0.6               1.00       100      1.0000000  1     0.9923312
  0.4  3          0.6               1.00       150      1.0000000  1     0.9908045
  0.4  3          0.8               0.50        50      0.9996504  1     0.9862008
  0.4  3          0.8               0.50       100      1.0000000  1     0.9862126
  0.4  3          0.8               0.50       150      1.0000000  1     0.9877510
  0.4  3          0.8               0.75        50      1.0000000  1     0.9892660
  0.4  3          0.8               0.75       100      1.0000000  1     0.9877510
  0.4  3          0.8               0.75       150      1.0000000  1     0.9877393
  0.4  3          0.8               1.00        50      1.0000000  1     0.9938579
  0.4  3          0.8               1.00       100      1.0000000  1     0.9923312
  0.4  3          0.8               1.00       150      1.0000000  1     0.9938579

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 50, max_depth = 2, eta = 0.3, gamma = 0, colsample_bytree = 0.8, min_child_weight = 1 and subsample = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     50.0      0.8
  positive      0.0     49.2
                            
 Accuracy (average) : 0.9923

[1] "TEST accuracy: 0.992343032159265"
[1] "TEST +precision: 1"
[1] "TEST -precision: 0.984917043740573"
[1] "TEST specifity: 1"
[1] "TEST sensitivity: 0.98468606431853"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        1        4
            positive        3      215
[1] "TEST accuracy: 0.968609865470852"
[1] "TEST +precision: 0.986238532110092"
[1] "TEST -precision: 0.2"
[1] "TEST specifity: 0.25"
[1] "TEST sensitivity: 0.981735159817352"

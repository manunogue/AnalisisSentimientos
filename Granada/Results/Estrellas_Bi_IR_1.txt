[1] "DATASET NAME: Estrellas_Bi_IR_1"
[1] "TRAIN INSTANCES: 756"
[1] "TEST INSTANCES: 138"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 3.26818680763245"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

756 samples
940 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 605, 604, 606, 604, 605 
Resampling results:

  ROC  Sens  Spec     
  1    1     0.9947018

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     50.0      0.3
  positive      0.0     49.7
                            
 Accuracy (average) : 0.9974

[1] "TEST accuracy: 0.997354497354497"
[1] "TEST +precision: 1"
[1] "TEST -precision: 0.994736842105263"
[1] "TEST specifity: 1"
[1] "TEST sensitivity: 0.994708994708995"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        1        1
            positive        9      127
[1] "TEST accuracy: 0.927536231884058"
[1] "TEST +precision: 0.933823529411765"
[1] "TEST -precision: 0.5"
[1] "TEST specifity: 0.1"
[1] "TEST sensitivity: 0.9921875"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 1.96126361687978"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

756 samples
940 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 605, 605, 605, 604, 605 
Resampling results across tuning parameters:

  C      M  ROC        Sens       Spec     
  0.010  1  0.9657486  0.9762456  0.9206667
  0.010  2  0.9690965  0.9762456  0.9075088
  0.010  3  0.9705963  0.9736140  0.9022807
  0.255  1  0.9845150  1.0000000  0.9443860
  0.255  2  0.9912361  1.0000000  0.9496842
  0.255  3  0.9865785  0.9920351  0.9391930
  0.500  1  0.9776357  1.0000000  0.9550526
  0.500  2  0.9924841  1.0000000  0.9603509
  0.500  3  0.9886837  0.9946667  0.9445263

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.5 and M = 2.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative       50        2
  positive        0       48
                            
 Accuracy (average) : 0.9802

[1] "TEST accuracy: 0.98015873015873"
[1] "TEST +precision: 1"
[1] "TEST -precision: 0.961832061068702"
[1] "TEST specifity: 1"
[1] "TEST sensitivity: 0.96031746031746"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        1        4
            positive        9      124
[1] "TEST accuracy: 0.905797101449275"
[1] "TEST +precision: 0.932330827067669"
[1] "TEST -precision: 0.2"
[1] "TEST specifity: 0.1"
[1] "TEST sensitivity: 0.96875"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 3.57230528195699"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

756 samples
940 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 606, 605, 604, 605, 604 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens       Spec     
  0.3  1          0.6               0.50        50      0.9594150  0.9128070  0.9126316
  0.3  1          0.6               0.50       100      0.9813801  0.9603860  0.9312281
  0.3  1          0.6               0.50       150      0.9898605  0.9788421  0.9391579
  0.3  1          0.6               0.75        50      0.9684758  0.9235088  0.9338246
  0.3  1          0.6               0.75       100      0.9834451  0.9735439  0.9497544
  0.3  1          0.6               0.75       150      0.9889366  0.9947368  0.9576842
  0.3  1          0.6               1.00        50      0.9711174  0.9366316  0.9231579
  0.3  1          0.6               1.00       100      0.9875699  0.9656491  0.9524211
  0.3  1          0.6               1.00       150      0.9911155  0.9894386  0.9550526
  0.3  1          0.8               0.50        50      0.9631819  0.9073333  0.9257895
  0.3  1          0.8               0.50       100      0.9793296  0.9603860  0.9338246
  0.3  1          0.8               0.50       150      0.9872805  0.9841754  0.9444561
  0.3  1          0.8               0.75        50      0.9663254  0.9287368  0.9391579
  0.3  1          0.8               0.75       100      0.9863230  0.9656491  0.9523860
  0.3  1          0.8               0.75       150      0.9883310  0.9947368  0.9577193
  0.3  1          0.8               1.00        50      0.9693369  0.9340702  0.9470877
  0.3  1          0.8               1.00       100      0.9872586  0.9656491  0.9550526
  0.3  1          0.8               1.00       150      0.9902627  0.9894386  0.9550526
  0.3  2          0.6               0.50        50      0.9833767  0.9656491  0.9364912
  0.3  2          0.6               0.50       100      0.9913689  0.9761754  0.9365965
  0.3  2          0.6               0.50       150      0.9927398  0.9788421  0.9471579
  0.3  2          0.6               0.75        50      0.9891099  0.9814386  0.9576842
  0.3  2          0.6               0.75       100      0.9933336  0.9947368  0.9656140
  0.3  2          0.6               0.75       150      0.9952689  1.0000000  0.9629825
  0.3  2          0.6               1.00        50      0.9868098  0.9656491  0.9602807
  0.3  2          0.6               1.00       100      0.9938276  0.9947368  0.9629474
  0.3  2          0.6               1.00       150      0.9955599  0.9947368  0.9708772
  0.3  2          0.8               0.50        50      0.9812121  0.9628772  0.9311228
  0.3  2          0.8               0.50       100      0.9910745  0.9868421  0.9576842
  0.3  2          0.8               0.50       150      0.9927509  0.9868421  0.9576842
  0.3  2          0.8               0.75        50      0.9871615  0.9709123  0.9523158
  0.3  2          0.8               0.75       100      0.9917185  1.0000000  0.9629474
  0.3  2          0.8               0.75       150      0.9933719  0.9947368  0.9656140
  0.3  2          0.8               1.00        50      0.9894995  0.9656491  0.9576842
  0.3  2          0.8               1.00       100      0.9939076  0.9947368  0.9682456
  0.3  2          0.8               1.00       150      0.9953474  0.9947368  0.9682807
  0.3  3          0.6               0.50        50      0.9900707  0.9709474  0.9550175
  0.3  3          0.6               0.50       100      0.9942028  0.9788421  0.9576842
  0.3  3          0.6               0.50       150      0.9927859  0.9867368  0.9524211
  0.3  3          0.6               0.75        50      0.9944933  0.9947368  0.9576842
  0.3  3          0.6               0.75       100      0.9968744  1.0000000  0.9709123
  0.3  3          0.6               0.75       150      0.9944882  1.0000000  0.9629474
  0.3  3          0.6               1.00        50      0.9937966  0.9920702  0.9656140
  0.3  3          0.6               1.00       100      0.9970906  1.0000000  0.9735439
  0.3  3          0.6               1.00       150      0.9969034  1.0000000  0.9735439
  0.3  3          0.8               0.50        50      0.9879443  0.9735439  0.9523860
  0.3  3          0.8               0.50       100      0.9922273  0.9867368  0.9497544
  0.3  3          0.8               0.50       150      0.9924855  0.9867368  0.9550175
  0.3  3          0.8               0.75        50      0.9908434  0.9868421  0.9550175
  0.3  3          0.8               0.75       100      0.9950357  0.9947368  0.9682807
  0.3  3          0.8               0.75       150      0.9935944  0.9947368  0.9576842
  0.3  3          0.8               1.00        50      0.9920510  0.9947368  0.9629474
  0.3  3          0.8               1.00       100      0.9950986  0.9947368  0.9708772
  0.3  3          0.8               1.00       150      0.9969736  1.0000000  0.9709123
  0.4  1          0.6               0.50        50      0.9680026  0.9497544  0.9205614
  0.4  1          0.6               0.50       100      0.9835088  0.9761754  0.9338596
  0.4  1          0.6               0.50       150      0.9904069  0.9788421  0.9365614
  0.4  1          0.6               0.75        50      0.9742150  0.9498596  0.9391930
  0.4  1          0.6               0.75       100      0.9897140  0.9841053  0.9524561
  0.4  1          0.6               0.75       150      0.9912022  0.9947368  0.9603509
  0.4  1          0.6               1.00        50      0.9764156  0.9498596  0.9391579
  0.4  1          0.6               1.00       100      0.9900012  0.9920702  0.9523860
  0.4  1          0.6               1.00       150      0.9926438  0.9947368  0.9603509
  0.4  1          0.8               0.50        50      0.9710536  0.9392632  0.9125965
  0.4  1          0.8               0.50       100      0.9849055  0.9762105  0.9418596
  0.4  1          0.8               0.50       150      0.9911404  0.9947368  0.9576842
  0.4  1          0.8               0.75        50      0.9722649  0.9445965  0.9283860
  0.4  1          0.8               0.75       100      0.9881335  0.9947368  0.9550526
  0.4  1          0.8               0.75       150      0.9913296  0.9947368  0.9629825
  0.4  1          0.8               1.00        50      0.9768580  0.9551228  0.9523158
  0.4  1          0.8               1.00       100      0.9909649  0.9894386  0.9576842
  0.4  1          0.8               1.00       150      0.9929971  0.9947368  0.9576842
  0.4  2          0.6               0.50        50      0.9858418  0.9761754  0.9337544
  0.4  2          0.6               0.50       100      0.9931696  0.9788421  0.9444211
  0.4  2          0.6               0.50       150      0.9933044  0.9841053  0.9391228
  0.4  2          0.6               0.75        50      0.9893245  0.9921053  0.9549825
  0.4  2          0.6               0.75       100      0.9932271  0.9947368  0.9630175
  0.4  2          0.6               0.75       150      0.9944629  0.9947368  0.9629825
  0.4  2          0.6               1.00        50      0.9900442  0.9894386  0.9629474
  0.4  2          0.6               1.00       100      0.9956758  0.9947368  0.9709123
  0.4  2          0.6               1.00       150      0.9966171  1.0000000  0.9735439
  0.4  2          0.8               0.50        50      0.9848760  0.9788421  0.9365263
  0.4  2          0.8               0.50       100      0.9898759  0.9788421  0.9471228
  0.4  2          0.8               0.50       150      0.9903184  0.9788421  0.9497895
  0.4  2          0.8               0.75        50      0.9903096  0.9947368  0.9550526
  0.4  2          0.8               0.75       100      0.9939608  0.9947368  0.9656140
  0.4  2          0.8               0.75       150      0.9942873  0.9947368  0.9603158
  0.4  2          0.8               1.00        50      0.9902936  0.9920702  0.9602807
  0.4  2          0.8               1.00       100      0.9950161  0.9947368  0.9735088
  0.4  2          0.8               1.00       150      0.9954793  1.0000000  0.9709123
  0.4  3          0.6               0.50        50      0.9928161  0.9788421  0.9629825
  0.4  3          0.6               0.50       100      0.9937213  0.9947368  0.9524561
  0.4  3          0.6               0.50       150      0.9935665  0.9947368  0.9445263
  0.4  3          0.6               0.75        50      0.9933654  0.9947368  0.9656140
  0.4  3          0.6               0.75       100      0.9941056  1.0000000  0.9682456
  0.4  3          0.6               0.75       150      0.9936590  1.0000000  0.9603158
  0.4  3          0.6               1.00        50      0.9944709  0.9947368  0.9682456
  0.4  3          0.6               1.00       100      0.9969352  1.0000000  0.9682456
  0.4  3          0.6               1.00       150      0.9962718  0.9947368  0.9682456
  0.4  3          0.8               0.50        50      0.9924980  0.9868421  0.9577193
  0.4  3          0.8               0.50       100      0.9909275  0.9947368  0.9523860
  0.4  3          0.8               0.50       150      0.9917481  0.9947368  0.9523860
  0.4  3          0.8               0.75        50      0.9941560  1.0000000  0.9682456
  0.4  3          0.8               0.75       100      0.9951758  0.9947368  0.9682807
  0.4  3          0.8               0.75       150      0.9947229  1.0000000  0.9603158
  0.4  3          0.8               1.00        50      0.9941125  0.9947368  0.9682456
  0.4  3          0.8               1.00       100      0.9961465  1.0000000  0.9735789
  0.4  3          0.8               1.00       150      0.9958096  0.9947368  0.9629825

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 100, max_depth = 3, eta = 0.3, gamma = 0, colsample_bytree = 0.6, min_child_weight = 1 and subsample = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     50.0      1.3
  positive      0.0     48.7
                            
 Accuracy (average) : 0.9868

[1] "TEST accuracy: 0.986772486772487"
[1] "TEST +precision: 1"
[1] "TEST -precision: 0.974226804123711"
[1] "TEST specifity: 1"
[1] "TEST sensitivity: 0.973544973544973"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        3        4
            positive        7      124
[1] "TEST accuracy: 0.920289855072464"
[1] "TEST +precision: 0.946564885496183"
[1] "TEST -precision: 0.428571428571429"
[1] "TEST specifity: 0.3"
[1] "TEST sensitivity: 0.96875"

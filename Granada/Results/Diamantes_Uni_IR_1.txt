[1] "DATASET NAME: Diamantes_Uni_IR_1"
[1] "TRAIN INSTANCES: 748"
[1] "TEST INSTANCES: 131"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 2.96042895317078"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

748 samples
797 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 598, 598, 600, 598, 598 
Resampling results:

  ROC  Sens  Spec     
  1    1     0.9973333

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     50.0      0.1
  positive      0.0     49.9
                            
 Accuracy (average) : 0.9987

[1] "TEST accuracy: 0.998663101604278"
[1] "TEST +precision: 1"
[1] "TEST -precision: 0.997333333333333"
[1] "TEST specifity: 1"
[1] "TEST sensitivity: 0.997326203208556"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        2        0
            positive        7      122
[1] "TEST accuracy: 0.946564885496183"
[1] "TEST +precision: 0.945736434108527"
[1] "TEST -precision: 1"
[1] "TEST specifity: 0.222222222222222"
[1] "TEST sensitivity: 1"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 1.38335403601329"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

748 samples
797 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 599, 599, 598, 598, 598 
Resampling results across tuning parameters:

  C      M  ROC        Sens  Spec     
  0.010  1  0.9816740  1     0.9172252
  0.010  2  0.9816740  1     0.9172252
  0.010  3  0.9811154  1     0.9172252
  0.255  1  0.9803138  1     0.9278919
  0.255  2  0.9803138  1     0.9278919
  0.255  3  0.9797552  1     0.9278919
  0.500  1  0.9732891  1     0.9439640
  0.500  2  0.9732891  1     0.9439640
  0.500  3  0.9764195  1     0.9385586

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.01 and M = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     50.0      4.1
  positive      0.0     45.9
                            
 Accuracy (average) : 0.9586

[1] "TEST accuracy: 0.95855614973262"
[1] "TEST +precision: 1"
[1] "TEST -precision: 0.923456790123457"
[1] "TEST specifity: 1"
[1] "TEST sensitivity: 0.917112299465241"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        4        6
            positive        5      116
[1] "TEST accuracy: 0.916030534351145"
[1] "TEST +precision: 0.958677685950413"
[1] "TEST -precision: 0.4"
[1] "TEST specifity: 0.444444444444444"
[1] "TEST sensitivity: 0.950819672131147"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 3.07994301716487"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

748 samples
797 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 598, 599, 599, 598, 598 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens  Spec     
  0.3  1          0.6               0.50        50      0.9958929  1     0.9652252
  0.3  1          0.6               0.50       100      0.9977398  1     0.9758919
  0.3  1          0.6               0.50       150      0.9977168  1     0.9758919
  0.3  1          0.6               0.75        50      0.9930191  1     0.9625586
  0.3  1          0.6               0.75       100      0.9958852  1     0.9732252
  0.3  1          0.6               0.75       150      0.9973223  1     0.9785586
  0.3  1          0.6               1.00        50      0.9940435  1     0.9678919
  0.3  1          0.6               1.00       100      0.9958044  1     0.9732252
  0.3  1          0.6               1.00       150      0.9965924  1     0.9785946
  0.3  1          0.8               0.50        50      0.9917247  1     0.9652252
  0.3  1          0.8               0.50       100      0.9943092  1     0.9732252
  0.3  1          0.8               0.50       150      0.9964041  1     0.9758919
  0.3  1          0.8               0.75        50      0.9926544  1     0.9705946
  0.3  1          0.8               0.75       100      0.9955200  1     0.9732252
  0.3  1          0.8               0.75       150      0.9972142  1     0.9758919
  0.3  1          0.8               1.00        50      0.9948358  1     0.9705586
  0.3  1          0.8               1.00       100      0.9954873  1     0.9758919
  0.3  1          0.8               1.00       150      0.9969552  1     0.9785946
  0.3  2          0.6               0.50        50      0.9998578  1     0.9785586
  0.3  2          0.6               0.50       100      0.9991111  1     0.9786306
  0.3  2          0.6               0.50       150      0.9977956  1     0.9812973
  0.3  2          0.6               0.75        50      0.9990400  1     0.9758919
  0.3  2          0.6               0.75       100      0.9986489  1     0.9839640
  0.3  2          0.6               0.75       150      0.9975467  1     0.9812973
  0.3  2          0.6               1.00        50      0.9992000  1     0.9785946
  0.3  2          0.6               1.00       100      0.9985067  1     0.9839640
  0.3  2          0.6               1.00       150      0.9977600  1     0.9839640
  0.3  2          0.8               0.50        50      0.9992356  1     0.9786306
  0.3  2          0.8               0.50       100      0.9984000  1     0.9812973
  0.3  2          0.8               0.50       150      0.9966578  1     0.9812973
  0.3  2          0.8               0.75        50      0.9981867  1     0.9785586
  0.3  2          0.8               0.75       100      0.9983644  1     0.9839640
  0.3  2          0.8               0.75       150      0.9980800  1     0.9839640
  0.3  2          0.8               1.00        50      0.9964954  1     0.9839640
  0.3  2          0.8               1.00       100      0.9975063  1     0.9839640
  0.3  2          0.8               1.00       150      0.9971181  1     0.9839640
  0.3  3          0.6               0.50        50      0.9988569  1     0.9812973
  0.3  3          0.6               0.50       100      0.9971176  1     0.9786306
  0.3  3          0.6               0.50       150      0.9967976  1     0.9786306
  0.3  3          0.6               0.75        50      0.9997156  1     0.9866306
  0.3  3          0.6               0.75       100      0.9986489  1     0.9812973
  0.3  3          0.6               0.75       150      0.9991111  1     0.9812973
  0.3  3          0.6               1.00        50      1.0000000  1     0.9785946
  0.3  3          0.6               1.00       100      0.9990756  1     0.9812973
  0.3  3          0.6               1.00       150      0.9988267  1     0.9812973
  0.3  3          0.8               0.50        50      0.9990400  1     0.9785946
  0.3  3          0.8               0.50       100      0.9988978  1     0.9759279
  0.3  3          0.8               0.50       150      0.9988978  1     0.9812973
  0.3  3          0.8               0.75        50      0.9997156  1     0.9866306
  0.3  3          0.8               0.75       100      0.9989689  1     0.9839640
  0.3  3          0.8               0.75       150      0.9989689  1     0.9839640
  0.3  3          0.8               1.00        50      0.9996089  1     0.9839640
  0.3  3          0.8               1.00       100      0.9988622  1     0.9812973
  0.3  3          0.8               1.00       150      0.9988622  1     0.9812973
  0.4  1          0.6               0.50        50      0.9957204  1     0.9678919
  0.4  1          0.6               0.50       100      0.9966506  1     0.9732252
  0.4  1          0.6               0.50       150      0.9957996  1     0.9705586
  0.4  1          0.6               0.75        50      0.9966213  1     0.9705946
  0.4  1          0.6               0.75       100      0.9971104  1     0.9759279
  0.4  1          0.6               0.75       150      0.9966501  1     0.9732252
  0.4  1          0.6               1.00        50      0.9945211  1     0.9732252
  0.4  1          0.6               1.00       100      0.9967390  1     0.9758919
  0.4  1          0.6               1.00       150      0.9971839  1     0.9812613
  0.4  1          0.8               0.50        50      0.9952937  1     0.9785946
  0.4  1          0.8               0.50       100      0.9950962  1     0.9785946
  0.4  1          0.8               0.50       150      0.9936831  1     0.9732613
  0.4  1          0.8               0.75        50      0.9928471  1     0.9705586
  0.4  1          0.8               0.75       100      0.9983755  1     0.9785946
  0.4  1          0.8               0.75       150      0.9985730  1     0.9758919
  0.4  1          0.8               1.00        50      0.9944471  1     0.9705586
  0.4  1          0.8               1.00       100      0.9961989  1     0.9758919
  0.4  1          0.8               1.00       150      0.9982535  1     0.9839279
  0.4  2          0.6               0.50        50      0.9988622  1     0.9839279
  0.4  2          0.6               0.50       100      0.9986465  1     0.9812973
  0.4  2          0.6               0.50       150      0.9986489  1     0.9786306
  0.4  2          0.6               0.75        50      0.9990756  1     0.9812613
  0.4  2          0.6               0.75       100      0.9984711  1     0.9812973
  0.4  2          0.6               0.75       150      0.9974400  1     0.9759640
  0.4  2          0.6               1.00        50      0.9976490  1     0.9785946
  0.4  2          0.6               1.00       100      0.9973170  1     0.9839640
  0.4  2          0.6               1.00       150      0.9979680  1     0.9786306
  0.4  2          0.8               0.50        50      0.9969754  1     0.9785946
  0.4  2          0.8               0.50       100      0.9958376  1     0.9732613
  0.4  2          0.8               0.50       150      0.9961244  1     0.9786306
  0.4  2          0.8               0.75        50      0.9993225  1     0.9785946
  0.4  2          0.8               0.75       100      0.9979656  1     0.9812973
  0.4  2          0.8               0.75       150      0.9978643  1     0.9786306
  0.4  2          0.8               1.00        50      0.9985600  1     0.9839640
  0.4  2          0.8               1.00       100      0.9980089  1     0.9812973
  0.4  2          0.8               1.00       150      0.9976533  1     0.9812973
  0.4  3          0.6               0.50        50      1.0000000  1     0.9839640
  0.4  3          0.6               0.50       100      1.0000000  1     0.9812613
  0.4  3          0.6               0.50       150      0.9996089  1     0.9785946
  0.4  3          0.6               0.75        50      0.9996444  1     0.9812973
  0.4  3          0.6               0.75       100      0.9988622  1     0.9786306
  0.4  3          0.6               0.75       150      0.9988978  1     0.9786306
  0.4  3          0.6               1.00        50      1.0000000  1     0.9812252
  0.4  3          0.6               1.00       100      0.9997867  1     0.9866306
  0.4  3          0.6               1.00       150      0.9997867  1     0.9839640
  0.4  3          0.8               0.50        50      0.9954133  1     0.9785946
  0.4  3          0.8               0.50       100      0.9952711  1     0.9813333
  0.4  3          0.8               0.50       150      0.9962311  1     0.9786667
  0.4  3          0.8               0.75        50      0.9997867  1     0.9812973
  0.4  3          0.8               0.75       100      0.9989689  1     0.9812973
  0.4  3          0.8               0.75       150      0.9986489  1     0.9812973
  0.4  3          0.8               1.00        50      0.9979378  1     0.9839640
  0.4  3          0.8               1.00       100      0.9969067  1     0.9812973
  0.4  3          0.8               1.00       150      0.9978311  1     0.9812973

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 50, max_depth = 3, eta = 0.3, gamma = 0, colsample_bytree = 0.6, min_child_weight = 1 and subsample = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     50.0      1.1
  positive      0.0     48.9
                            
 Accuracy (average) : 0.9893

[1] "TEST accuracy: 0.989304812834225"
[1] "TEST +precision: 1"
[1] "TEST -precision: 0.979057591623037"
[1] "TEST specifity: 1"
[1] "TEST sensitivity: 0.978609625668449"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        1        1
            positive        8      121
[1] "TEST accuracy: 0.931297709923664"
[1] "TEST +precision: 0.937984496124031"
[1] "TEST -precision: 0.5"
[1] "TEST specifity: 0.111111111111111"
[1] "TEST sensitivity: 0.991803278688525"

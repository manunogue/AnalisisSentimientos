[1] "DATASET NAME: Diamantes_Bi_IR_2"
[1] "TRAIN INSTANCES: 566"
[1] "TEST INSTANCES: 131"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 2.76247811317444"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

566 samples
947 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 452, 453, 453, 453, 453 
Resampling results:

  ROC  Sens  Spec
  1    1     1   

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     34.5      0.0
  positive      0.0     65.5
                       
 Accuracy (average) : 1

[1] "TEST accuracy: 1"
[1] "TEST +precision: 1"
[1] "TEST -precision: 1"
[1] "TEST specifity: 1"
[1] "TEST sensitivity: 1"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        0        0
            positive        6      125
[1] "TEST accuracy: 0.954198473282443"
[1] "TEST +precision: 0.954198473282443"
[1] "TEST -precision: NaN"
[1] "TEST specifity: 0"
[1] "TEST sensitivity: 1"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 1.57218639850616"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

566 samples
947 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 452, 453, 453, 453, 453 
Resampling results across tuning parameters:

  C      M  ROC        Sens       Spec     
  0.010  1  0.9692197  0.9179487  0.9919279
  0.010  2  0.9672562  0.9179487  0.9811532
  0.010  3  0.9653629  0.9179487  0.9730450
  0.255  1  0.9745211  0.9333333  0.9946306
  0.255  2  0.9727309  0.9333333  0.9865586
  0.255  3  0.9710455  0.9333333  0.9784505
  0.500  1  0.9745211  0.9333333  0.9946306
  0.500  2  0.9757736  0.9333333  0.9892252
  0.500  3  0.9743959  0.9333333  0.9811171

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.5 and M = 2.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     32.2      0.7
  positive      2.3     64.8
                          
 Accuracy (average) : 0.97

[1] "TEST accuracy: 0.969964664310954"
[1] "TEST +precision: 0.965789473684211"
[1] "TEST -precision: 0.978494623655914"
[1] "TEST specifity: 0.933333333333333"
[1] "TEST sensitivity: 0.98921832884097"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        0        4
            positive        6      121
[1] "TEST accuracy: 0.923664122137405"
[1] "TEST +precision: 0.952755905511811"
[1] "TEST -precision: 0"
[1] "TEST specifity: 0"
[1] "TEST sensitivity: 0.968"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 2.85338686704636"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

566 samples
947 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 452, 453, 453, 453, 453 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens       Spec     
  0.3  1          0.6               0.50        50      0.9679238  0.7589744  0.9892252
  0.3  1          0.6               0.50       100      0.9820670  0.9128205  0.9973333
  0.3  1          0.6               0.50       150      0.9900596  0.9487179  0.9973333
  0.3  1          0.6               0.75        50      0.9672793  0.8051282  0.9973333
  0.3  1          0.6               0.75       100      0.9885969  0.9282051  0.9919279
  0.3  1          0.6               0.75       150      0.9896364  0.9641026  0.9919279
  0.3  1          0.6               1.00        50      0.9681788  0.8205128  0.9973333
  0.3  1          0.6               1.00       100      0.9885295  0.9384615  0.9973333
  0.3  1          0.6               1.00       150      0.9887374  0.9641026  0.9973333
  0.3  1          0.8               0.50        50      0.9670820  0.7538462  0.9919279
  0.3  1          0.8               0.50       100      0.9785105  0.8974359  0.9919279
  0.3  1          0.8               0.50       150      0.9834109  0.9538462  0.9919279
  0.3  1          0.8               0.75        50      0.9662356  0.7846154  0.9892613
  0.3  1          0.8               0.75       100      0.9877708  0.9384615  0.9919279
  0.3  1          0.8               0.75       150      0.9907424  0.9641026  0.9946306
  0.3  1          0.8               1.00        50      0.9598194  0.7948718  0.9973333
  0.3  1          0.8               1.00       100      0.9888713  0.9384615  0.9973333
  0.3  1          0.8               1.00       150      0.9890792  0.9641026  0.9973333
  0.3  2          0.6               0.50        50      0.9857131  0.9179487  0.9946667
  0.3  2          0.6               0.50       100      0.9876757  0.9538462  0.9892613
  0.3  2          0.6               0.50       150      0.9861585  0.9692308  0.9865586
  0.3  2          0.6               0.75        50      0.9893611  0.9384615  0.9973333
  0.3  2          0.6               0.75       100      0.9911629  0.9794872  0.9973333
  0.3  2          0.6               0.75       150      0.9914401  0.9794872  0.9946667
  0.3  2          0.6               1.00        50      0.9905271  0.9384615  0.9973333
  0.3  2          0.6               1.00       100      0.9925341  0.9794872  0.9973333
  0.3  2          0.6               1.00       150      0.9930903  0.9794872  0.9973333
  0.3  2          0.8               0.50        50      0.9761044  0.8666667  0.9892613
  0.3  2          0.8               0.50       100      0.9806754  0.9384615  0.9892613
  0.3  2          0.8               0.50       150      0.9798512  0.9384615  0.9865946
  0.3  2          0.8               0.75        50      0.9901880  0.9179487  0.9973333
  0.3  2          0.8               0.75       100      0.9917154  0.9794872  0.9973333
  0.3  2          0.8               0.75       150      0.9919908  0.9794872  0.9919640
  0.3  2          0.8               1.00        50      0.9899108  0.9384615  0.9973333
  0.3  2          0.8               1.00       100      0.9920610  0.9794872  0.9973333
  0.3  2          0.8               1.00       150      0.9918522  0.9794872  0.9973333
  0.3  3          0.6               0.50        50      0.9800522  0.8820513  0.9919640
  0.3  3          0.6               0.50       100      0.9800947  0.9282051  0.9919640
  0.3  3          0.6               0.50       150      0.9802000  0.9282051  0.9892613
  0.3  3          0.6               0.75        50      0.9910908  0.9641026  0.9973333
  0.3  3          0.6               0.75       100      0.9919926  0.9794872  0.9946667
  0.3  3          0.6               0.75       150      0.9914364  0.9794872  0.9946667
  0.3  3          0.6               1.00        50      0.9912996  0.9641026  0.9973333
  0.3  3          0.6               1.00       100      0.9906048  0.9794872  0.9973333
  0.3  3          0.6               1.00       150      0.9903941  0.9794872  0.9946667
  0.3  3          0.8               0.50        50      0.9894322  0.9179487  0.9946667
  0.3  3          0.8               0.50       100      0.9924084  0.9487179  0.9946667
  0.3  3          0.8               0.50       150      0.9917246  0.9641026  0.9946667
  0.3  3          0.8               0.75        50      0.9923326  0.9641026  0.9973333
  0.3  3          0.8               0.75       100      0.9921294  0.9794872  0.9946667
  0.3  3          0.8               0.75       150      0.9928889  0.9794872  0.9946667
  0.3  3          0.8               1.00        50      0.9913569  0.9641026  0.9973333
  0.3  3          0.8               1.00       100      0.9918429  0.9794872  0.9973333
  0.3  3          0.8               1.00       150      0.9921164  0.9794872  0.9919640
  0.4  1          0.6               0.50        50      0.9659852  0.8307692  0.9865946
  0.4  1          0.6               0.50       100      0.9810095  0.9128205  0.9920000
  0.4  1          0.6               0.50       150      0.9840018  0.9435897  0.9920000
  0.4  1          0.6               0.75        50      0.9816817  0.8307692  0.9973333
  0.4  1          0.6               0.75       100      0.9898443  0.9487179  0.9973333
  0.4  1          0.6               0.75       150      0.9905355  0.9641026  0.9973333
  0.4  1          0.6               1.00        50      0.9810973  0.8461538  0.9973333
  0.4  1          0.6               1.00       100      0.9889406  0.9487179  0.9973333
  0.4  1          0.6               1.00       150      0.9892178  0.9641026  0.9946306
  0.4  1          0.8               0.50        50      0.9756609  0.8153846  0.9919279
  0.4  1          0.8               0.50       100      0.9859469  0.9384615  0.9946306
  0.4  1          0.8               0.50       150      0.9871933  0.9538462  0.9865586
  0.4  1          0.8               0.75        50      0.9818166  0.8307692  0.9946306
  0.4  1          0.8               0.75       100      0.9905927  0.9641026  0.9946306
  0.4  1          0.8               0.75       150      0.9908810  0.9641026  0.9946306
  0.4  1          0.8               1.00        50      0.9816170  0.8564103  0.9973333
  0.4  1          0.8               1.00       100      0.9889406  0.9641026  0.9973333
  0.4  1          0.8               1.00       150      0.9890811  0.9641026  0.9946306
  0.4  2          0.6               0.50        50      0.9823608  0.8564103  0.9865586
  0.4  2          0.6               0.50       100      0.9847697  0.9384615  0.9865586
  0.4  2          0.6               0.50       150      0.9838124  0.9384615  0.9865586
  0.4  2          0.6               0.75        50      0.9903423  0.9487179  0.9973333
  0.4  2          0.6               0.75       100      0.9904089  0.9641026  0.9973333
  0.4  2          0.6               0.75       150      0.9912941  0.9794872  0.9892973
  0.4  2          0.6               1.00        50      0.9904671  0.9641026  0.9973333
  0.4  2          0.6               1.00       100      0.9927521  0.9794872  0.9973333
  0.4  2          0.6               1.00       150      0.9919871  0.9794872  0.9973333
  0.4  2          0.8               0.50        50      0.9787937  0.8666667  0.9919640
  0.4  2          0.8               0.50       100      0.9812908  0.9333333  0.9838919
  0.4  2          0.8               0.50       150      0.9817653  0.9487179  0.9838919
  0.4  2          0.8               0.75        50      0.9911610  0.9487179  0.9973333
  0.4  2          0.8               0.75       100      0.9921294  0.9794872  0.9973333
  0.4  2          0.8               0.75       150      0.9914327  0.9794872  0.9946667
  0.4  2          0.8               1.00        50      0.9917782  0.9487179  0.9973333
  0.4  2          0.8               1.00       100      0.9924749  0.9794872  0.9946306
  0.4  2          0.8               1.00       150      0.9927466  0.9794872  0.9919640
  0.4  3          0.6               0.50        50      0.9830136  0.9435897  0.9892973
  0.4  3          0.6               0.50       100      0.9852913  0.9589744  0.9892973
  0.4  3          0.6               0.50       150      0.9846288  0.9589744  0.9919640
  0.4  3          0.6               0.75        50      0.9914382  0.9794872  0.9973333
  0.4  3          0.6               0.75       100      0.9919187  0.9794872  0.9946667
  0.4  3          0.6               0.75       150      0.9928113  0.9794872  0.9946667
  0.4  3          0.6               1.00        50      0.9921903  0.9794872  0.9946306
  0.4  3          0.6               1.00       100      0.9926708  0.9794872  0.9919279
  0.4  3          0.6               1.00       150      0.9926708  0.9794872  0.9919640
  0.4  3          0.8               0.50        50      0.9752451  0.9179487  0.9919640
  0.4  3          0.8               0.50       100      0.9762037  0.9230769  0.9892613
  0.4  3          0.8               0.50       150      0.9757237  0.9230769  0.9838559
  0.4  3          0.8               0.75        50      0.9884828  0.9641026  0.9973333
  0.4  3          0.8               0.75       100      0.9894091  0.9641026  0.9946667
  0.4  3          0.8               0.75       150      0.9887762  0.9641026  0.9946667
  0.4  3          0.8               1.00        50      0.9925396  0.9794872  0.9973333
  0.4  3          0.8               1.00       100      0.9933638  0.9794872  0.9919640
  0.4  3          0.8               1.00       150      0.9923918  0.9794872  0.9946667

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 100, max_depth = 3, eta = 0.4, gamma = 0, colsample_bytree = 0.8, min_child_weight = 1 and subsample = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     33.7      0.5
  positive      0.7     65.0
                            
 Accuracy (average) : 0.9876

[1] "TEST accuracy: 0.987632508833922"
[1] "TEST +precision: 0.989247311827957"
[1] "TEST -precision: 0.984536082474227"
[1] "TEST specifity: 0.979487179487179"
[1] "TEST sensitivity: 0.991913746630728"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        0        3
            positive        6      122
[1] "TEST accuracy: 0.931297709923664"
[1] "TEST +precision: 0.953125"
[1] "TEST -precision: 0"
[1] "TEST specifity: 0"
[1] "TEST sensitivity: 0.976"

[1] "DATASET NAME: Carmela_Bi_IR_5"
[1] "TRAIN INSTANCES: 360"
[1] "TEST INSTANCES: 103"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 2.44000196456909"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

360 samples
967 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 288, 288, 288, 288, 288 
Resampling results:

  ROC        Sens  Spec     
  0.9852632  0.92  0.9964912

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     19.2      0.3
  positive      1.7     78.9
                            
 Accuracy (average) : 0.9806

[1] "TEST accuracy: 0.980555555555556"
[1] "TEST +precision: 0.979310344827586"
[1] "TEST -precision: 0.985714285714286"
[1] "TEST specifity: 0.92"
[1] "TEST sensitivity: 0.996491228070175"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        5        0
            positive       11       87
[1] "TEST accuracy: 0.893203883495146"
[1] "TEST +precision: 0.887755102040816"
[1] "TEST -precision: 1"
[1] "TEST specifity: 0.3125"
[1] "TEST sensitivity: 1"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 1.27861103216807"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

360 samples
967 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 288, 288, 288, 288, 288 
Resampling results across tuning parameters:

  C      M  ROC        Sens       Spec     
  0.010  1  0.6754386  0.3466667  0.9894737
  0.010  2  0.6768421  0.3466667  0.9894737
  0.010  3  0.6626901  0.3200000  0.9929825
  0.255  1  0.7685380  0.5066667  0.9859649
  0.255  2  0.7683041  0.5066667  0.9894737
  0.255  3  0.7715789  0.5066667  0.9929825
  0.500  1  0.8029240  0.5733333  0.9859649
  0.500  2  0.8029240  0.5733333  0.9894737
  0.500  3  0.7771930  0.5066667  0.9894737

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.5 and M = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     11.9      1.1
  positive      8.9     78.1
                         
 Accuracy (average) : 0.9

[1] "TEST accuracy: 0.9"
[1] "TEST +precision: 0.89776357827476"
[1] "TEST -precision: 0.914893617021277"
[1] "TEST specifity: 0.573333333333333"
[1] "TEST sensitivity: 0.985964912280702"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        1        1
            positive       15       86
[1] "TEST accuracy: 0.844660194174757"
[1] "TEST +precision: 0.851485148514851"
[1] "TEST -precision: 0.5"
[1] "TEST specifity: 0.0625"
[1] "TEST sensitivity: 0.988505747126437"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 2.03972106774648"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

360 samples
967 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 288, 288, 288, 288, 288 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens       Spec     
  0.3  1          0.6               0.50        50      0.8387135  0.3466667  0.9894737
  0.3  1          0.6               0.50       100      0.8361404  0.4933333  0.9859649
  0.3  1          0.6               0.50       150      0.8315789  0.4933333  0.9859649
  0.3  1          0.6               0.75        50      0.8416374  0.3866667  0.9964912
  0.3  1          0.6               0.75       100      0.8670175  0.5200000  0.9894737
  0.3  1          0.6               0.75       150      0.8663158  0.5466667  0.9894737
  0.3  1          0.6               1.00        50      0.8518129  0.4266667  0.9964912
  0.3  1          0.6               1.00       100      0.8743860  0.4933333  0.9894737
  0.3  1          0.6               1.00       150      0.8798830  0.5466667  0.9894737
  0.3  1          0.8               0.50        50      0.8300585  0.4266667  0.9824561
  0.3  1          0.8               0.50       100      0.8378947  0.4800000  0.9824561
  0.3  1          0.8               0.50       150      0.8474854  0.5200000  0.9824561
  0.3  1          0.8               0.75        50      0.8185965  0.4000000  0.9894737
  0.3  1          0.8               0.75       100      0.8497076  0.4933333  0.9859649
  0.3  1          0.8               0.75       150      0.8581287  0.4933333  0.9859649
  0.3  1          0.8               1.00        50      0.8364912  0.4266667  0.9964912
  0.3  1          0.8               1.00       100      0.8666667  0.4666667  0.9929825
  0.3  1          0.8               1.00       150      0.8630409  0.5200000  0.9894737
  0.3  2          0.6               0.50        50      0.8460819  0.4400000  0.9824561
  0.3  2          0.6               0.50       100      0.8487719  0.4400000  0.9789474
  0.3  2          0.6               0.50       150      0.8596491  0.4800000  0.9789474
  0.3  2          0.6               0.75        50      0.8581287  0.4933333  0.9894737
  0.3  2          0.6               0.75       100      0.8529825  0.5466667  0.9824561
  0.3  2          0.6               0.75       150      0.8581287  0.5466667  0.9824561
  0.3  2          0.6               1.00        50      0.8785965  0.5066667  0.9929825
  0.3  2          0.6               1.00       100      0.8759064  0.5866667  0.9859649
  0.3  2          0.6               1.00       150      0.8721637  0.5866667  0.9824561
  0.3  2          0.8               0.50        50      0.8592982  0.4400000  0.9859649
  0.3  2          0.8               0.50       100      0.8596491  0.4533333  0.9754386
  0.3  2          0.8               0.50       150      0.8604678  0.4800000  0.9929825
  0.3  2          0.8               0.75        50      0.8770760  0.5200000  0.9894737
  0.3  2          0.8               0.75       100      0.8814035  0.5733333  0.9789474
  0.3  2          0.8               0.75       150      0.8809357  0.5733333  0.9929825
  0.3  2          0.8               1.00        50      0.8775439  0.5600000  0.9894737
  0.3  2          0.8               1.00       100      0.8792982  0.5866667  0.9929825
  0.3  2          0.8               1.00       150      0.8789474  0.5866667  0.9859649
  0.3  3          0.6               0.50        50      0.8492398  0.4133333  0.9789474
  0.3  3          0.6               0.50       100      0.8543860  0.4666667  0.9824561
  0.3  3          0.6               0.50       150      0.8580117  0.4800000  0.9754386
  0.3  3          0.6               0.75        50      0.8749708  0.5466667  0.9894737
  0.3  3          0.6               0.75       100      0.8721637  0.5466667  0.9894737
  0.3  3          0.6               0.75       150      0.8864327  0.5600000  0.9824561
  0.3  3          0.6               1.00        50      0.8857310  0.5866667  0.9929825
  0.3  3          0.6               1.00       100      0.8780117  0.5866667  0.9929825
  0.3  3          0.6               1.00       150      0.8815205  0.6000000  0.9929825
  0.3  3          0.8               0.50        50      0.8494737  0.4533333  0.9824561
  0.3  3          0.8               0.50       100      0.8632749  0.4800000  0.9859649
  0.3  3          0.8               0.50       150      0.8602339  0.4800000  0.9929825
  0.3  3          0.8               0.75        50      0.8704094  0.5600000  0.9894737
  0.3  3          0.8               0.75       100      0.8697076  0.5600000  0.9964912
  0.3  3          0.8               0.75       150      0.8755556  0.5600000  0.9964912
  0.3  3          0.8               1.00        50      0.8859649  0.5866667  0.9929825
  0.3  3          0.8               1.00       100      0.8818713  0.5866667  0.9859649
  0.3  3          0.8               1.00       150      0.8849123  0.6133333  0.9859649
  0.4  1          0.6               0.50        50      0.8299415  0.3600000  0.9859649
  0.4  1          0.6               0.50       100      0.8292398  0.4000000  0.9824561
  0.4  1          0.6               0.50       150      0.8323977  0.4133333  0.9789474
  0.4  1          0.6               0.75        50      0.8367251  0.4400000  0.9894737
  0.4  1          0.6               0.75       100      0.8582456  0.4933333  0.9859649
  0.4  1          0.6               0.75       150      0.8598830  0.4933333  0.9859649
  0.4  1          0.6               1.00        50      0.8580117  0.4533333  0.9929825
  0.4  1          0.6               1.00       100      0.8665497  0.5200000  0.9894737
  0.4  1          0.6               1.00       150      0.8659649  0.5466667  0.9894737
  0.4  1          0.8               0.50        50      0.8184795  0.3733333  0.9789474
  0.4  1          0.8               0.50       100      0.8364912  0.4400000  0.9719298
  0.4  1          0.8               0.50       150      0.8392982  0.4400000  0.9789474
  0.4  1          0.8               0.75        50      0.8595322  0.4933333  0.9929825
  0.4  1          0.8               0.75       100      0.8684211  0.5466667  0.9894737
  0.4  1          0.8               0.75       150      0.8700585  0.5733333  0.9859649
  0.4  1          0.8               1.00        50      0.8609357  0.4533333  0.9929825
  0.4  1          0.8               1.00       100      0.8614035  0.5200000  0.9859649
  0.4  1          0.8               1.00       150      0.8647953  0.5466667  0.9894737
  0.4  2          0.6               0.50        50      0.8463158  0.4133333  0.9789474
  0.4  2          0.6               0.50       100      0.8672515  0.4400000  0.9789474
  0.4  2          0.6               0.50       150      0.8714620  0.4933333  0.9859649
  0.4  2          0.6               0.75        50      0.8774269  0.5466667  0.9859649
  0.4  2          0.6               0.75       100      0.8797661  0.5733333  0.9929825
  0.4  2          0.6               0.75       150      0.8881871  0.5733333  0.9929825
  0.4  2          0.6               1.00        50      0.8763743  0.5866667  0.9929825
  0.4  2          0.6               1.00       100      0.8743860  0.5866667  0.9824561
  0.4  2          0.6               1.00       150      0.8807018  0.5866667  0.9929825
  0.4  2          0.8               0.50        50      0.8433918  0.4133333  0.9789474
  0.4  2          0.8               0.50       100      0.8450292  0.4533333  0.9894737
  0.4  2          0.8               0.50       150      0.8450292  0.4666667  0.9754386
  0.4  2          0.8               0.75        50      0.8645614  0.5333333  0.9859649
  0.4  2          0.8               0.75       100      0.8666667  0.5333333  0.9824561
  0.4  2          0.8               0.75       150      0.8706433  0.5333333  0.9824561
  0.4  2          0.8               1.00        50      0.8771930  0.5866667  0.9859649
  0.4  2          0.8               1.00       100      0.8846784  0.5866667  0.9824561
  0.4  2          0.8               1.00       150      0.8851462  0.6133333  0.9824561
  0.4  3          0.6               0.50        50      0.8548538  0.3866667  0.9824561
  0.4  3          0.6               0.50       100      0.8550877  0.4000000  0.9719298
  0.4  3          0.6               0.50       150      0.8588304  0.4266667  0.9754386
  0.4  3          0.6               0.75        50      0.8756725  0.5466667  0.9929825
  0.4  3          0.6               0.75       100      0.8864327  0.5600000  0.9859649
  0.4  3          0.6               0.75       150      0.8829240  0.5733333  0.9929825
  0.4  3          0.6               1.00        50      0.8873684  0.5866667  0.9929825
  0.4  3          0.6               1.00       100      0.8906433  0.5866667  0.9929825
  0.4  3          0.6               1.00       150      0.8894737  0.6000000  0.9929825
  0.4  3          0.8               0.50        50      0.8643275  0.5200000  0.9894737
  0.4  3          0.8               0.50       100      0.8723977  0.5200000  0.9824561
  0.4  3          0.8               0.50       150      0.8775439  0.5200000  0.9859649
  0.4  3          0.8               0.75        50      0.8721637  0.5200000  0.9894737
  0.4  3          0.8               0.75       100      0.8730994  0.5333333  0.9894737
  0.4  3          0.8               0.75       150      0.8747368  0.5200000  0.9964912
  0.4  3          0.8               1.00        50      0.8826901  0.6000000  0.9894737
  0.4  3          0.8               1.00       100      0.8866667  0.6133333  0.9929825
  0.4  3          0.8               1.00       150      0.8885380  0.6133333  0.9964912

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 100, max_depth = 3, eta = 0.4, gamma = 0, colsample_bytree = 0.6, min_child_weight = 1 and subsample = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     12.2      0.6
  positive      8.6     78.6
                            
 Accuracy (average) : 0.9083

[1] "TEST accuracy: 0.908333333333333"
[1] "TEST +precision: 0.901273885350318"
[1] "TEST -precision: 0.956521739130435"
[1] "TEST specifity: 0.586666666666667"
[1] "TEST sensitivity: 0.992982456140351"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        1        0
            positive       15       87
[1] "TEST accuracy: 0.854368932038835"
[1] "TEST +precision: 0.852941176470588"
[1] "TEST -precision: 1"
[1] "TEST specifity: 0.0625"
[1] "TEST sensitivity: 1"

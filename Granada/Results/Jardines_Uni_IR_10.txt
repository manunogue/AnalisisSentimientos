[1] "DATASET NAME: Jardines_Uni_IR_10"
[1] "TRAIN INSTANCES: 733"
[1] "TEST INSTANCES: 223"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 2.78953695297241"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

733 samples
736 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 586, 587, 586, 586, 587 
Resampling results:

  ROC  Sens   Spec
  1    0.975  1   

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     10.6      0.0
  positive      0.3     89.1
                            
 Accuracy (average) : 0.9973

[1] "TEST accuracy: 0.997271487039563"
[1] "TEST +precision: 0.996946564885496"
[1] "TEST -precision: 1"
[1] "TEST specifity: 0.975"
[1] "TEST sensitivity: 1"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        1        0
            positive        3      219
[1] "TEST accuracy: 0.986547085201794"
[1] "TEST +precision: 0.986486486486487"
[1] "TEST -precision: 1"
[1] "TEST specifity: 0.25"
[1] "TEST sensitivity: 1"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 1.28785656690598"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

733 samples
736 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 586, 587, 586, 587, 586 
Resampling results across tuning parameters:

  C      M  ROC        Sens    Spec     
  0.010  1  0.8890975  0.7750  0.9770053
  0.010  2  0.8892418  0.7750  0.9754668
  0.010  3  0.8858360  0.7750  0.9754786
  0.255  1  0.9569319  0.9250  0.9754786
  0.255  2  0.9572655  0.9250  0.9754786
  0.255  3  0.9547203  0.9250  0.9754786
  0.500  1  0.9725675  0.9625  0.9754786
  0.500  2  0.9729973  0.9625  0.9754786
  0.500  3  0.9547203  0.9250  0.9754786

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.5 and M = 2.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     10.5      2.2
  positive      0.4     86.9
                            
 Accuracy (average) : 0.9741

[1] "TEST accuracy: 0.974079126875853"
[1] "TEST +precision: 0.9953125"
[1] "TEST -precision: 0.827956989247312"
[1] "TEST specifity: 0.9625"
[1] "TEST sensitivity: 0.975497702909648"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        2        5
            positive        2      214
[1] "TEST accuracy: 0.968609865470852"
[1] "TEST +precision: 0.990740740740741"
[1] "TEST -precision: 0.285714285714286"
[1] "TEST specifity: 0.5"
[1] "TEST sensitivity: 0.977168949771689"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 2.96649976571401"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

733 samples
736 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 587, 586, 587, 586, 586 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens    Spec     
  0.3  1          0.6               0.50        50      0.9839607  0.6625  0.9953846
  0.3  1          0.6               0.50       100      0.9958764  0.7625  0.9938579
  0.3  1          0.6               0.50       150      0.9980784  0.9625  0.9908045
  0.3  1          0.6               0.75        50      0.9870200  0.6625  0.9953964
  0.3  1          0.6               0.75       100      0.9963528  0.9125  0.9938579
  0.3  1          0.6               0.75       150      0.9974060  1.0000  0.9923312
  0.3  1          0.6               1.00        50      0.9888454  0.6500  0.9953964
  0.3  1          0.6               1.00       100      0.9964504  0.8625  0.9953964
  0.3  1          0.6               1.00       150      0.9973128  0.9750  0.9953964
  0.3  1          0.8               0.50        50      0.9815113  0.6875  0.9892777
  0.3  1          0.8               0.50       100      0.9943438  0.8375  0.9877393
  0.3  1          0.8               0.50       150      0.9956878  0.9375  0.9892660
  0.3  1          0.8               0.75        50      0.9886069  0.6875  0.9923312
  0.3  1          0.8               0.75       100      0.9972196  0.9500  0.9953964
  0.3  1          0.8               0.75       150      0.9984637  0.9750  0.9953964
  0.3  1          0.8               1.00        50      0.9896044  0.6375  0.9969231
  0.3  1          0.8               1.00       100      0.9963542  0.8625  0.9938579
  0.3  1          0.8               1.00       150      0.9977929  0.9250  0.9938696
  0.3  2          0.6               0.50        50      0.9958786  0.8750  0.9938579
  0.3  2          0.6               0.50       100      0.9968357  1.0000  0.9907927
  0.3  2          0.6               0.50       150      0.9973187  1.0000  0.9907927
  0.3  2          0.6               0.75        50      0.9978890  0.9750  0.9953964
  0.3  2          0.6               0.75       100      0.9995192  1.0000  0.9923312
  0.3  2          0.6               0.75       150      0.9998077  1.0000  0.9923312
  0.3  2          0.6               1.00        50      0.9985592  0.9375  0.9953964
  0.3  2          0.6               1.00       100      0.9999038  1.0000  0.9969348
  0.3  2          0.6               1.00       150      0.9999038  1.0000  0.9953964
  0.3  2          0.8               0.50        50      0.9968372  0.9125  0.9938696
  0.3  2          0.8               0.50       100      0.9989445  1.0000  0.9892777
  0.3  2          0.8               0.50       150      0.9994253  1.0000  0.9938696
  0.3  2          0.8               0.75        50      0.9987500  0.9250  0.9938579
  0.3  2          0.8               0.75       100      0.9997115  1.0000  0.9938579
  0.3  2          0.8               0.75       150      0.9998077  1.0000  0.9908045
  0.3  2          0.8               1.00        50      0.9981745  0.9750  0.9953964
  0.3  2          0.8               1.00       100      0.9997115  1.0000  0.9969348
  0.3  2          0.8               1.00       150      0.9999038  1.0000  0.9954081
  0.3  3          0.6               0.50        50      0.9979822  0.9625  0.9938579
  0.3  3          0.6               0.50       100      0.9991346  1.0000  0.9923194
  0.3  3          0.6               0.50       150      0.9991346  1.0000  0.9907927
  0.3  3          0.6               0.75        50      0.9992308  1.0000  0.9953964
  0.3  3          0.6               0.75       100      0.9993269  1.0000  0.9923312
  0.3  3          0.6               0.75       150      0.9993269  1.0000  0.9923312
  0.3  3          0.6               1.00        50      0.9999038  1.0000  0.9923429
  0.3  3          0.6               1.00       100      1.0000000  1.0000  0.9923429
  0.3  3          0.6               1.00       150      1.0000000  1.0000  0.9938696
  0.3  3          0.8               0.50        50      0.9996154  0.9750  0.9969231
  0.3  3          0.8               0.50       100      0.9999038  1.0000  0.9953964
  0.3  3          0.8               0.50       150      1.0000000  1.0000  0.9953964
  0.3  3          0.8               0.75        50      0.9999038  1.0000  0.9938696
  0.3  3          0.8               0.75       100      1.0000000  1.0000  0.9938696
  0.3  3          0.8               0.75       150      1.0000000  1.0000  0.9938696
  0.3  3          0.8               1.00        50      0.9995192  1.0000  0.9938579
  0.3  3          0.8               1.00       100      0.9999038  1.0000  0.9938696
  0.3  3          0.8               1.00       150      1.0000000  1.0000  0.9953964
  0.4  1          0.6               0.50        50      0.9895574  0.7500  0.9861891
  0.4  1          0.6               0.50       100      0.9956841  0.9000  0.9923312
  0.4  1          0.6               0.50       150      0.9960680  0.9500  0.9923312
  0.4  1          0.6               0.75        50      0.9937757  0.7000  0.9923429
  0.4  1          0.6               0.75       100      0.9977958  0.9750  0.9953964
  0.4  1          0.6               0.75       150      0.9980828  1.0000  0.9923312
  0.4  1          0.6               1.00        50      0.9942491  0.7625  0.9969231
  0.4  1          0.6               1.00       100      0.9975051  0.9375  0.9938579
  0.4  1          0.6               1.00       150      0.9986538  0.9750  0.9969231
  0.4  1          0.8               0.50        50      0.9914775  0.7375  0.9923429
  0.4  1          0.8               0.50       100      0.9964511  0.9000  0.9923429
  0.4  1          0.8               0.50       150      0.9963586  0.9750  0.9923429
  0.4  1          0.8               0.75        50      0.9923371  0.7250  0.9938579
  0.4  1          0.8               0.75       100      0.9977921  0.9375  0.9938579
  0.4  1          0.8               0.75       150      0.9980821  1.0000  0.9908045
  0.4  1          0.8               1.00        50      0.9938630  0.7500  0.9969231
  0.4  1          0.8               1.00       100      0.9971205  0.9625  0.9953964
  0.4  1          0.8               1.00       150      0.9987500  0.9750  0.9953964
  0.4  2          0.6               0.50        50      0.9977914  0.9125  0.9938579
  0.4  2          0.6               0.50       100      0.9986583  0.9875  0.9907927
  0.4  2          0.6               0.50       150      0.9989445  1.0000  0.9923312
  0.4  2          0.6               0.75        50      0.9990399  1.0000  0.9938579
  0.4  2          0.6               0.75       100      0.9999038  1.0000  0.9923312
  0.4  2          0.6               0.75       150      1.0000000  1.0000  0.9923312
  0.4  2          0.6               1.00        50      0.9995192  1.0000  0.9923312
  0.4  2          0.6               1.00       100      0.9997115  1.0000  0.9953964
  0.4  2          0.6               1.00       150      0.9999038  1.0000  0.9953964
  0.4  2          0.8               0.50        50      0.9986575  0.9625  0.9938579
  0.4  2          0.8               0.50       100      0.9991368  1.0000  0.9923312
  0.4  2          0.8               0.50       150      0.9997115  1.0000  0.9938696
  0.4  2          0.8               0.75        50      0.9989438  0.9750  0.9953846
  0.4  2          0.8               0.75       100      0.9991346  1.0000  0.9938696
  0.4  2          0.8               0.75       150      0.9991346  1.0000  0.9938696
  0.4  2          0.8               1.00        50      0.9988462  1.0000  0.9938696
  0.4  2          0.8               1.00       100      0.9996161  1.0000  0.9908045
  0.4  2          0.8               1.00       150      0.9998084  1.0000  0.9908045
  0.4  3          0.6               0.50        50      0.9988491  0.9750  0.9923312
  0.4  3          0.6               0.50       100      0.9994231  1.0000  0.9938579
  0.4  3          0.6               0.50       150      0.9994231  1.0000  0.9923312
  0.4  3          0.6               0.75        50      1.0000000  1.0000  0.9969231
  0.4  3          0.6               0.75       100      0.9999038  1.0000  0.9969231
  0.4  3          0.6               0.75       150      0.9999038  1.0000  0.9969231
  0.4  3          0.6               1.00        50      1.0000000  1.0000  0.9938696
  0.4  3          0.6               1.00       100      1.0000000  1.0000  0.9938696
  0.4  3          0.6               1.00       150      1.0000000  1.0000  0.9954081
  0.4  3          0.8               0.50        50      0.9975037  0.9375  0.9923312
  0.4  3          0.8               0.50       100      0.9981753  1.0000  0.9938579
  0.4  3          0.8               0.50       150      0.9983669  1.0000  0.9938696
  0.4  3          0.8               0.75        50      1.0000000  1.0000  0.9938696
  0.4  3          0.8               0.75       100      1.0000000  1.0000  0.9954081
  0.4  3          0.8               0.75       150      0.9998092  1.0000  0.9954081
  0.4  3          0.8               1.00        50      0.9999038  1.0000  0.9923429
  0.4  3          0.8               1.00       100      1.0000000  1.0000  0.9923429
  0.4  3          0.8               1.00       150      1.0000000  1.0000  0.9969348

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 50, max_depth = 3, eta = 0.4, gamma = 0, colsample_bytree = 0.6, min_child_weight = 1 and subsample = 0.75.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     10.9      0.3
  positive      0.0     88.8
                            
 Accuracy (average) : 0.9973

[1] "TEST accuracy: 0.997271487039563"
[1] "TEST +precision: 1"
[1] "TEST -precision: 0.975609756097561"
[1] "TEST specifity: 1"
[1] "TEST sensitivity: 0.996937212863706"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        1        0
            positive        3      219
[1] "TEST accuracy: 0.986547085201794"
[1] "TEST +precision: 0.986486486486487"
[1] "TEST -precision: 1"
[1] "TEST specifity: 0.25"
[1] "TEST sensitivity: 1"

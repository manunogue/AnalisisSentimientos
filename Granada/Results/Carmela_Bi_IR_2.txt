[1] "DATASET NAME: Carmela_Bi_IR_2"
[1] "TRAIN INSTANCES: 439"
[1] "TEST INSTANCES: 103"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 2.24030494689941"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

439 samples
967 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 351, 351, 352, 351, 351 
Resampling results:

  ROC  Sens  Spec
  1    1     1   

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     35.1      0.0
  positive      0.0     64.9
                       
 Accuracy (average) : 1

[1] "TEST accuracy: 1"
[1] "TEST +precision: 1"
[1] "TEST -precision: 1"
[1] "TEST specifity: 1"
[1] "TEST sensitivity: 1"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        1        0
            positive       15       87
[1] "TEST accuracy: 0.854368932038835"
[1] "TEST +precision: 0.852941176470588"
[1] "TEST -precision: 1"
[1] "TEST specifity: 0.0625"
[1] "TEST sensitivity: 1"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 1.44581688642502"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

439 samples
967 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 351, 351, 352, 351, 351 
Resampling results across tuning parameters:

  C      M  ROC        Sens       Spec     
  0.010  1  0.9562611  0.9083871  0.9684211
  0.010  2  0.9579023  0.9083871  0.9684211
  0.010  3  0.9465327  0.8819355  0.9508772
  0.255  1  0.9725750  0.9475269  0.9719298
  0.255  2  0.9746312  0.9475269  0.9789474
  0.255  3  0.9704056  0.9215054  0.9543860
  0.500  1  0.9712243  0.9475269  0.9824561
  0.500  2  0.9770081  0.9475269  0.9859649
  0.500  3  0.9720355  0.9215054  0.9614035

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.5 and M = 2.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     33.3      0.9
  positive      1.8     64.0
                            
 Accuracy (average) : 0.9727

[1] "TEST accuracy: 0.972665148063781"
[1] "TEST +precision: 0.972318339100346"
[1] "TEST -precision: 0.973333333333333"
[1] "TEST specifity: 0.948051948051948"
[1] "TEST sensitivity: 0.985964912280702"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        1        0
            positive       15       87
[1] "TEST accuracy: 0.854368932038835"
[1] "TEST +precision: 0.852941176470588"
[1] "TEST -precision: 1"
[1] "TEST specifity: 0.0625"
[1] "TEST sensitivity: 1"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 2.35823001464208"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

439 samples
967 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 351, 351, 351, 351, 352 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens       Spec     
  0.3  1          0.6               0.50        50      0.9359781  0.7012903  0.9929825
  0.3  1          0.6               0.50       100      0.9565723  0.8047312  0.9789474
  0.3  1          0.6               0.50       150      0.9670779  0.8498925  0.9719298
  0.3  1          0.6               0.75        50      0.9430372  0.7402151  0.9929825
  0.3  1          0.6               0.75       100      0.9573552  0.8240860  0.9929825
  0.3  1          0.6               0.75       150      0.9758781  0.8627957  0.9894737
  0.3  1          0.6               1.00        50      0.9359630  0.7402151  0.9964912
  0.3  1          0.6               1.00       100      0.9614469  0.8369892  0.9929825
  0.3  1          0.6               1.00       150      0.9776627  0.8632258  0.9894737
  0.3  1          0.8               0.50        50      0.9435937  0.6561290  0.9964912
  0.3  1          0.8               0.50       100      0.9576325  0.8369892  0.9789474
  0.3  1          0.8               0.50       150      0.9718846  0.8698925  0.9789474
  0.3  1          0.8               0.75        50      0.9485191  0.7722581  0.9964912
  0.3  1          0.8               0.75       100      0.9726636  0.8827957  0.9894737
  0.3  1          0.8               0.75       150      0.9887474  0.8961290  0.9859649
  0.3  1          0.8               1.00        50      0.9381702  0.7402151  0.9964912
  0.3  1          0.8               1.00       100      0.9614469  0.8369892  0.9929825
  0.3  1          0.8               1.00       150      0.9794171  0.8632258  0.9929825
  0.3  2          0.6               0.50        50      0.9565214  0.7789247  0.9824561
  0.3  2          0.6               0.50       100      0.9625184  0.8305376  0.9649123
  0.3  2          0.6               0.50       150      0.9607093  0.8632258  0.9719298
  0.3  2          0.6               0.75        50      0.9720223  0.8569892  0.9964912
  0.3  2          0.6               0.75       100      0.9808338  0.9092473  0.9859649
  0.3  2          0.6               0.75       150      0.9892058  0.9225806  0.9859649
  0.3  2          0.6               1.00        50      0.9631862  0.8240860  0.9964912
  0.3  2          0.6               1.00       100      0.9827165  0.8961290  0.9929825
  0.3  2          0.6               1.00       150      0.9801585  0.8961290  0.9859649
  0.3  2          0.8               0.50        50      0.9673703  0.7789247  0.9789474
  0.3  2          0.8               0.50       100      0.9678344  0.8698925  0.9754386
  0.3  2          0.8               0.50       150      0.9679287  0.8763441  0.9684211
  0.3  2          0.8               0.75        50      0.9630768  0.8369892  0.9894737
  0.3  2          0.8               0.75       100      0.9809979  0.8961290  0.9859649
  0.3  2          0.8               0.75       150      0.9798529  0.9025806  0.9754386
  0.3  2          0.8               1.00        50      0.9624373  0.8369892  0.9964912
  0.3  2          0.8               1.00       100      0.9821958  0.8961290  0.9894737
  0.3  2          0.8               1.00       150      0.9805980  0.8961290  0.9859649
  0.3  3          0.6               0.50        50      0.9533711  0.8176344  0.9719298
  0.3  3          0.6               0.50       100      0.9524128  0.8176344  0.9649123
  0.3  3          0.6               0.50       150      0.9595020  0.8369892  0.9614035
  0.3  3          0.6               0.75        50      0.9770043  0.8696774  0.9824561
  0.3  3          0.6               0.75       100      0.9761969  0.9090323  0.9754386
  0.3  3          0.6               0.75       150      0.9749406  0.9154839  0.9719298
  0.3  3          0.6               1.00        50      0.9804641  0.8632258  0.9894737
  0.3  3          0.6               1.00       100      0.9811828  0.8961290  0.9859649
  0.3  3          0.6               1.00       150      0.9808942  0.9090323  0.9859649
  0.3  3          0.8               0.50        50      0.9611696  0.8369892  0.9754386
  0.3  3          0.8               0.50       100      0.9633522  0.8563441  0.9684211
  0.3  3          0.8               0.50       150      0.9649783  0.8827957  0.9649123
  0.3  3          0.8               0.75        50      0.9800623  0.8825806  0.9859649
  0.3  3          0.8               0.75       100      0.9816620  0.9219355  0.9754386
  0.3  3          0.8               0.75       150      0.9809715  0.9412903  0.9754386
  0.3  3          0.8               1.00        50      0.9937351  0.9154839  0.9894737
  0.3  3          0.8               1.00       100      0.9974533  0.9419355  0.9859649
  0.3  3          0.8               1.00       150      0.9968006  0.9677419  0.9859649
  0.4  1          0.6               0.50        50      0.9301981  0.7335484  0.9929825
  0.4  1          0.6               0.50       100      0.9558668  0.8116129  0.9754386
  0.4  1          0.6               0.50       150      0.9657253  0.8503226  0.9719298
  0.4  1          0.6               0.75        50      0.9727731  0.7853763  0.9929825
  0.4  1          0.6               0.75       100      0.9865931  0.8825806  0.9929825
  0.4  1          0.6               0.75       150      0.9874194  0.9348387  0.9789474
  0.4  1          0.6               1.00        50      0.9491605  0.8047312  0.9929825
  0.4  1          0.6               1.00       100      0.9772024  0.8761290  0.9894737
  0.4  1          0.6               1.00       150      0.9822467  0.8961290  0.9894737
  0.4  1          0.8               0.50        50      0.9446218  0.7206452  0.9894737
  0.4  1          0.8               0.50       100      0.9599472  0.8176344  0.9789474
  0.4  1          0.8               0.50       150      0.9602981  0.8703226  0.9719298
  0.4  1          0.8               0.75        50      0.9765799  0.7724731  0.9964912
  0.4  1          0.8               0.75       100      0.9815035  0.9154839  0.9894737
  0.4  1          0.8               0.75       150      0.9846444  0.9419355  0.9824561
  0.4  1          0.8               1.00        50      0.9482400  0.8240860  0.9964912
  0.4  1          0.8               1.00       100      0.9776552  0.8632258  0.9894737
  0.4  1          0.8               1.00       150      0.9819015  0.8961290  0.9894737
  0.4  2          0.6               0.50        50      0.9543067  0.7853763  0.9789474
  0.4  2          0.6               0.50       100      0.9584116  0.8832258  0.9719298
  0.4  2          0.6               0.50       150      0.9583852  0.8832258  0.9789474
  0.4  2          0.6               0.75        50      0.9721694  0.8369892  0.9894737
  0.4  2          0.6               0.75       100      0.9754480  0.8892473  0.9859649
  0.4  2          0.6               0.75       150      0.9751839  0.9150538  0.9859649
  0.4  2          0.6               1.00        50      0.9800962  0.8761290  0.9964912
  0.4  2          0.6               1.00       100      0.9817544  0.8961290  0.9859649
  0.4  2          0.6               1.00       150      0.9810753  0.9090323  0.9824561
  0.4  2          0.8               0.50        50      0.9499510  0.8369892  0.9754386
  0.4  2          0.8               0.50       100      0.9621581  0.8434409  0.9649123
  0.4  2          0.8               0.50       150      0.9644482  0.8827957  0.9754386
  0.4  2          0.8               0.75        50      0.9756367  0.8698925  0.9894737
  0.4  2          0.8               0.75       100      0.9751556  0.8956989  0.9859649
  0.4  2          0.8               0.75       150      0.9766836  0.9086022  0.9754386
  0.4  2          0.8               1.00        50      0.9795925  0.8761290  0.9929825
  0.4  2          0.8               1.00       100      0.9809394  0.8961290  0.9859649
  0.4  2          0.8               1.00       150      0.9823656  0.9283871  0.9859649
  0.4  3          0.6               0.50        50      0.9512205  0.8369892  0.9649123
  0.4  3          0.6               0.50       100      0.9563290  0.8498925  0.9614035
  0.4  3          0.6               0.50       150      0.9565554  0.8498925  0.9649123
  0.4  3          0.6               0.75        50      0.9881268  0.9156989  0.9859649
  0.4  3          0.6               0.75       100      0.9904697  0.9544086  0.9754386
  0.4  3          0.6               0.75       150      0.9902434  0.9544086  0.9754386
  0.4  3          0.6               1.00        50      0.9907753  0.9161290  0.9894737
  0.4  3          0.6               1.00       100      0.9908885  0.9290323  0.9859649
  0.4  3          0.6               1.00       150      0.9917940  0.9354839  0.9824561
  0.4  3          0.8               0.50        50      0.9601075  0.8498925  0.9684211
  0.4  3          0.8               0.50       100      0.9603056  0.8627957  0.9649123
  0.4  3          0.8               0.50       150      0.9615507  0.8627957  0.9684211
  0.4  3          0.8               0.75        50      0.9747595  0.8827957  0.9824561
  0.4  3          0.8               0.75       100      0.9760800  0.9215054  0.9859649
  0.4  3          0.8               0.75       150      0.9766459  0.9215054  0.9789474
  0.4  3          0.8               1.00        50      0.9811545  0.8961290  0.9894737
  0.4  3          0.8               1.00       100      0.9814658  0.9090323  0.9859649
  0.4  3          0.8               1.00       150      0.9811187  0.9154839  0.9859649

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 100, max_depth = 3, eta = 0.3, gamma = 0, colsample_bytree = 0.8, min_child_weight = 1 and subsample = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     33.0      0.9
  positive      2.1     64.0
                            
 Accuracy (average) : 0.9704

[1] "TEST accuracy: 0.970387243735763"
[1] "TEST +precision: 0.968965517241379"
[1] "TEST -precision: 0.973154362416107"
[1] "TEST specifity: 0.941558441558442"
[1] "TEST sensitivity: 0.985964912280702"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        1        0
            positive       15       87
[1] "TEST accuracy: 0.854368932038835"
[1] "TEST +precision: 0.852941176470588"
[1] "TEST -precision: 1"
[1] "TEST specifity: 0.0625"
[1] "TEST sensitivity: 1"

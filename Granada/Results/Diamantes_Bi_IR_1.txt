[1] "DATASET NAME: Diamantes_Bi_IR_1"
[1] "TRAIN INSTANCES: 742"
[1] "TEST INSTANCES: 131"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 3.35775184631348"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

742 samples
947 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 594, 594, 594, 593, 593 
Resampling results:

  ROC  Sens  Spec
  1    1     1   

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative       50        0
  positive        0       50
                       
 Accuracy (average) : 1

[1] "TEST accuracy: 1"
[1] "TEST +precision: 1"
[1] "TEST -precision: 1"
[1] "TEST specifity: 1"
[1] "TEST sensitivity: 1"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        0        0
            positive        6      125
[1] "TEST accuracy: 0.954198473282443"
[1] "TEST +precision: 0.954198473282443"
[1] "TEST -precision: NaN"
[1] "TEST specifity: 0"
[1] "TEST sensitivity: 1"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 1.82808525164922"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

742 samples
947 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 593, 594, 594, 593, 594 
Resampling results across tuning parameters:

  C      M  ROC        Sens  Spec     
  0.010  1  0.9973153  1     0.9946306
  0.010  2  0.9972065  1     0.9919279
  0.010  3  0.9963331  1     0.9838559
  0.255  1  0.9973153  1     0.9946306
  0.255  2  0.9972065  1     0.9919279
  0.255  3  0.9963331  1     0.9838559
  0.500  1  0.9973153  1     0.9946306
  0.500  2  0.9972065  1     0.9919279
  0.500  3  0.9963331  1     0.9838559

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.01 and M = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     50.0      0.3
  positive      0.0     49.7
                            
 Accuracy (average) : 0.9973

[1] "TEST accuracy: 0.997304582210243"
[1] "TEST +precision: 1"
[1] "TEST -precision: 0.994638069705094"
[1] "TEST specifity: 1"
[1] "TEST sensitivity: 0.994609164420485"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        0        2
            positive        6      123
[1] "TEST accuracy: 0.938931297709924"
[1] "TEST +precision: 0.953488372093023"
[1] "TEST -precision: 0"
[1] "TEST specifity: 0"
[1] "TEST sensitivity: 0.984"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 3.57158486843109"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

742 samples
947 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 593, 593, 594, 594, 594 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens       Spec     
  0.3  1          0.6               0.50        50      0.9965332  0.9648649  0.9758198
  0.3  1          0.6               0.50       100      0.9976937  1.0000000  0.9892613
  0.3  1          0.6               0.50       150      0.9976216  1.0000000  0.9892613
  0.3  1          0.6               0.75        50      0.9965405  0.9810811  0.9892613
  0.3  1          0.6               0.75       100      0.9970090  1.0000000  0.9919640
  0.3  1          0.6               0.75       150      0.9975856  1.0000000  0.9919640
  0.3  1          0.6               1.00        50      0.9968649  0.9864865  0.9945946
  0.3  1          0.6               1.00       100      0.9977297  1.0000000  0.9972973
  0.3  1          0.6               1.00       150      0.9977297  1.0000000  0.9972973
  0.3  1          0.8               0.50        50      0.9916840  0.9837838  0.9758198
  0.3  1          0.8               0.50       100      0.9962508  1.0000000  0.9812252
  0.3  1          0.8               0.50       150      0.9980901  1.0000000  0.9865946
  0.3  1          0.8               0.75        50      0.9946467  0.9864865  0.9785586
  0.3  1          0.8               0.75       100      0.9980901  1.0000000  0.9919640
  0.3  1          0.8               0.75       150      0.9978018  1.0000000  0.9919640
  0.3  1          0.8               1.00        50      0.9960000  0.9676757  0.9919640
  0.3  1          0.8               1.00       100      0.9975856  1.0000000  0.9972973
  0.3  1          0.8               1.00       150      0.9975856  1.0000000  0.9972973
  0.3  2          0.6               0.50        50      0.9972252  1.0000000  0.9892613
  0.3  2          0.6               0.50       100      0.9975856  1.0000000  0.9945946
  0.3  2          0.6               0.50       150      0.9976937  1.0000000  0.9945946
  0.3  2          0.6               0.75        50      0.9977297  1.0000000  0.9919640
  0.3  2          0.6               0.75       100      0.9980901  1.0000000  0.9972973
  0.3  2          0.6               0.75       150      0.9978018  1.0000000  0.9972973
  0.3  2          0.6               1.00        50      0.9987387  1.0000000  0.9972973
  0.3  2          0.6               1.00       100      0.9995315  1.0000000  0.9972973
  0.3  2          0.6               1.00       150      0.9982703  1.0000000  0.9972973
  0.3  2          0.8               0.50        50      0.9967207  1.0000000  0.9785946
  0.3  2          0.8               0.50       100      0.9978018  1.0000000  0.9892613
  0.3  2          0.8               0.50       150      0.9978018  1.0000000  0.9892613
  0.3  2          0.8               0.75        50      0.9976937  1.0000000  0.9919640
  0.3  2          0.8               0.75       100      0.9976216  1.0000000  0.9972973
  0.3  2          0.8               0.75       150      0.9976937  1.0000000  0.9972973
  0.3  2          0.8               1.00        50      0.9975856  1.0000000  0.9972973
  0.3  2          0.8               1.00       100      0.9982703  1.0000000  0.9972973
  0.3  2          0.8               1.00       150      0.9982703  1.0000000  0.9972973
  0.3  3          0.6               0.50        50      0.9982342  1.0000000  0.9972973
  0.3  3          0.6               0.50       100      0.9980901  1.0000000  0.9972973
  0.3  3          0.6               0.50       150      0.9983063  1.0000000  0.9918919
  0.3  3          0.6               0.75        50      0.9975856  1.0000000  0.9972973
  0.3  3          0.6               0.75       100      0.9976937  1.0000000  0.9972973
  0.3  3          0.6               0.75       150      0.9976937  1.0000000  0.9945946
  0.3  3          0.6               1.00        50      0.9984505  1.0000000  0.9972973
  0.3  3          0.6               1.00       100      0.9988829  1.0000000  0.9972973
  0.3  3          0.6               1.00       150      0.9993153  1.0000000  0.9972973
  0.3  3          0.8               0.50        50      0.9976937  1.0000000  0.9945946
  0.3  3          0.8               0.50       100      0.9976937  1.0000000  0.9945946
  0.3  3          0.8               0.50       150      0.9980901  1.0000000  0.9919279
  0.3  3          0.8               0.75        50      0.9991351  1.0000000  0.9919640
  0.3  3          0.8               0.75       100      0.9986667  1.0000000  0.9972973
  0.3  3          0.8               0.75       150      0.9994955  1.0000000  0.9945946
  0.3  3          0.8               1.00        50      0.9982703  1.0000000  0.9972973
  0.3  3          0.8               1.00       100      0.9984865  1.0000000  0.9972973
  0.3  3          0.8               1.00       150      0.9988829  1.0000000  0.9972973
  0.4  1          0.6               0.50        50      0.9961802  0.9783784  0.9838559
  0.4  1          0.6               0.50       100      0.9976937  1.0000000  0.9865586
  0.4  1          0.6               0.50       150      0.9976216  1.0000000  0.9865946
  0.4  1          0.6               0.75        50      0.9963604  1.0000000  0.9919640
  0.4  1          0.6               0.75       100      0.9975856  1.0000000  0.9972973
  0.4  1          0.6               0.75       150      0.9975856  1.0000000  0.9972973
  0.4  1          0.6               1.00        50      0.9977658  1.0000000  0.9919640
  0.4  1          0.6               1.00       100      0.9975856  1.0000000  0.9972973
  0.4  1          0.6               1.00       150      0.9976216  1.0000000  0.9972973
  0.4  1          0.8               0.50        50      0.9926379  0.9920000  0.9678198
  0.4  1          0.8               0.50       100      0.9946136  1.0000000  0.9812252
  0.4  1          0.8               0.50       150      0.9972554  1.0000000  0.9865946
  0.4  1          0.8               0.75        50      0.9958919  1.0000000  0.9892613
  0.4  1          0.8               0.75       100      0.9975856  1.0000000  0.9919640
  0.4  1          0.8               0.75       150      0.9976216  1.0000000  0.9919640
  0.4  1          0.8               1.00        50      0.9977658  1.0000000  0.9919640
  0.4  1          0.8               1.00       100      0.9975856  1.0000000  0.9972973
  0.4  1          0.8               1.00       150      0.9975856  1.0000000  0.9972973
  0.4  2          0.6               0.50        50      0.9982703  1.0000000  0.9919640
  0.4  2          0.6               0.50       100      0.9984505  1.0000000  0.9946306
  0.4  2          0.6               0.50       150      0.9995315  1.0000000  0.9945946
  0.4  2          0.6               0.75        50      0.9982703  1.0000000  0.9972973
  0.4  2          0.6               0.75       100      0.9985225  1.0000000  0.9972973
  0.4  2          0.6               0.75       150      0.9990270  1.0000000  0.9972973
  0.4  2          0.6               1.00        50      0.9979099  1.0000000  0.9972973
  0.4  2          0.6               1.00       100      0.9984505  1.0000000  0.9972973
  0.4  2          0.6               1.00       150      0.9988829  1.0000000  0.9972973
  0.4  2          0.8               0.50        50      0.9971892  1.0000000  0.9812252
  0.4  2          0.8               0.50       100      0.9976937  1.0000000  0.9838919
  0.4  2          0.8               0.50       150      0.9978018  1.0000000  0.9865586
  0.4  2          0.8               0.75        50      0.9976937  1.0000000  0.9946306
  0.4  2          0.8               0.75       100      0.9976937  1.0000000  0.9972973
  0.4  2          0.8               0.75       150      0.9976937  1.0000000  0.9972973
  0.4  2          0.8               1.00        50      0.9976216  1.0000000  0.9972973
  0.4  2          0.8               1.00       100      0.9982703  1.0000000  0.9972973
  0.4  2          0.8               1.00       150      0.9986667  1.0000000  0.9972973
  0.4  3          0.6               0.50        50      0.9980901  1.0000000  0.9946306
  0.4  3          0.6               0.50       100      0.9984505  1.0000000  0.9919279
  0.4  3          0.6               0.50       150      0.9989910  1.0000000  0.9919279
  0.4  3          0.6               0.75        50      0.9976216  1.0000000  0.9919640
  0.4  3          0.6               0.75       100      0.9980901  1.0000000  0.9919279
  0.4  3          0.6               0.75       150      0.9980901  1.0000000  0.9892252
  0.4  3          0.6               1.00        50      0.9981802  1.0000000  0.9972973
  0.4  3          0.6               1.00       100      0.9986667  1.0000000  0.9972973
  0.4  3          0.6               1.00       150      0.9988829  1.0000000  0.9972973
  0.4  3          0.8               0.50        50      0.9994595  1.0000000  0.9919279
  0.4  3          0.8               0.50       100      0.9984505  1.0000000  0.9945946
  0.4  3          0.8               0.50       150      0.9988829  1.0000000  0.9945946
  0.4  3          0.8               0.75        50      0.9979820  1.0000000  0.9972973
  0.4  3          0.8               0.75       100      0.9988829  1.0000000  0.9945946
  0.4  3          0.8               0.75       150      0.9988829  1.0000000  0.9945946
  0.4  3          0.8               1.00        50      0.9991712  1.0000000  0.9972973
  0.4  3          0.8               1.00       100      0.9997117  1.0000000  0.9972973
  0.4  3          0.8               1.00       150      0.9997838  1.0000000  0.9945946

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 150, max_depth = 3, eta = 0.4, gamma = 0, colsample_bytree = 0.8, min_child_weight = 1 and subsample = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     50.0      0.3
  positive      0.0     49.7
                            
 Accuracy (average) : 0.9973

[1] "TEST accuracy: 0.997304582210243"
[1] "TEST +precision: 1"
[1] "TEST -precision: 0.994638069705094"
[1] "TEST specifity: 1"
[1] "TEST sensitivity: 0.994609164420485"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        0        3
            positive        6      122
[1] "TEST accuracy: 0.931297709923664"
[1] "TEST +precision: 0.953125"
[1] "TEST -precision: 0"
[1] "TEST specifity: 0"
[1] "TEST sensitivity: 0.976"

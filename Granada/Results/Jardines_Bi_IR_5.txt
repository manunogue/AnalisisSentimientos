[1] "DATASET NAME: Jardines_Bi_IR_5"
[1] "TRAIN INSTANCES: 797"
[1] "TEST INSTANCES: 223"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 3.58686995506287"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

797 samples
959 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 638, 637, 637, 638, 638 
Resampling results:

  ROC  Sens  Spec
  1    1     1   

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     17.9      0.0
  positive      0.0     82.1
                       
 Accuracy (average) : 1

[1] "TEST accuracy: 1"
[1] "TEST +precision: 1"
[1] "TEST -precision: 1"
[1] "TEST specifity: 1"
[1] "TEST sensitivity: 1"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        0        0
            positive        5      218
[1] "TEST accuracy: 0.977578475336323"
[1] "TEST +precision: 0.977578475336323"
[1] "TEST -precision: NaN"
[1] "TEST specifity: 0"
[1] "TEST sensitivity: 1"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 2.01827838420868"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

797 samples
959 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 638, 638, 637, 638, 637 
Resampling results across tuning parameters:

  C      M  ROC        Sens       Spec     
  0.010  1  0.8635465  0.6706897  0.9938814
  0.010  2  0.8635465  0.6706897  0.9938814
  0.010  3  0.8653045  0.6706897  0.9969348
  0.255  1  0.8885897  0.6921182  0.9984615
  0.255  2  0.8878264  0.6921182  0.9954081
  0.255  3  0.8802717  0.6921182  0.9938814
  0.500  1  0.9258458  0.7201970  0.9984615
  0.500  2  0.9250825  0.7201970  0.9954081
  0.500  3  0.9145684  0.7059113  0.9954081

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.5 and M = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     12.9      0.1
  positive      5.0     81.9
                            
 Accuracy (average) : 0.9486

[1] "TEST accuracy: 0.948557089084065"
[1] "TEST +precision: 0.942279942279942"
[1] "TEST -precision: 0.990384615384615"
[1] "TEST specifity: 0.72027972027972"
[1] "TEST sensitivity: 0.998470948012232"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        0        0
            positive        5      218
[1] "TEST accuracy: 0.977578475336323"
[1] "TEST +precision: 0.977578475336323"
[1] "TEST -precision: NaN"
[1] "TEST specifity: 0"
[1] "TEST sensitivity: 1"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 3.93848119974136"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

797 samples
959 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 637, 638, 637, 638, 638 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens       Spec     
  0.3  1          0.6               0.50        50      0.9448211  0.7268473  1.0000000
  0.3  1          0.6               0.50       100      0.9610091  0.8679803  1.0000000
  0.3  1          0.6               0.50       150      0.9651084  0.8886700  1.0000000
  0.3  1          0.6               0.75        50      0.9530938  0.7637931  1.0000000
  0.3  1          0.6               0.75       100      0.9636486  0.8822660  0.9984733
  0.3  1          0.6               0.75       150      0.9712575  0.9029557  0.9984733
  0.3  1          0.6               1.00        50      0.9599360  0.7770936  1.0000000
  0.3  1          0.6               1.00       100      0.9682372  0.8960591  0.9969466
  0.3  1          0.6               1.00       150      0.9736394  0.9167488  0.9969466
  0.3  1          0.8               0.50        50      0.9335160  0.7142857  1.0000000
  0.3  1          0.8               0.50       100      0.9601922  0.8394089  0.9954198
  0.3  1          0.8               0.50       150      0.9662407  0.8886700  0.9908397
  0.3  1          0.8               0.75        50      0.9517322  0.7697044  1.0000000
  0.3  1          0.8               0.75       100      0.9631473  0.9024631  1.0000000
  0.3  1          0.8               0.75       150      0.9710802  0.9024631  0.9969466
  0.3  1          0.8               1.00        50      0.9597216  0.7839901  1.0000000
  0.3  1          0.8               1.00       100      0.9682372  0.8960591  0.9969466
  0.3  1          0.8               1.00       150      0.9742663  0.9167488  0.9954198
  0.3  2          0.6               0.50        50      0.9682190  0.8463054  0.9984733
  0.3  2          0.6               0.50       100      0.9706210  0.8955665  0.9984733
  0.3  2          0.6               0.50       150      0.9713661  0.8955665  0.9908397
  0.3  2          0.6               0.75        50      0.9707213  0.8960591  1.0000000
  0.3  2          0.6               0.75       100      0.9758919  0.9167488  0.9984733
  0.3  2          0.6               0.75       150      0.9755724  0.9167488  0.9969466
  0.3  2          0.6               1.00        50      0.9685005  0.8960591  0.9969466
  0.3  2          0.6               1.00       100      0.9773390  0.9167488  0.9969466
  0.3  2          0.6               1.00       150      0.9759640  0.9167488  0.9969466
  0.3  2          0.8               0.50        50      0.9572294  0.8674877  0.9969466
  0.3  2          0.8               0.50       100      0.9721845  0.8881773  0.9954198
  0.3  2          0.8               0.50       150      0.9719741  0.8881773  0.9908397
  0.3  2          0.8               0.75        50      0.9658944  0.8817734  0.9984733
  0.3  2          0.8               0.75       100      0.9722318  0.9024631  0.9984733
  0.3  2          0.8               0.75       150      0.9712807  0.9024631  0.9984733
  0.3  2          0.8               1.00        50      0.9710445  0.9167488  0.9969466
  0.3  2          0.8               1.00       100      0.9768563  0.9167488  0.9969466
  0.3  2          0.8               1.00       150      0.9768625  0.9167488  0.9969466
  0.3  3          0.6               0.50        50      0.9751220  0.8743842  1.0000000
  0.3  3          0.6               0.50       100      0.9764768  0.9029557  0.9954198
  0.3  3          0.6               0.50       150      0.9779614  0.9029557  0.9954198
  0.3  3          0.6               0.75        50      0.9712324  0.9024631  0.9984733
  0.3  3          0.6               0.75       100      0.9719868  0.9024631  0.9984733
  0.3  3          0.6               0.75       150      0.9722989  0.9024631  0.9984733
  0.3  3          0.6               1.00        50      0.9875871  0.9581281  0.9969466
  0.3  3          0.6               1.00       100      0.9867964  0.9581281  0.9984733
  0.3  3          0.6               1.00       150      0.9872297  0.9581281  0.9984733
  0.3  3          0.8               0.50        50      0.9737764  0.8960591  0.9954198
  0.3  3          0.8               0.50       100      0.9728032  0.9167488  0.9938931
  0.3  3          0.8               0.50       150      0.9728553  0.9167488  0.9938931
  0.3  3          0.8               0.75        50      0.9806095  0.9443350  0.9984733
  0.3  3          0.8               0.75       100      0.9803903  0.9443350  0.9984733
  0.3  3          0.8               0.75       150      0.9808628  0.9443350  0.9969466
  0.3  3          0.8               1.00        50      0.9832155  0.9443350  0.9954198
  0.3  3          0.8               1.00       100      0.9822281  0.9443350  0.9969466
  0.3  3          0.8               1.00       150      0.9833963  0.9443350  0.9969466
  0.4  1          0.6               0.50        50      0.9504512  0.8125616  0.9984733
  0.4  1          0.6               0.50       100      0.9620426  0.8746305  0.9938931
  0.4  1          0.6               0.50       150      0.9617823  0.8746305  0.9923664
  0.4  1          0.6               0.75        50      0.9605473  0.8960591  1.0000000
  0.4  1          0.6               0.75       100      0.9728357  0.9167488  0.9984733
  0.4  1          0.6               0.75       150      0.9759428  0.9167488  0.9984733
  0.4  1          0.6               1.00        50      0.9609146  0.8674877  0.9969466
  0.4  1          0.6               1.00       100      0.9708818  0.9167488  0.9954198
  0.4  1          0.6               1.00       150      0.9748781  0.9167488  0.9969466
  0.4  1          0.8               0.50        50      0.9492669  0.8041872  0.9969466
  0.4  1          0.8               0.50       100      0.9643347  0.8817734  0.9923664
  0.4  1          0.8               0.50       150      0.9637326  0.8817734  0.9893130
  0.4  1          0.8               0.75        50      0.9597534  0.8960591  0.9984733
  0.4  1          0.8               0.75       100      0.9736322  0.9167488  0.9984733
  0.4  1          0.8               0.75       150      0.9759928  0.9167488  0.9984733
  0.4  1          0.8               1.00        50      0.9608633  0.8674877  0.9984733
  0.4  1          0.8               1.00       100      0.9719420  0.9167488  0.9969466
  0.4  1          0.8               1.00       150      0.9751945  0.9167488  0.9969466
  0.4  2          0.6               0.50        50      0.9676982  0.8465517  0.9984733
  0.4  2          0.6               0.50       100      0.9697533  0.8958128  0.9969466
  0.4  2          0.6               0.50       150      0.9718242  0.8958128  0.9969466
  0.4  2          0.6               0.75        50      0.9747342  0.9167488  0.9954198
  0.4  2          0.6               0.75       100      0.9758500  0.9167488  0.9969466
  0.4  2          0.6               0.75       150      0.9760098  0.9167488  0.9969466
  0.4  2          0.6               1.00        50      0.9746373  0.9167488  0.9969466
  0.4  2          0.6               1.00       100      0.9767066  0.9167488  0.9984733
  0.4  2          0.6               1.00       150      0.9779285  0.9167488  0.9984733
  0.4  2          0.8               0.50        50      0.9703738  0.9098522  0.9984733
  0.4  2          0.8               0.50       100      0.9734806  0.9098522  0.9954198
  0.4  2          0.8               0.50       150      0.9747716  0.9098522  0.9938931
  0.4  2          0.8               0.75        50      0.9714613  0.9029557  0.9954198
  0.4  2          0.8               0.75       100      0.9735460  0.9029557  0.9969466
  0.4  2          0.8               0.75       150      0.9741265  0.9029557  0.9954198
  0.4  2          0.8               1.00        50      0.9747414  0.9167488  0.9969466
  0.4  2          0.8               1.00       100      0.9762779  0.9167488  0.9984733
  0.4  2          0.8               1.00       150      0.9776089  0.9167488  0.9984733
  0.4  3          0.6               0.50        50      0.9649968  0.8738916  0.9908397
  0.4  3          0.6               0.50       100      0.9696369  0.9024631  0.9893130
  0.4  3          0.6               0.50       150      0.9703927  0.9024631  0.9923664
  0.4  3          0.6               0.75        50      0.9713210  0.9029557  0.9954198
  0.4  3          0.6               0.75       100      0.9732070  0.9029557  0.9954198
  0.4  3          0.6               0.75       150      0.9745933  0.9029557  0.9938931
  0.4  3          0.6               1.00        50      0.9821287  0.9443350  0.9969466
  0.4  3          0.6               1.00       100      0.9828749  0.9443350  0.9969466
  0.4  3          0.6               1.00       150      0.9828628  0.9443350  0.9969466
  0.4  3          0.8               0.50        50      0.9754897  0.9098522  0.9923664
  0.4  3          0.8               0.50       100      0.9760983  0.9098522  0.9908397
  0.4  3          0.8               0.50       150      0.9771094  0.9098522  0.9923664
  0.4  3          0.8               0.75        50      0.9809372  0.9443350  0.9969466
  0.4  3          0.8               0.75       100      0.9814694  0.9443350  0.9923664
  0.4  3          0.8               0.75       150      0.9822186  0.9443350  0.9969466
  0.4  3          0.8               1.00        50      0.9815327  0.9443350  0.9984733
  0.4  3          0.8               1.00       100      0.9822818  0.9443350  0.9984733
  0.4  3          0.8               1.00       150      0.9820546  0.9443350  0.9984733

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 50, max_depth = 3, eta = 0.3, gamma = 0, colsample_bytree = 0.6, min_child_weight = 1 and subsample = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     17.2      0.3
  positive      0.8     81.8
                          
 Accuracy (average) : 0.99

[1] "TEST accuracy: 0.989962358845671"
[1] "TEST +precision: 0.990881458966565"
[1] "TEST -precision: 0.985611510791367"
[1] "TEST specifity: 0.958041958041958"
[1] "TEST sensitivity: 0.996941896024465"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        0        0
            positive        5      218
[1] "TEST accuracy: 0.977578475336323"
[1] "TEST +precision: 0.977578475336323"
[1] "TEST -precision: NaN"
[1] "TEST specifity: 0"
[1] "TEST sensitivity: 1"

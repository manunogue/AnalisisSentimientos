[1] "DATASET NAME: Carmela_Uni_IR_5"
[1] "TRAIN INSTANCES: 357"
[1] "TEST INSTANCES: 103"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 1.82711100578308"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

357 samples
746 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 285, 286, 286, 285, 286 
Resampling results:

  ROC        Sens    Spec     
  0.9938636  0.8875  0.9855844

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     19.9      1.1
  positive      2.5     76.5
                            
 Accuracy (average) : 0.9636

[1] "TEST accuracy: 0.963585434173669"
[1] "TEST +precision: 0.968085106382979"
[1] "TEST -precision: 0.946666666666667"
[1] "TEST specifity: 0.8875"
[1] "TEST sensitivity: 0.985559566787004"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        2        2
            positive        6       93
[1] "TEST accuracy: 0.922330097087379"
[1] "TEST +precision: 0.939393939393939"
[1] "TEST -precision: 0.5"
[1] "TEST specifity: 0.25"
[1] "TEST sensitivity: 0.978947368421053"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 54.3073680400848"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

357 samples
746 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 285, 286, 285, 286, 286 
Resampling results across tuning parameters:

  C      M  ROC        Sens    Spec     
  0.010  1  0.7851603  0.6125  0.9387013
  0.010  2  0.7713332  0.5750  0.9387013
  0.010  3  0.7355519  0.5000  0.9386364
  0.255  1  0.9093263  0.8750  0.9242857
  0.255  2  0.9162967  0.8625  0.9314286
  0.255  3  0.9003490  0.7625  0.9350000
  0.500  1  0.9263880  0.9250  0.9206494
  0.500  2  0.9313494  0.9000  0.9242857
  0.500  3  0.9122707  0.8000  0.9207143

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.5 and M = 2.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     20.2      5.9
  positive      2.2     71.7
                            
 Accuracy (average) : 0.9188

[1] "TEST accuracy: 0.918767507002801"
[1] "TEST +precision: 0.96969696969697"
[1] "TEST -precision: 0.774193548387097"
[1] "TEST specifity: 0.9"
[1] "TEST sensitivity: 0.924187725631769"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        1        7
            positive        7       88
[1] "TEST accuracy: 0.864077669902913"
[1] "TEST +precision: 0.926315789473684"
[1] "TEST -precision: 0.125"
[1] "TEST specifity: 0.125"
[1] "TEST sensitivity: 0.926315789473684"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 1.7969294667244"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

357 samples
746 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 285, 286, 286, 285, 286 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens    Spec     
  0.3  1          0.6               0.50        50      0.9702922  0.7375  0.9890909
  0.3  1          0.6               0.50       100      0.9822890  0.8000  0.9890909
  0.3  1          0.6               0.50       150      0.9843182  0.8625  0.9854545
  0.3  1          0.6               0.75        50      0.9768466  0.7375  0.9927273
  0.3  1          0.6               0.75       100      0.9870495  0.9125  0.9818182
  0.3  1          0.6               0.75       150      0.9922727  0.9250  0.9818831
  0.3  1          0.6               1.00        50      0.9753693  0.7625  0.9854545
  0.3  1          0.6               1.00       100      0.9842208  0.9000  0.9818182
  0.3  1          0.6               1.00       150      0.9897808  0.9250  0.9818831
  0.3  1          0.8               0.50        50      0.9683320  0.6625  0.9818831
  0.3  1          0.8               0.50       100      0.9886567  0.7875  0.9746104
  0.3  1          0.8               0.50       150      0.9875081  0.9125  0.9746104
  0.3  1          0.8               0.75        50      0.9759476  0.8125  0.9854545
  0.3  1          0.8               0.75       100      0.9863758  0.8875  0.9746753
  0.3  1          0.8               0.75       150      0.9915950  0.9375  0.9818831
  0.3  1          0.8               1.00        50      0.9744602  0.8250  0.9745455
  0.3  1          0.8               1.00       100      0.9835308  0.9000  0.9818182
  0.3  1          0.8               1.00       150      0.9902313  0.9250  0.9855195
  0.3  2          0.6               0.50        50      0.9854627  0.8625  0.9782468
  0.3  2          0.6               0.50       100      0.9918222  0.9375  0.9746753
  0.3  2          0.6               0.50       150      0.9929545  0.9375  0.9818831
  0.3  2          0.6               0.75        50      0.9900041  0.8875  0.9927273
  0.3  2          0.6               0.75       100      0.9945455  0.9250  0.9855195
  0.3  2          0.6               0.75       150      0.9943182  0.9375  0.9927922
  0.3  2          0.6               1.00        50      0.9856859  0.9125  0.9818182
  0.3  2          0.6               1.00       100      0.9922727  0.9375  0.9890909
  0.3  2          0.6               1.00       150      0.9922727  0.9750  0.9927273
  0.3  2          0.8               0.50        50      0.9852354  0.8625  0.9818831
  0.3  2          0.8               0.50       100      0.9856818  0.9250  0.9746753
  0.3  2          0.8               0.50       150      0.9868182  0.9375  0.9818831
  0.3  2          0.8               0.75        50      0.9829667  0.8875  0.9855195
  0.3  2          0.8               0.75       100      0.9915909  0.9250  0.9891558
  0.3  2          0.8               0.75       150      0.9911364  0.9375  0.9891558
  0.3  2          0.8               1.00        50      0.9860268  0.9250  0.9818182
  0.3  2          0.8               1.00       100      0.9927313  0.9375  0.9819481
  0.3  2          0.8               1.00       150      0.9927313  0.9750  0.9855844
  0.3  3          0.6               0.50        50      0.9852394  0.8625  0.9855195
  0.3  3          0.6               0.50       100      0.9881818  0.9000  0.9818831
  0.3  3          0.6               0.50       150      0.9865909  0.9000  0.9855195
  0.3  3          0.6               0.75        50      0.9863677  0.9000  0.9855195
  0.3  3          0.6               0.75       100      0.9897727  0.9750  0.9855195
  0.3  3          0.6               0.75       150      0.9906818  0.9750  0.9891558
  0.3  3          0.6               1.00        50      0.9906899  0.9250  0.9855195
  0.3  3          0.6               1.00       100      0.9925000  0.9625  0.9927922
  0.3  3          0.6               1.00       150      0.9922727  0.9625  0.9927922
  0.3  3          0.8               0.50        50      0.9866153  0.8750  0.9782468
  0.3  3          0.8               0.50       100      0.9879708  0.9000  0.9782468
  0.3  3          0.8               0.50       150      0.9865990  0.9250  0.9782468
  0.3  3          0.8               0.75        50      0.9911364  0.9250  0.9890909
  0.3  3          0.8               0.75       100      0.9913636  0.9750  0.9855195
  0.3  3          0.8               0.75       150      0.9906818  0.9625  0.9927273
  0.3  3          0.8               1.00        50      0.9929545  0.9250  0.9927922
  0.3  3          0.8               1.00       100      0.9927273  0.9750  0.9927922
  0.3  3          0.8               1.00       150      0.9918182  0.9750  0.9927922
  0.4  1          0.6               0.50        50      0.9768872  0.7625  0.9746104
  0.4  1          0.6               0.50       100      0.9888718  0.8875  0.9855195
  0.4  1          0.6               0.50       150      0.9913758  0.9125  0.9818831
  0.4  1          0.6               0.75        50      0.9777760  0.8375  0.9854545
  0.4  1          0.6               0.75       100      0.9884172  0.9125  0.9891558
  0.4  1          0.6               0.75       150      0.9922808  0.9375  0.9782468
  0.4  1          0.6               1.00        50      0.9780925  0.8625  0.9745455
  0.4  1          0.6               1.00       100      0.9876218  0.9125  0.9855195
  0.4  1          0.6               1.00       150      0.9925041  0.9375  0.9855195
  0.4  1          0.8               0.50        50      0.9761567  0.8000  0.9891558
  0.4  1          0.8               0.50       100      0.9865990  0.8375  0.9782468
  0.4  1          0.8               0.50       150      0.9875081  0.8875  0.9782468
  0.4  1          0.8               0.75        50      0.9776664  0.8125  0.9781818
  0.4  1          0.8               0.75       100      0.9915950  0.9250  0.9818831
  0.4  1          0.8               0.75       150      0.9918222  0.9250  0.9818831
  0.4  1          0.8               1.00        50      0.9785430  0.8625  0.9854545
  0.4  1          0.8               1.00       100      0.9887541  0.9375  0.9890909
  0.4  1          0.8               1.00       150      0.9934131  0.9500  0.9855195
  0.4  2          0.6               0.50        50      0.9832183  0.8625  0.9819481
  0.4  2          0.6               0.50       100      0.9888758  0.8875  0.9891558
  0.4  2          0.6               0.50       150      0.9888677  0.9125  0.9782468
  0.4  2          0.6               0.75        50      0.9879545  0.9250  0.9818182
  0.4  2          0.6               0.75       100      0.9909091  0.9500  0.9818831
  0.4  2          0.6               0.75       150      0.9913636  0.9375  0.9855195
  0.4  2          0.6               1.00        50      0.9909091  0.9125  0.9782468
  0.4  2          0.6               1.00       100      0.9931859  0.9750  0.9819481
  0.4  2          0.6               1.00       150      0.9938636  0.9750  0.9892208
  0.4  2          0.8               0.50        50      0.9909131  0.8875  0.9890909
  0.4  2          0.8               0.50       100      0.9927313  0.9500  0.9782468
  0.4  2          0.8               0.50       150      0.9915950  0.9625  0.9782468
  0.4  2          0.8               0.75        50      0.9943182  0.9375  0.9890909
  0.4  2          0.8               0.75       100      0.9931818  0.9750  0.9818831
  0.4  2          0.8               0.75       150      0.9934091  0.9750  0.9855195
  0.4  2          0.8               1.00        50      0.9881899  0.9125  0.9855195
  0.4  2          0.8               1.00       100      0.9922727  0.9375  0.9855844
  0.4  2          0.8               1.00       150      0.9915909  0.9625  0.9855844
  0.4  3          0.6               0.50        50      0.9884213  0.8875  0.9855195
  0.4  3          0.6               0.50       100      0.9863880  0.9000  0.9891558
  0.4  3          0.6               0.50       150      0.9884091  0.8750  0.9855195
  0.4  3          0.6               0.75        50      0.9881899  0.9250  0.9855195
  0.4  3          0.6               0.75       100      0.9893222  0.9750  0.9891558
  0.4  3          0.6               0.75       150      0.9881818  0.9750  0.9855195
  0.4  3          0.6               1.00        50      0.9922808  0.9500  0.9927922
  0.4  3          0.6               1.00       100      0.9920495  0.9750  0.9927922
  0.4  3          0.6               1.00       150      0.9913636  0.9750  0.9927922
  0.4  3          0.8               0.50        50      0.9854586  0.9375  0.9746104
  0.4  3          0.8               0.50       100      0.9897727  0.9250  0.9855195
  0.4  3          0.8               0.50       150      0.9895455  0.9500  0.9818831
  0.4  3          0.8               0.75        50      0.9877273  0.9375  0.9818831
  0.4  3          0.8               0.75       100      0.9879545  0.9750  0.9818182
  0.4  3          0.8               0.75       150      0.9884091  0.9750  0.9781818
  0.4  3          0.8               1.00        50      0.9909091  0.9625  0.9855195
  0.4  3          0.8               1.00       100      0.9913636  0.9625  0.9855195
  0.4  3          0.8               1.00       150      0.9915909  0.9625  0.9855195

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 100, max_depth = 2, eta = 0.3, gamma = 0, colsample_bytree = 0.6, min_child_weight = 1 and subsample = 0.75.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     20.7      1.1
  positive      1.7     76.5
                           
 Accuracy (average) : 0.972

[1] "TEST accuracy: 0.971988795518207"
[1] "TEST +precision: 0.978494623655914"
[1] "TEST -precision: 0.948717948717949"
[1] "TEST specifity: 0.925"
[1] "TEST sensitivity: 0.985559566787004"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        3        4
            positive        5       91
[1] "TEST accuracy: 0.912621359223301"
[1] "TEST +precision: 0.947916666666667"
[1] "TEST -precision: 0.428571428571429"
[1] "TEST specifity: 0.375"
[1] "TEST sensitivity: 0.957894736842105"

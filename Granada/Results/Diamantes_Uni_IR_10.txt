[1] "DATASET NAME: Diamantes_Uni_IR_10"
[1] "TRAIN INSTANCES: 426"
[1] "TEST INSTANCES: 131"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 1.90760803222656"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

426 samples
797 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 340, 341, 341, 341, 341 
Resampling results:

  ROC    Sens       Spec    
  0.992  0.8654545  0.991964

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     10.6      0.7
  positive      1.6     87.1
                            
 Accuracy (average) : 0.9765

[1] "TEST accuracy: 0.976525821596244"
[1] "TEST +precision: 0.981481481481482"
[1] "TEST -precision: 0.9375"
[1] "TEST specifity: 0.865384615384615"
[1] "TEST sensitivity: 0.991978609625668"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        2        0
            positive        7      122
[1] "TEST accuracy: 0.946564885496183"
[1] "TEST +precision: 0.945736434108527"
[1] "TEST -precision: 1"
[1] "TEST specifity: 0.222222222222222"
[1] "TEST sensitivity: 1"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 1.02213976780574"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

426 samples
797 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 340, 341, 341, 341, 341 
Resampling results across tuning parameters:

  C      M  ROC        Sens       Spec     
  0.010  1  0.7995607  0.6381818  0.9732973
  0.010  2  0.7806775  0.5981818  0.9706306
  0.010  3  0.7756452  0.5981818  0.9625946
  0.255  1  0.8832911  0.8072727  0.9732973
  0.255  2  0.8831312  0.8072727  0.9706306
  0.255  3  0.8671548  0.7109091  0.9706667
  0.500  1  0.9266244  0.9018182  0.9732973
  0.500  2  0.9281251  0.9018182  0.9706306
  0.500  3  0.8671548  0.7109091  0.9706667

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.5 and M = 2.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     11.0      2.6
  positive      1.2     85.2
                            
 Accuracy (average) : 0.9624

[1] "TEST accuracy: 0.962441314553991"
[1] "TEST +precision: 0.986413043478261"
[1] "TEST -precision: 0.810344827586207"
[1] "TEST specifity: 0.903846153846154"
[1] "TEST sensitivity: 0.970588235294118"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        0        8
            positive        9      114
[1] "TEST accuracy: 0.870229007633588"
[1] "TEST +precision: 0.926829268292683"
[1] "TEST -precision: 0"
[1] "TEST specifity: 0"
[1] "TEST sensitivity: 0.934426229508197"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 2.09457253615061"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

426 samples
797 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 341, 341, 341, 340, 341 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens       Spec     
  0.3  1          0.6               0.50        50      0.9942912  0.7290909  0.9892973
  0.3  1          0.6               0.50       100      0.9946162  0.9036364  0.9892973
  0.3  1          0.6               0.50       150      0.9908907  0.9254545  0.9892973
  0.3  1          0.6               0.75        50      0.9949412  0.7127273  0.9920000
  0.3  1          0.6               0.75       100      0.9946470  0.9018182  0.9892973
  0.3  1          0.6               0.75       150      0.9939099  0.9818182  0.9865946
  0.3  1          0.6               1.00        50      0.9958834  0.7672727  0.9946667
  0.3  1          0.6               1.00       100      0.9961566  0.9018182  0.9920000
  0.3  1          0.6               1.00       150      0.9946713  0.9818182  0.9892973
  0.3  1          0.8               0.50        50      0.9863276  0.6709091  0.9892973
  0.3  1          0.8               0.50       100      0.9902585  0.8472727  0.9839279
  0.3  1          0.8               0.50       150      0.9887214  0.9036364  0.9812613
  0.3  1          0.8               0.75        50      0.9935833  0.6509091  0.9919640
  0.3  1          0.8               0.75       100      0.9942670  0.9418182  0.9865946
  0.3  1          0.8               0.75       150      0.9939099  0.9618182  0.9892973
  0.3  1          0.8               1.00        50      0.9938857  0.7672727  0.9920000
  0.3  1          0.8               1.00       100      0.9966624  0.9218182  0.9920000
  0.3  1          0.8               1.00       150      0.9939132  0.9818182  0.9892973
  0.3  2          0.6               0.50        50      0.9919089  0.8472727  0.9892973
  0.3  2          0.6               0.50       100      0.9904269  0.9054545  0.9839279
  0.3  2          0.6               0.50       150      0.9917183  0.9236364  0.9866306
  0.3  2          0.6               0.75        50      0.9951836  0.9418182  0.9892973
  0.3  2          0.6               0.75       100      0.9942008  0.9618182  0.9892973
  0.3  2          0.6               0.75       150      0.9931826  0.9618182  0.9866306
  0.3  2          0.6               1.00        50      0.9945887  0.9618182  0.9892973
  0.3  2          0.6               1.00       100      0.9943043  0.9618182  0.9892973
  0.3  2          0.6               1.00       150      0.9940619  0.9618182  0.9920000
  0.3  2          0.8               0.50        50      0.9930791  0.8454545  0.9866306
  0.3  2          0.8               0.50       100      0.9922968  0.9054545  0.9839640
  0.3  2          0.8               0.50       150      0.9925425  0.9236364  0.9866667
  0.3  2          0.8               0.75        50      0.9923892  0.9418182  0.9920000
  0.3  2          0.8               0.75       100      0.9921435  0.9618182  0.9893333
  0.3  2          0.8               0.75       150      0.9918978  0.9618182  0.9893333
  0.3  2          0.8               1.00        50      0.9961291  0.9618182  0.9920000
  0.3  2          0.8               1.00       100      0.9933523  0.9618182  0.9920000
  0.3  2          0.8               1.00       150      0.9938857  0.9618182  0.9892973
  0.3  3          0.6               0.50        50      0.9912878  0.8654545  0.9892973
  0.3  3          0.6               0.50       100      0.9902113  0.9054545  0.9866306
  0.3  3          0.6               0.50       150      0.9924645  0.9054545  0.9893333
  0.3  3          0.6               0.75        50      0.9959220  0.9618182  0.9920000
  0.3  3          0.6               0.75       100      0.9956554  0.9618182  0.9920000
  0.3  3          0.6               0.75       150      0.9953887  0.9618182  0.9920000
  0.3  3          0.6               1.00        50      0.9943495  0.9618182  0.9892973
  0.3  3          0.6               1.00       100      0.9946162  0.9618182  0.9892973
  0.3  3          0.6               1.00       150      0.9933314  0.9618182  0.9866306
  0.3  3          0.8               0.50        50      0.9912072  0.8872727  0.9892973
  0.3  3          0.8               0.50       100      0.9923905  0.9254545  0.9839279
  0.3  3          0.8               0.50       150      0.9929690  0.9236364  0.9839279
  0.3  3          0.8               0.75        50      0.9949799  0.9618182  0.9920000
  0.3  3          0.8               0.75       100      0.9949038  0.9618182  0.9866306
  0.3  3          0.8               0.75       150      0.9951705  0.9618182  0.9893333
  0.3  3          0.8               1.00        50      0.9938857  0.9618182  0.9920000
  0.3  3          0.8               1.00       100      0.9930857  0.9618182  0.9866306
  0.3  3          0.8               1.00       150      0.9920223  0.9618182  0.9866306
  0.4  1          0.6               0.50        50      0.9923597  0.8254545  0.9865946
  0.4  1          0.6               0.50       100      0.9910552  0.9254545  0.9892973
  0.4  1          0.6               0.50       150      0.9924809  0.9436364  0.9865946
  0.4  1          0.6               0.75        50      0.9952288  0.8618182  0.9892973
  0.4  1          0.6               0.75       100      0.9931826  0.9618182  0.9865946
  0.4  1          0.6               0.75       150      0.9934703  0.9818182  0.9839279
  0.4  1          0.6               1.00        50      0.9973605  0.8654545  0.9920000
  0.4  1          0.6               1.00       100      0.9951528  0.9418182  0.9920000
  0.4  1          0.6               1.00       150      0.9941556  0.9818182  0.9892973
  0.4  1          0.8               0.50        50      0.9887279  0.8672727  0.9865946
  0.4  1          0.8               0.50       100      0.9882123  0.9054545  0.9865946
  0.4  1          0.8               0.50       150      0.9893989  0.9236364  0.9839279
  0.4  1          0.8               0.75        50      0.9951384  0.8818182  0.9920000
  0.4  1          0.8               0.75       100      0.9936740  0.9418182  0.9865946
  0.4  1          0.8               0.75       150      0.9928950  0.9818182  0.9866306
  0.4  1          0.8               1.00        50      0.9952691  0.8672727  0.9920000
  0.4  1          0.8               1.00       100      0.9946680  0.9818182  0.9892973
  0.4  1          0.8               1.00       150      0.9944013  0.9818182  0.9892973
  0.4  2          0.6               0.50        50      0.9906306  0.8454545  0.9892973
  0.4  2          0.6               0.50       100      0.9912819  0.9054545  0.9866306
  0.4  2          0.6               0.50       150      0.9930581  0.9054545  0.9866306
  0.4  2          0.6               0.75        50      0.9956586  0.9618182  0.9893333
  0.4  2          0.6               0.75       100      0.9946516  0.9618182  0.9866667
  0.4  2          0.6               0.75       150      0.9946549  0.9618182  0.9893333
  0.4  2          0.6               1.00        50      0.9934349  0.9618182  0.9920000
  0.4  2          0.6               1.00       100      0.9924102  0.9618182  0.9892973
  0.4  2          0.6               1.00       150      0.9921192  0.9618182  0.9892973
  0.4  2          0.8               0.50        50      0.9901405  0.8854545  0.9839640
  0.4  2          0.8               0.50       100      0.9926296  0.9054545  0.9839640
  0.4  2          0.8               0.50       150      0.9929029  0.9054545  0.9866306
  0.4  2          0.8               0.75        50      0.9931728  0.9236364  0.9866667
  0.4  2          0.8               0.75       100      0.9929271  0.9618182  0.9840000
  0.4  2          0.8               0.75       150      0.9924246  0.9618182  0.9840000
  0.4  2          0.8               1.00        50      0.9934316  0.9818182  0.9920000
  0.4  2          0.8               1.00       100      0.9934559  0.9618182  0.9920000
  0.4  2          0.8               1.00       150      0.9937225  0.9618182  0.9920000
  0.4  3          0.6               0.50        50      0.9916455  0.8854545  0.9866667
  0.4  3          0.6               0.50       100      0.9899518  0.9436364  0.9866667
  0.4  3          0.6               0.50       150      0.9887607  0.9618182  0.9866667
  0.4  3          0.6               0.75        50      0.9929061  0.9618182  0.9840000
  0.4  3          0.6               0.75       100      0.9946726  0.9618182  0.9812973
  0.4  3          0.6               0.75       150      0.9944269  0.9618182  0.9839640
  0.4  3          0.6               1.00        50      0.9938857  0.9618182  0.9893333
  0.4  3          0.6               1.00       100      0.9933733  0.9618182  0.9866667
  0.4  3          0.6               1.00       150      0.9928400  0.9618182  0.9866667
  0.4  3          0.8               0.50        50      0.9926428  0.8654545  0.9866306
  0.4  3          0.8               0.50       100      0.9944157  0.9618182  0.9840000
  0.4  3          0.8               0.50       150      0.9923066  0.9618182  0.9840000
  0.4  3          0.8               0.75        50      0.9933425  0.9618182  0.9786306
  0.4  3          0.8               0.75       100      0.9941183  0.9618182  0.9839640
  0.4  3          0.8               0.75       150      0.9941392  0.9618182  0.9893333
  0.4  3          0.8               1.00        50      0.9920432  0.9618182  0.9920000
  0.4  3          0.8               1.00       100      0.9922857  0.9618182  0.9893333
  0.4  3          0.8               1.00       150      0.9922857  0.9618182  0.9893333

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 50, max_depth = 1, eta = 0.4, gamma = 0, colsample_bytree = 0.6, min_child_weight = 1 and subsample = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     10.6      0.7
  positive      1.6     87.1
                            
 Accuracy (average) : 0.9765

[1] "TEST accuracy: 0.976525821596244"
[1] "TEST +precision: 0.981481481481482"
[1] "TEST -precision: 0.9375"
[1] "TEST specifity: 0.865384615384615"
[1] "TEST sensitivity: 0.991978609625668"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        0        0
            positive        9      122
[1] "TEST accuracy: 0.931297709923664"
[1] "TEST +precision: 0.931297709923664"
[1] "TEST -precision: NaN"
[1] "TEST specifity: 0"
[1] "TEST sensitivity: 1"

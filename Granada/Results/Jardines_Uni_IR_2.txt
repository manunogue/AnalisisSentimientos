[1] "DATASET NAME: Jardines_Uni_IR_2"
[1] "TRAIN INSTANCES: 987"
[1] "TEST INSTANCES: 223"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 3.36300301551819"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

987 samples
736 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 790, 790, 789, 789, 790 
Resampling results:

  ROC  Sens  Spec
  1    1     1   

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     33.8      0.0
  positive      0.0     66.2
                       
 Accuracy (average) : 1

[1] "TEST accuracy: 1"
[1] "TEST +precision: 1"
[1] "TEST -precision: 1"
[1] "TEST specifity: 1"
[1] "TEST sensitivity: 1"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        1        0
            positive        3      219
[1] "TEST accuracy: 0.986547085201794"
[1] "TEST +precision: 0.986486486486487"
[1] "TEST -precision: 1"
[1] "TEST specifity: 0.25"
[1] "TEST sensitivity: 1"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 1.53775728146235"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

987 samples
736 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 790, 789, 790, 789, 790 
Resampling results across tuning parameters:

  C      M  ROC        Sens  Spec     
  0.010  1  0.9912606  1     0.9755138
  0.010  2  0.9912606  1     0.9755138
  0.010  3  0.9912606  1     0.9755138
  0.255  1  0.9929800  1     0.9785907
  0.255  2  0.9929800  1     0.9785907
  0.255  3  0.9933560  1     0.9785907
  0.500  1  0.9949596  1     0.9831826
  0.500  2  0.9949596  1     0.9831826
  0.500  3  0.9940510  1     0.9801174

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.5 and M = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     33.8      1.1
  positive      0.0     65.0
                            
 Accuracy (average) : 0.9889

[1] "TEST accuracy: 0.988855116514691"
[1] "TEST +precision: 1"
[1] "TEST -precision: 0.968115942028986"
[1] "TEST specifity: 1"
[1] "TEST sensitivity: 0.983154670750383"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        1        3
            positive        3      216
[1] "TEST accuracy: 0.973094170403587"
[1] "TEST +precision: 0.986301369863014"
[1] "TEST -precision: 0.25"
[1] "TEST specifity: 0.25"
[1] "TEST sensitivity: 0.986301369863014"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 3.74470068216324"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

987 samples
736 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 789, 791, 790, 789, 789 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens       Spec     
  0.3  1          0.6               0.50        50      0.9935703  0.9641791  0.9739636
  0.3  1          0.6               0.50       100      0.9973568  1.0000000  0.9846741
  0.3  1          0.6               0.50       150      0.9985948  1.0000000  0.9877393
  0.3  1          0.6               0.75        50      0.9954394  0.9761194  0.9816324
  0.3  1          0.6               0.75       100      0.9976624  1.0000000  0.9862126
  0.3  1          0.6               0.75       150      0.9983941  1.0000000  0.9846741
  0.3  1          0.6               1.00        50      0.9936604  1.0000000  0.9846858
  0.3  1          0.6               1.00       100      0.9975082  1.0000000  0.9862126
  0.3  1          0.6               1.00       150      0.9982146  1.0000000  0.9846741
  0.3  1          0.8               0.50        50      0.9948870  0.9641791  0.9800822
  0.3  1          0.8               0.50       100      0.9968830  1.0000000  0.9800705
  0.3  1          0.8               0.50       150      0.9974372  1.0000000  0.9846624
  0.3  1          0.8               0.75        50      0.9937641  0.9641791  0.9831591
  0.3  1          0.8               0.75       100      0.9960927  1.0000000  0.9831356
  0.3  1          0.8               0.75       150      0.9973686  1.0000000  0.9846624
  0.3  1          0.8               1.00        50      0.9944625  1.0000000  0.9816324
  0.3  1          0.8               1.00       100      0.9975082  1.0000000  0.9862126
  0.3  1          0.8               1.00       150      0.9979867  1.0000000  0.9846741
  0.3  2          0.6               0.50        50      0.9985763  1.0000000  0.9862126
  0.3  2          0.6               0.50       100      0.9985993  1.0000000  0.9892777
  0.3  2          0.6               0.50       150      0.9994259  1.0000000  0.9908045
  0.3  2          0.6               0.75        50      0.9995888  1.0000000  0.9923547
  0.3  2          0.6               0.75       100      0.9994719  1.0000000  0.9908162
  0.3  2          0.6               0.75       150      0.9996556  1.0000000  0.9923429
  0.3  2          0.6               1.00        50      0.9991504  1.0000000  0.9908045
  0.3  2          0.6               1.00       100      0.9992423  1.0000000  0.9908162
  0.3  2          0.6               1.00       150      0.9994259  1.0000000  0.9892895
  0.3  2          0.8               0.50        50      0.9989208  1.0000000  0.9892660
  0.3  2          0.8               0.50       100      0.9992193  1.0000000  0.9892777
  0.3  2          0.8               0.50       150      0.9996326  1.0000000  0.9877393
  0.3  2          0.8               0.75        50      0.9989437  1.0000000  0.9892660
  0.3  2          0.8               0.75       100      0.9985993  1.0000000  0.9908045
  0.3  2          0.8               0.75       150      0.9991274  1.0000000  0.9892777
  0.3  2          0.8               1.00        50      0.9991281  1.0000000  0.9892777
  0.3  2          0.8               1.00       100      0.9988978  1.0000000  0.9908045
  0.3  2          0.8               1.00       150      0.9991274  1.0000000  0.9892777
  0.3  3          0.6               0.50        50      1.0000000  1.0000000  0.9923429
  0.3  3          0.6               0.50       100      1.0000000  1.0000000  0.9908045
  0.3  3          0.6               0.50       150      1.0000000  1.0000000  0.9908045
  0.3  3          0.6               0.75        50      1.0000000  1.0000000  0.9923312
  0.3  3          0.6               0.75       100      1.0000000  1.0000000  0.9908045
  0.3  3          0.6               0.75       150      1.0000000  1.0000000  0.9892777
  0.3  3          0.6               1.00        50      1.0000000  1.0000000  0.9923547
  0.3  3          0.6               1.00       100      1.0000000  1.0000000  0.9908162
  0.3  3          0.6               1.00       150      1.0000000  1.0000000  0.9923547
  0.3  3          0.8               0.50        50      0.9994259  1.0000000  0.9892777
  0.3  3          0.8               0.50       100      0.9999082  1.0000000  0.9892777
  0.3  3          0.8               0.50       150      0.9999082  1.0000000  0.9892777
  0.3  3          0.8               0.75        50      0.9994259  1.0000000  0.9908162
  0.3  3          0.8               0.75       100      0.9996556  1.0000000  0.9892777
  0.3  3          0.8               0.75       150      0.9999082  1.0000000  0.9877510
  0.3  3          0.8               1.00        50      0.9998622  1.0000000  0.9923312
  0.3  3          0.8               1.00       100      0.9999082  1.0000000  0.9908162
  0.3  3          0.8               1.00       150      1.0000000  1.0000000  0.9892777
  0.4  1          0.6               0.50        50      0.9971103  0.9761194  0.9785555
  0.4  1          0.6               0.50       100      0.9983262  1.0000000  0.9877393
  0.4  1          0.6               0.50       150      0.9990158  1.0000000  0.9846741
  0.4  1          0.6               0.75        50      0.9964979  1.0000000  0.9862126
  0.4  1          0.6               0.75       100      0.9976896  1.0000000  0.9877393
  0.4  1          0.6               0.75       150      0.9985993  1.0000000  0.9846624
  0.4  1          0.6               1.00        50      0.9958546  1.0000000  0.9846858
  0.4  1          0.6               1.00       100      0.9982146  1.0000000  0.9862008
  0.4  1          0.6               1.00       150      0.9985555  1.0000000  0.9877393
  0.4  1          0.8               0.50        50      0.9948518  1.0000000  0.9816324
  0.4  1          0.8               0.50       100      0.9980069  1.0000000  0.9831356
  0.4  1          0.8               0.50       150      0.9981662  1.0000000  0.9831356
  0.4  1          0.8               0.75        50      0.9958041  1.0000000  0.9846858
  0.4  1          0.8               0.75       100      0.9972319  1.0000000  0.9831356
  0.4  1          0.8               0.75       150      0.9978244  1.0000000  0.9846624
  0.4  1          0.8               1.00        50      0.9969841  1.0000000  0.9831709
  0.4  1          0.8               1.00       100      0.9981234  1.0000000  0.9862126
  0.4  1          0.8               1.00       150      0.9986466  1.0000000  0.9877393
  0.4  2          0.6               0.50        50      0.9983720  1.0000000  0.9877158
  0.4  2          0.6               0.50       100      0.9991763  1.0000000  0.9877275
  0.4  2          0.6               0.50       150      0.9987658  1.0000000  0.9892543
  0.4  2          0.6               0.75        50      0.9999082  1.0000000  0.9877393
  0.4  2          0.6               0.75       100      1.0000000  1.0000000  0.9908045
  0.4  2          0.6               0.75       150      1.0000000  1.0000000  0.9923312
  0.4  2          0.6               1.00        50      0.9991069  1.0000000  0.9892777
  0.4  2          0.6               1.00       100      0.9991274  1.0000000  0.9908162
  0.4  2          0.6               1.00       150      0.9995637  1.0000000  0.9892777
  0.4  2          0.8               0.50        50      0.9997217  1.0000000  0.9907927
  0.4  2          0.8               0.50       100      1.0000000  1.0000000  0.9892777
  0.4  2          0.8               0.50       150      1.0000000  1.0000000  0.9892777
  0.4  2          0.8               0.75        50      0.9992193  1.0000000  0.9908045
  0.4  2          0.8               0.75       100      0.9990126  1.0000000  0.9892777
  0.4  2          0.8               0.75       150      0.9991274  1.0000000  0.9892777
  0.4  2          0.8               1.00        50      0.9990356  1.0000000  0.9923312
  0.4  2          0.8               1.00       100      0.9988289  1.0000000  0.9923429
  0.4  2          0.8               1.00       150      0.9992193  1.0000000  0.9892777
  0.4  3          0.6               0.50        50      1.0000000  1.0000000  0.9892660
  0.4  3          0.6               0.50       100      1.0000000  1.0000000  0.9877393
  0.4  3          0.6               0.50       150      1.0000000  1.0000000  0.9892777
  0.4  3          0.6               0.75        50      1.0000000  1.0000000  0.9892660
  0.4  3          0.6               0.75       100      1.0000000  1.0000000  0.9908045
  0.4  3          0.6               0.75       150      1.0000000  1.0000000  0.9908045
  0.4  3          0.6               1.00        50      0.9997933  1.0000000  0.9938696
  0.4  3          0.6               1.00       100      0.9999082  1.0000000  0.9908162
  0.4  3          0.6               1.00       150      0.9999082  1.0000000  0.9892777
  0.4  3          0.8               0.50        50      0.9994726  1.0000000  0.9907927
  0.4  3          0.8               0.50       100      1.0000000  1.0000000  0.9923312
  0.4  3          0.8               0.50       150      1.0000000  1.0000000  0.9908045
  0.4  3          0.8               0.75        50      1.0000000  1.0000000  0.9892777
  0.4  3          0.8               0.75       100      1.0000000  1.0000000  0.9892777
  0.4  3          0.8               0.75       150      1.0000000  1.0000000  0.9892777
  0.4  3          0.8               1.00        50      0.9997704  1.0000000  0.9923547
  0.4  3          0.8               1.00       100      1.0000000  1.0000000  0.9908162
  0.4  3          0.8               1.00       150      1.0000000  1.0000000  0.9908162

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 50, max_depth = 3, eta = 0.3, gamma = 0, colsample_bytree = 0.6, min_child_weight = 1 and subsample = 0.5.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     33.8      0.5
  positive      0.0     65.7
                            
 Accuracy (average) : 0.9949

[1] "TEST accuracy: 0.994934143870314"
[1] "TEST +precision: 1"
[1] "TEST -precision: 0.985250737463127"
[1] "TEST specifity: 1"
[1] "TEST sensitivity: 0.992343032159265"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        1        0
            positive        3      219
[1] "TEST accuracy: 0.986547085201794"
[1] "TEST +precision: 0.986486486486487"
[1] "TEST -precision: 1"
[1] "TEST specifity: 0.25"
[1] "TEST sensitivity: 1"

[1] "DATASET NAME: TCT_Uni_IR_10"
[1] "TRAIN INSTANCES: 732"
[1] "TEST INSTANCES: 225"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 2.37528204917908"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

732 samples
725 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 585, 586, 585, 586, 586 
Resampling results:

  ROC        Sens  Spec    
  0.9973853  0.96  0.992076

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     13.1      0.7
  positive      0.5     85.7
                            
 Accuracy (average) : 0.9877

[1] "TRAIN accuracy: 0.987704918032787"
[1] "TRAIN +precision: 0.993660855784469"
[1] "TRAIN -precision: 0.95049504950495"
[1] "TRAIN specifity: 0.96"
[1] "TRAIN sensitivity: 0.992088607594937"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       13        1
            positive        6      205
[1] "TEST accuracy: 0.968888888888889"
[1] "TEST +precision: 0.971563981042654"
[1] "TEST -precision: 0.928571428571429"
[1] "TEST specifity: 0.684210526315789"
[1] "TEST sensitivity: 0.995145631067961"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 1.13995610078176"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

732 samples
725 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 586, 585, 586, 585, 586 
Resampling results across tuning parameters:

  C      M  ROC        Sens  Spec     
  0.010  1  0.7131680  0.46  0.9746907
  0.010  2  0.6513926  0.34  0.9794401
  0.010  3  0.6506871  0.34  0.9778653
  0.255  1  0.8902487  0.78  0.9731159
  0.255  2  0.8860036  0.71  0.9746907
  0.255  3  0.8468938  0.67  0.9715286
  0.500  1  0.8914392  0.78  0.9731159
  0.500  2  0.8871941  0.71  0.9746907
  0.500  3  0.8468938  0.67  0.9715286

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.5 and M = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     10.7      2.3
  positive      3.0     84.0
                            
 Accuracy (average) : 0.9467

[1] "TRAIN accuracy: 0.94672131147541"
[1] "TRAIN +precision: 0.965463108320251"
[1] "TRAIN -precision: 0.821052631578947"
[1] "TRAIN specifity: 0.78"
[1] "TRAIN sensitivity: 0.973101265822785"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        8        7
            positive       11      199
[1] "TEST accuracy: 0.92"
[1] "TEST +precision: 0.947619047619048"
[1] "TEST -precision: 0.533333333333333"
[1] "TEST specifity: 0.421052631578947"
[1] "TEST sensitivity: 0.966019417475728"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 2.74424943526586"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

732 samples
725 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 585, 585, 586, 586, 586 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens  Spec     
  0.3  1          0.6               0.50        50      0.9857605  0.63  0.9936883
  0.3  1          0.6               0.50       100      0.9891620  0.75  0.9905387
  0.3  1          0.6               0.50       150      0.9888351  0.82  0.9921135
  0.3  1          0.6               0.75        50      0.9818070  0.62  0.9905137
  0.3  1          0.6               0.75       100      0.9890442  0.81  0.9889389
  0.3  1          0.6               0.75       150      0.9885921  0.83  0.9921010
  0.3  1          0.6               1.00        50      0.9854037  0.65  0.9937008
  0.3  1          0.6               1.00       100      0.9879237  0.82  0.9905262
  0.3  1          0.6               1.00       150      0.9876812  0.85  0.9921135
  0.3  1          0.8               0.50        50      0.9735880  0.64  0.9952506
  0.3  1          0.8               0.50       100      0.9889289  0.78  0.9905262
  0.3  1          0.8               0.50       150      0.9839283  0.77  0.9921135
  0.3  1          0.8               0.75        50      0.9854462  0.64  0.9952506
  0.3  1          0.8               0.75       100      0.9892288  0.81  0.9921010
  0.3  1          0.8               0.75       150      0.9881190  0.85  0.9905137
  0.3  1          0.8               1.00        50      0.9854037  0.63  0.9936883
  0.3  1          0.8               1.00       100      0.9874091  0.83  0.9905387
  0.3  1          0.8               1.00       150      0.9873247  0.85  0.9905262
  0.3  2          0.6               0.50        50      0.9823385  0.76  0.9921135
  0.3  2          0.6               0.50       100      0.9878890  0.77  0.9936883
  0.3  2          0.6               0.50       150      0.9855156  0.77  0.9905137
  0.3  2          0.6               0.75        50      0.9928784  0.82  0.9952631
  0.3  2          0.6               0.75       100      0.9902593  0.83  0.9905137
  0.3  2          0.6               0.75       150      0.9874003  0.82  0.9921010
  0.3  2          0.6               1.00        50      0.9892307  0.80  0.9905387
  0.3  2          0.6               1.00       100      0.9868479  0.83  0.9921135
  0.3  2          0.6               1.00       150      0.9859743  0.84  0.9936883
  0.3  2          0.8               0.50        50      0.9870935  0.81  0.9952756
  0.3  2          0.8               0.50       100      0.9850300  0.83  0.9921010
  0.3  2          0.8               0.50       150      0.9850325  0.85  0.9889389
  0.3  2          0.8               0.75        50      0.9899444  0.81  0.9936883
  0.3  2          0.8               0.75       100      0.9841445  0.81  0.9921135
  0.3  2          0.8               0.75       150      0.9839120  0.85  0.9905262
  0.3  2          0.8               1.00        50      0.9894657  0.82  0.9952631
  0.3  2          0.8               1.00       100      0.9881940  0.84  0.9905262
  0.3  2          0.8               1.00       150      0.9883552  0.85  0.9936883
  0.3  3          0.6               0.50        50      0.9816904  0.79  0.9936883
  0.3  3          0.6               0.50       100      0.9820054  0.82  0.9921010
  0.3  3          0.6               0.50       150      0.9828815  0.84  0.9921010
  0.3  3          0.6               0.75        50      0.9859099  0.85  0.9921135
  0.3  3          0.6               0.75       100      0.9850362  0.85  0.9905262
  0.3  3          0.6               0.75       150      0.9859149  0.89  0.9905262
  0.3  3          0.6               1.00        50      0.9893807  0.86  0.9952631
  0.3  3          0.6               1.00       100      0.9880352  0.89  0.9905137
  0.3  3          0.6               1.00       150      0.9875697  0.88  0.9905137
  0.3  3          0.8               0.50        50      0.9851175  0.78  0.9905262
  0.3  3          0.8               0.50       100      0.9833127  0.81  0.9889389
  0.3  3          0.8               0.50       150      0.9836364  0.86  0.9857643
  0.3  3          0.8               0.75        50      0.9916823  0.81  0.9936883
  0.3  3          0.8               0.75       100      0.9893895  0.87  0.9905137
  0.3  3          0.8               0.75       150      0.9886802  0.89  0.9905137
  0.3  3          0.8               1.00        50      0.9869229  0.84  0.9936883
  0.3  3          0.8               1.00       100      0.9852725  0.87  0.9921010
  0.3  3          0.8               1.00       150      0.9864717  0.88  0.9889264
  0.4  1          0.6               0.50        50      0.9828553  0.70  0.9905262
  0.4  1          0.6               0.50       100      0.9900244  0.83  0.9905262
  0.4  1          0.6               0.50       150      0.9891489  0.83  0.9857893
  0.4  1          0.6               0.75        50      0.9867854  0.74  0.9873516
  0.4  1          0.6               0.75       100      0.9912545  0.83  0.9921135
  0.4  1          0.6               0.75       150      0.9899481  0.84  0.9921010
  0.4  1          0.6               1.00        50      0.9878131  0.72  0.9905387
  0.4  1          0.6               1.00       100      0.9885586  0.85  0.9921135
  0.4  1          0.6               1.00       150      0.9878796  0.87  0.9905262
  0.4  1          0.8               0.50        50      0.9849166  0.72  0.9920885
  0.4  1          0.8               0.50       100      0.9909049  0.79  0.9905137
  0.4  1          0.8               0.50       150      0.9901825  0.81  0.9921135
  0.4  1          0.8               0.75        50      0.9866998  0.73  0.9905262
  0.4  1          0.8               0.75       100      0.9890745  0.83  0.9921135
  0.4  1          0.8               0.75       150      0.9889908  0.83  0.9921135
  0.4  1          0.8               1.00        50      0.9859939  0.76  0.9905387
  0.4  1          0.8               1.00       100      0.9881996  0.87  0.9905387
  0.4  1          0.8               1.00       150      0.9862955  0.87  0.9905262
  0.4  2          0.6               0.50        50      0.9890789  0.76  0.9889389
  0.4  2          0.6               0.50       100      0.9851162  0.79  0.9905262
  0.4  2          0.6               0.50       150      0.9840945  0.82  0.9857768
  0.4  2          0.6               0.75        50      0.9900553  0.82  0.9952631
  0.4  2          0.6               0.75       100      0.9869229  0.85  0.9936883
  0.4  2          0.6               0.75       150      0.9858155  0.88  0.9857768
  0.4  2          0.6               1.00        50      0.9864536  0.83  0.9936883
  0.4  2          0.6               1.00       100      0.9847844  0.83  0.9921135
  0.4  2          0.6               1.00       150      0.9847069  0.86  0.9889514
  0.4  2          0.8               0.50        50      0.9859811  0.81  0.9921135
  0.4  2          0.8               0.50       100      0.9883640  0.83  0.9905262
  0.4  2          0.8               0.50       150      0.9876553  0.88  0.9873641
  0.4  2          0.8               0.75        50      0.9884389  0.83  0.9921010
  0.4  2          0.8               0.75       100      0.9852012  0.82  0.9889389
  0.4  2          0.8               0.75       150      0.9831440  0.86  0.9857893
  0.4  2          0.8               1.00        50      0.9864070  0.86  0.9936883
  0.4  2          0.8               1.00       100      0.9861305  0.87  0.9905262
  0.4  2          0.8               1.00       150      0.9858224  0.87  0.9921010
  0.4  3          0.6               0.50        50      0.9836939  0.78  0.9873391
  0.4  3          0.6               0.50       100      0.9808418  0.85  0.9825897
  0.4  3          0.6               0.50       150      0.9822703  0.84  0.9841895
  0.4  3          0.6               0.75        50      0.9887577  0.86  0.9921010
  0.4  3          0.6               0.75       100      0.9840957  0.90  0.9905137
  0.4  3          0.6               0.75       150      0.9835439  0.91  0.9889264
  0.4  3          0.6               1.00        50      0.9860524  0.84  0.9905137
  0.4  3          0.6               1.00       100      0.9847100  0.88  0.9905137
  0.4  3          0.6               1.00       150      0.9838383  0.89  0.9873391
  0.4  3          0.8               0.50        50      0.9843970  0.81  0.9857643
  0.4  3          0.8               0.50       100      0.9886808  0.88  0.9873391
  0.4  3          0.8               0.50       150      0.9856680  0.87  0.9841895
  0.4  3          0.8               0.75        50      0.9878721  0.85  0.9921010
  0.4  3          0.8               0.75       100      0.9877234  0.88  0.9905262
  0.4  3          0.8               0.75       150      0.9875709  0.89  0.9889389
  0.4  3          0.8               1.00        50      0.9867660  0.85  0.9936883
  0.4  3          0.8               1.00       100      0.9869329  0.88  0.9921010
  0.4  3          0.8               1.00       150      0.9872516  0.89  0.9889264

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 50, max_depth = 2, eta = 0.3, gamma = 0, colsample_bytree = 0.6, min_child_weight = 1 and subsample = 0.75.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     11.2      0.4
  positive      2.5     85.9
                            
 Accuracy (average) : 0.9713

[1] "TRAIN accuracy: 0.971311475409836"
[1] "TRAIN +precision: 0.972179289026275"
[1] "TRAIN -precision: 0.964705882352941"
[1] "TRAIN specifity: 0.82"
[1] "TRAIN sensitivity: 0.995253164556962"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        6        3
            positive       13      203
[1] "TEST accuracy: 0.928888888888889"
[1] "TEST +precision: 0.939814814814815"
[1] "TEST -precision: 0.666666666666667"
[1] "TEST specifity: 0.315789473684211"
[1] "TEST sensitivity: 0.985436893203884"

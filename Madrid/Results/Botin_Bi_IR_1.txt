[1] "DATASET NAME: Botin_Bi_IR_1"
[1] "TRAIN INSTANCES: 2932"
[1] "TEST INSTANCES: 573"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 12.4008550643921"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

2932 samples
 834 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 2346, 2346, 2345, 2346, 2345 
Resampling results:

  ROC        Sens       Spec     
  0.9887074  0.9754406  0.9584036

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     48.8      2.1
  positive      1.2     47.9
                            
 Accuracy (average) : 0.9669

[1] "TRAIN accuracy: 0.966916780354707"
[1] "TRAIN +precision: 0.975017349063151"
[1] "TRAIN -precision: 0.959087860496311"
[1] "TRAIN specifity: 0.975443383356071"
[1] "TRAIN sensitivity: 0.958390177353342"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       43       15
            positive       49      466
[1] "TEST accuracy: 0.888307155322862"
[1] "TEST +precision: 0.904854368932039"
[1] "TEST -precision: 0.741379310344828"
[1] "TEST specifity: 0.467391304347826"
[1] "TEST sensitivity: 0.968814968814969"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 7.74022241830826"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

2932 samples
 834 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 2345, 2345, 2346, 2346, 2346 
Resampling results across tuning parameters:

  C      M  ROC        Sens       Spec     
  0.010  1  0.9309048  0.9119895  0.7925727
  0.010  2  0.9334025  0.9543103  0.7633187
  0.010  3  0.9313132  0.9495322  0.7571800
  0.255  1  0.9425578  0.9583896  0.8035523
  0.255  2  0.9491086  0.9133384  0.8239465
  0.255  3  0.9485183  0.9017483  0.8341877
  0.500  1  0.9441117  0.9583896  0.8096933
  0.500  2  0.9558745  0.9556616  0.8240138
  0.500  3  0.9507560  0.9406608  0.8212951

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.5 and M = 2.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     47.8      8.8
  positive      2.2     41.2
                            
 Accuracy (average) : 0.8898

[1] "TRAIN accuracy: 0.889836289222374"
[1] "TRAIN +precision: 0.948939512961508"
[1] "TRAIN -precision: 0.844484629294756"
[1] "TRAIN specifity: 0.955661664392906"
[1] "TRAIN sensitivity: 0.824010914051842"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       25       16
            positive       67      465
[1] "TEST accuracy: 0.855148342059337"
[1] "TEST +precision: 0.87406015037594"
[1] "TEST -precision: 0.609756097560976"
[1] "TEST specifity: 0.271739130434783"
[1] "TEST sensitivity: 0.966735966735967"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 12.5493452986081"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

2932 samples
 834 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 2345, 2346, 2346, 2345, 2346 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens       Spec     
  0.3  1          0.6               0.50        50      0.8381375  0.5600195  0.9270019
  0.3  1          0.6               0.50       100      0.8925348  0.7087228  0.9160827
  0.3  1          0.6               0.50       150      0.9219103  0.7987950  0.9113139
  0.3  1          0.6               0.75        50      0.8482801  0.5852685  0.9338279
  0.3  1          0.6               0.75       100      0.9082772  0.7278354  0.9242785
  0.3  1          0.6               0.75       150      0.9324102  0.8042395  0.9249541
  0.3  1          0.6               1.00        50      0.8493704  0.5954842  0.9345058
  0.3  1          0.6               1.00       100      0.9065998  0.7121265  0.9310952
  0.3  1          0.6               1.00       150      0.9344033  0.7980915  0.9270042
  0.3  1          0.8               0.50        50      0.8423495  0.5743633  0.9290590
  0.3  1          0.8               0.50       100      0.9037818  0.7244248  0.9222261
  0.3  1          0.8               0.50       150      0.9257245  0.8117225  0.9160944
  0.3  1          0.8               0.75        50      0.8469958  0.5961691  0.9365513
  0.3  1          0.8               0.75       100      0.9027363  0.7278145  0.9235913
  0.3  1          0.8               0.75       150      0.9316775  0.7905760  0.9229087
  0.3  1          0.8               1.00        50      0.8484535  0.5948086  0.9345058
  0.3  1          0.8               1.00       100      0.9079043  0.7148499  0.9304149
  0.3  1          0.8               1.00       150      0.9348493  0.7967263  0.9270042
  0.3  2          0.6               0.50        50      0.9078535  0.7053122  0.9310928
  0.3  2          0.6               0.50       100      0.9436892  0.8424137  0.9208725
  0.3  2          0.6               0.50       150      0.9602714  0.8772120  0.9201946
  0.3  2          0.6               0.75        50      0.9117492  0.7332730  0.9317754
  0.3  2          0.6               0.75       100      0.9496427  0.8451371  0.9256483
  0.3  2          0.6               0.75       150      0.9643685  0.8765387  0.9270135
  0.3  2          0.6               1.00        50      0.9137232  0.7250702  0.9324627
  0.3  2          0.6               1.00       100      0.9521177  0.8506234  0.9290474
  0.3  2          0.6               1.00       150      0.9687476  0.8772190  0.9345035
  0.3  2          0.8               0.50        50      0.8995700  0.7189594  0.9222354
  0.3  2          0.8               0.50       100      0.9454838  0.8615124  0.9119965
  0.3  2          0.8               0.50       150      0.9588834  0.8833554  0.9188201
  0.3  2          0.8               0.75        50      0.9085072  0.7400873  0.9229087
  0.3  2          0.8               0.75       100      0.9510558  0.8519723  0.9249611
  0.3  2          0.8               0.75       150      0.9666423  0.8772097  0.9283741
  0.3  2          0.8               1.00        50      0.9145078  0.7271319  0.9331430
  0.3  2          0.8               1.00       100      0.9530224  0.8499338  0.9304219
  0.3  2          0.8               1.00       150      0.9699137  0.8778970  0.9345035
  0.3  3          0.6               0.50        50      0.9312567  0.7987416  0.9222261
  0.3  3          0.6               0.50       100      0.9627677  0.8717490  0.9215528
  0.3  3          0.6               0.50       150      0.9735246  0.9194911  0.9290497
  0.3  3          0.6               0.75        50      0.9403631  0.8240115  0.9304219
  0.3  3          0.6               0.75       100      0.9692779  0.8860788  0.9338348
  0.3  3          0.6               0.75       150      0.9788625  0.9399619  0.9297300
  0.3  3          0.6               1.00        50      0.9409374  0.8062664  0.9304149
  0.3  3          0.6               1.00       100      0.9694029  0.8744910  0.9338186
  0.3  3          0.6               1.00       150      0.9807910  0.9201946  0.9372269
  0.3  3          0.8               0.50        50      0.9300341  0.7987625  0.9174503
  0.3  3          0.8               0.50       100      0.9654582  0.8744840  0.9256414
  0.3  3          0.8               0.50       150      0.9739346  0.9229157  0.9304149
  0.3  3          0.8               0.75        50      0.9398965  0.8137796  0.9263147
  0.3  3          0.8               0.75       100      0.9690395  0.8772190  0.9351884
  0.3  3          0.8               0.75       150      0.9779438  0.9365559  0.9317685
  0.3  3          0.8               1.00        50      0.9410889  0.8137703  0.9304126
  0.3  3          0.8               1.00       100      0.9706952  0.8819879  0.9317801
  0.3  3          0.8               1.00       150      0.9808005  0.9208772  0.9358710
  0.4  1          0.6               0.50        50      0.8598317  0.6644331  0.8621509
  0.4  1          0.6               0.50       100      0.9172853  0.7516821  0.9201876
  0.4  1          0.6               0.50       150      0.9410957  0.8301549  0.9092800
  0.4  1          0.6               0.75        50      0.8682674  0.6343758  0.9297300
  0.4  1          0.6               0.75       100      0.9238078  0.7810313  0.9242762
  0.4  1          0.6               0.75       150      0.9475454  0.8424323  0.9215365
  0.4  1          0.6               1.00        50      0.8649162  0.6350561  0.9331430
  0.4  1          0.6               1.00       100      0.9213701  0.7591976  0.9297346
  0.4  1          0.6               1.00       150      0.9494272  0.8437858  0.9263217
  0.4  1          0.8               0.50        50      0.8645538  0.6262125  0.9317847
  0.4  1          0.8               0.50       100      0.9152475  0.7660050  0.9140373
  0.4  1          0.8               0.50       150      0.9385525  0.8369645  0.9079009
  0.4  1          0.8               0.75        50      0.8690597  0.6241508  0.9379211
  0.4  1          0.8               0.75       100      0.9244816  0.7687609  0.9256460
  0.4  1          0.8               0.75       150      0.9442724  0.8349121  0.9126814
  0.4  1          0.8               1.00        50      0.8704314  0.6343828  0.9358733
  0.4  1          0.8               1.00       100      0.9234012  0.7667085  0.9256391
  0.4  1          0.8               1.00       150      0.9475268  0.8437905  0.9235936
  0.4  2          0.6               0.50        50      0.9183529  0.7701052  0.9195027
  0.4  2          0.6               0.50       100      0.9568564  0.8744747  0.9160920
  0.4  2          0.6               0.50       150      0.9672626  0.9085928  0.9208655
  0.4  2          0.6               0.75        50      0.9252108  0.7694133  0.9297300
  0.4  2          0.6               0.75       100      0.9625500  0.8847206  0.9263217
  0.4  2          0.6               0.75       150      0.9736263  0.9072253  0.9304103
  0.4  2          0.6               1.00        50      0.9313273  0.7694319  0.9310975
  0.4  2          0.6               1.00       100      0.9653376  0.8833600  0.9310975
  0.4  2          0.6               1.00       150      0.9767350  0.9031436  0.9351838
  0.4  2          0.8               0.50        50      0.9186399  0.7701214  0.9270019
  0.4  2          0.8               0.50       100      0.9521090  0.8744770  0.9160967
  0.4  2          0.8               0.50       150      0.9671600  0.9147268  0.9201830
  0.4  2          0.8               0.75        50      0.9261126  0.7605489  0.9324627
  0.4  2          0.8               0.75       100      0.9631878  0.8826774  0.9249634
  0.4  2          0.8               0.75       150      0.9752384  0.9120081  0.9365513
  0.4  2          0.8               1.00        50      0.9278993  0.7769288  0.9311021
  0.4  2          0.8               1.00       100      0.9621599  0.8785726  0.9338279
  0.4  2          0.8               1.00       150      0.9768785  0.9017808  0.9406445
  0.4  3          0.6               0.50        50      0.9441150  0.8369715  0.9085742
  0.4  3          0.6               0.50       100      0.9699770  0.9099557  0.9215481
  0.4  3          0.6               0.50       150      0.9762240  0.9399735  0.9263193
  0.4  3          0.6               0.75        50      0.9507964  0.8355924  0.9263193
  0.4  3          0.6               0.75       100      0.9753303  0.9106475  0.9345035
  0.4  3          0.6               0.75       150      0.9820248  0.9474821  0.9351861
  0.4  3          0.6               1.00        50      0.9581795  0.8471895  0.9269973
  0.4  3          0.6               1.00       100      0.9783068  0.9133756  0.9406468
  0.4  3          0.6               1.00       150      0.9854005  0.9515869  0.9447354
  0.4  3          0.8               0.50        50      0.9458174  0.8287781  0.9195027
  0.4  3          0.8               0.50       100      0.9694650  0.9140489  0.9263170
  0.4  3          0.8               0.50       150      0.9759593  0.9447354  0.9283764
  0.4  3          0.8               0.75        50      0.9506724  0.8444638  0.9263170
  0.4  3          0.8               0.75       100      0.9756405  0.9092661  0.9310952
  0.4  3          0.8               0.75       150      0.9822404  0.9522579  0.9351861
  0.4  3          0.8               1.00        50      0.9529640  0.8485524  0.9290497
  0.4  3          0.8               1.00       100      0.9775319  0.9120150  0.9365559
  0.4  3          0.8               1.00       150      0.9841606  0.9433958  0.9345035

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 150, max_depth = 3, eta = 0.4, gamma = 0, colsample_bytree = 0.6, min_child_weight = 1 and subsample = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     47.6      2.8
  positive      2.4     47.2
                            
 Accuracy (average) : 0.9482

[1] "TRAIN accuracy: 0.948158253751705"
[1] "TRAIN +precision: 0.951236263736264"
[1] "TRAIN -precision: 0.945121951219512"
[1] "TRAIN specifity: 0.951568894952251"
[1] "TRAIN sensitivity: 0.94474761255116"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       42       28
            positive       50      453
[1] "TEST accuracy: 0.863874345549738"
[1] "TEST +precision: 0.900596421471173"
[1] "TEST -precision: 0.6"
[1] "TEST specifity: 0.456521739130435"
[1] "TEST sensitivity: 0.941787941787942"

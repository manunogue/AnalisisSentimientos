[1] "DATASET NAME: Botin_Bi_IR_0"
[1] "TRAIN INSTANCES: 1716"
[1] "TEST INSTANCES: 573"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 7.27825999259949"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

1716 samples
 834 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1373, 1373, 1373, 1372, 1373 
Resampling results:

  ROC        Sens   Spec     
  0.9005548  0.516  0.9754731

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative      7.5      2.1
  positive      7.1     83.3
                            
 Accuracy (average) : 0.9085

[1] "TRAIN accuracy: 0.908508158508158"
[1] "TRAIN +precision: 0.921985815602837"
[1] "TRAIN -precision: 0.781818181818182"
[1] "TRAIN specifity: 0.516"
[1] "TRAIN sensitivity: 0.975443383356071"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       41       13
            positive       51      468
[1] "TEST accuracy: 0.888307155322862"
[1] "TEST +precision: 0.901734104046243"
[1] "TEST -precision: 0.759259259259259"
[1] "TEST specifity: 0.445652173913043"
[1] "TEST sensitivity: 0.972972972972973"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 3.45228324731191"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

1716 samples
 834 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1373, 1373, 1373, 1373, 1372 
Resampling results across tuning parameters:

  C      M  ROC        Sens   Spec     
  0.010  1  0.5152355  0.064  0.9856655
  0.010  2  0.5122321  0.040  0.9904437
  0.010  3  0.5122321  0.040  0.9904437
  0.255  1  0.6190380  0.152  0.9774790
  0.255  2  0.6236224  0.152  0.9774767
  0.255  3  0.5872387  0.144  0.9781616
  0.500  1  0.6193250  0.140  0.9686216
  0.500  2  0.6591971  0.136  0.9692995
  0.500  3  0.6244543  0.188  0.9699821

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.5 and M = 2.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative      2.0      2.6
  positive     12.6     82.8
                            
 Accuracy (average) : 0.8479

[1] "TRAIN accuracy: 0.847902097902098"
[1] "TRAIN +precision: 0.868051313378131"
[1] "TRAIN -precision: 0.430379746835443"
[1] "TRAIN specifity: 0.136"
[1] "TRAIN sensitivity: 0.969304229195089"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       12       15
            positive       80      466
[1] "TEST accuracy: 0.834205933682374"
[1] "TEST +precision: 0.853479853479853"
[1] "TEST -precision: 0.444444444444444"
[1] "TEST specifity: 0.130434782608696"
[1] "TEST sensitivity: 0.968814968814969"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 7.65404574871063"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

1716 samples
 834 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1373, 1373, 1372, 1373, 1373 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens   Spec     
  0.3  1          0.6               0.50        50      0.7357662  0.080  0.9931764
  0.3  1          0.6               0.50       100      0.7722319  0.132  0.9884005
  0.3  1          0.6               0.50       150      0.7710784  0.196  0.9781686
  0.3  1          0.6               0.75        50      0.7454355  0.044  0.9938567
  0.3  1          0.6               0.75       100      0.7846122  0.120  0.9945416
  0.3  1          0.6               0.75       150      0.8010841  0.148  0.9863551
  0.3  1          0.6               1.00        50      0.7626050  0.036  0.9979522
  0.3  1          0.6               1.00       100      0.7948845  0.096  0.9972696
  0.3  1          0.6               1.00       150      0.8051443  0.112  0.9952218
  0.3  1          0.8               0.50        50      0.7425290  0.092  0.9931741
  0.3  1          0.8               0.50       100      0.7659707  0.148  0.9843073
  0.3  1          0.8               0.50       150      0.7852386  0.208  0.9808943
  0.3  1          0.8               0.75        50      0.7547460  0.060  0.9965870
  0.3  1          0.8               0.75       100      0.7971445  0.124  0.9918089
  0.3  1          0.8               0.75       150      0.8029700  0.180  0.9863504
  0.3  1          0.8               1.00        50      0.7633030  0.036  0.9986348
  0.3  1          0.8               1.00       100      0.7931497  0.096  0.9979522
  0.3  1          0.8               1.00       150      0.8037134  0.132  0.9952218
  0.3  2          0.6               0.50        50      0.7627014  0.180  0.9843073
  0.3  2          0.6               0.50       100      0.7831029  0.244  0.9781686
  0.3  2          0.6               0.50       150      0.7880975  0.272  0.9747580
  0.3  2          0.6               0.75        50      0.7780057  0.132  0.9924915
  0.3  2          0.6               0.75       100      0.8090853  0.244  0.9829468
  0.3  2          0.6               0.75       150      0.8137315  0.296  0.9747649
  0.3  2          0.6               1.00        50      0.8002731  0.148  0.9924915
  0.3  2          0.6               1.00       100      0.8170926  0.220  0.9870377
  0.3  2          0.6               1.00       150      0.8220442  0.264  0.9836317
  0.3  2          0.8               0.50        50      0.7692400  0.172  0.9822595
  0.3  2          0.8               0.50       100      0.7878125  0.248  0.9781663
  0.3  2          0.8               0.50       150      0.7959403  0.280  0.9754406
  0.3  2          0.8               0.75        50      0.7884856  0.144  0.9890808
  0.3  2          0.8               0.75       100      0.8045158  0.244  0.9795338
  0.3  2          0.8               0.75       150      0.8088621  0.288  0.9761255
  0.3  2          0.8               1.00        50      0.7984210  0.132  0.9959044
  0.3  2          0.8               1.00       100      0.8173292  0.216  0.9877156
  0.3  2          0.8               1.00       150      0.8238993  0.260  0.9856771
  0.3  3          0.6               0.50        50      0.7827420  0.200  0.9829398
  0.3  3          0.6               0.50       100      0.7923253  0.244  0.9761162
  0.3  3          0.6               0.50       150      0.7974230  0.272  0.9733974
  0.3  3          0.6               0.75        50      0.8096295  0.240  0.9849876
  0.3  3          0.6               0.75       100      0.8159206  0.296  0.9774883
  0.3  3          0.6               0.75       150      0.8140832  0.296  0.9720369
  0.3  3          0.6               1.00        50      0.8095602  0.232  0.9897657
  0.3  3          0.6               1.00       100      0.8177033  0.280  0.9843120
  0.3  3          0.6               1.00       150      0.8210900  0.300  0.9809013
  0.3  3          0.8               0.50        50      0.7754922  0.212  0.9754382
  0.3  3          0.8               0.50       100      0.7834893  0.284  0.9740730
  0.3  3          0.8               0.50       150      0.7926341  0.292  0.9727125
  0.3  3          0.8               0.75        50      0.7961883  0.240  0.9836224
  0.3  3          0.8               0.75       100      0.8087776  0.312  0.9768081
  0.3  3          0.8               0.75       150      0.8094144  0.320  0.9768104
  0.3  3          0.8               1.00        50      0.8122291  0.188  0.9904507
  0.3  3          0.8               1.00       100      0.8214049  0.276  0.9829444
  0.3  3          0.8               1.00       150      0.8217780  0.292  0.9802210
  0.4  1          0.6               0.50        50      0.7468840  0.100  0.9904460
  0.4  1          0.6               0.50       100      0.7823520  0.156  0.9849899
  0.4  1          0.6               0.50       150      0.7795064  0.252  0.9761139
  0.4  1          0.6               0.75        50      0.7584500  0.096  0.9931787
  0.4  1          0.6               0.75       100      0.7990433  0.144  0.9904460
  0.4  1          0.6               0.75       150      0.8023720  0.252  0.9795361
  0.4  1          0.6               1.00        50      0.7677296  0.064  0.9965870
  0.4  1          0.6               1.00       100      0.8040362  0.136  0.9965870
  0.4  1          0.6               1.00       150      0.8100621  0.164  0.9918112
  0.4  1          0.8               0.50        50      0.7550589  0.116  0.9890808
  0.4  1          0.8               0.50       100      0.7753147  0.164  0.9822549
  0.4  1          0.8               0.50       150      0.7792872  0.244  0.9761208
  0.4  1          0.8               0.75        50      0.7616458  0.116  0.9931741
  0.4  1          0.8               0.75       100      0.8032855  0.164  0.9870354
  0.4  1          0.8               0.75       150      0.8091325  0.240  0.9788558
  0.4  1          0.8               1.00        50      0.7678365  0.072  0.9965870
  0.4  1          0.8               1.00       100      0.8028779  0.136  0.9952218
  0.4  1          0.8               1.00       150      0.8083331  0.176  0.9945416
  0.4  2          0.6               0.50        50      0.7620888  0.164  0.9863528
  0.4  2          0.6               0.50       100      0.7743083  0.240  0.9781686
  0.4  2          0.6               0.50       150      0.7899720  0.288  0.9720345
  0.4  2          0.6               0.75        50      0.7893343  0.200  0.9836201
  0.4  2          0.6               0.75       100      0.8087626  0.284  0.9754452
  0.4  2          0.6               0.75       150      0.8115440  0.324  0.9733951
  0.4  2          0.6               1.00        50      0.8062116  0.172  0.9904437
  0.4  2          0.6               1.00       100      0.8200431  0.240  0.9829514
  0.4  2          0.6               1.00       150      0.8201051  0.276  0.9809036
  0.4  2          0.8               0.50        50      0.7761129  0.200  0.9836247
  0.4  2          0.8               0.50       100      0.7932377  0.256  0.9768057
  0.4  2          0.8               0.50       150      0.7943102  0.284  0.9727125
  0.4  2          0.8               0.75        50      0.7956329  0.228  0.9802117
  0.4  2          0.8               0.75       100      0.8037686  0.280  0.9774883
  0.4  2          0.8               0.75       150      0.8096966  0.308  0.9740823
  0.4  2          0.8               1.00        50      0.8008842  0.172  0.9938590
  0.4  2          0.8               1.00       100      0.8147895  0.256  0.9815839
  0.4  2          0.8               1.00       150      0.8162329  0.288  0.9822688
  0.4  3          0.6               0.50        50      0.7737110  0.220  0.9740684
  0.4  3          0.6               0.50       100      0.7859281  0.276  0.9720253
  0.4  3          0.6               0.50       150      0.7965241  0.304  0.9740823
  0.4  3          0.6               0.75        50      0.8000206  0.228  0.9808990
  0.4  3          0.6               0.75       100      0.8134030  0.284  0.9768104
  0.4  3          0.6               0.75       150      0.8083876  0.336  0.9706694
  0.4  3          0.6               1.00        50      0.8135045  0.252  0.9849922
  0.4  3          0.6               1.00       100      0.8227119  0.296  0.9809013
  0.4  3          0.6               1.00       150      0.8193989  0.336  0.9781709
  0.4  3          0.8               0.50        50      0.7885195  0.228  0.9795338
  0.4  3          0.8               0.50       100      0.7791101  0.296  0.9733928
  0.4  3          0.8               0.50       150      0.8019206  0.304  0.9679390
  0.4  3          0.8               0.75        50      0.7949978  0.264  0.9815793
  0.4  3          0.8               0.75       100      0.8076173  0.316  0.9761231
  0.4  3          0.8               0.75       150      0.8033537  0.328  0.9747626
  0.4  3          0.8               1.00        50      0.8109830  0.232  0.9863574
  0.4  3          0.8               1.00       100      0.8185832  0.300  0.9795361
  0.4  3          0.8               1.00       150      0.8158556  0.316  0.9754406

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 150, max_depth = 2, eta = 0.3, gamma = 0, colsample_bytree = 0.8, min_child_weight = 1 and subsample = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative      3.8      1.2
  positive     10.8     84.2
                          
 Accuracy (average) : 0.88

[1] "TRAIN accuracy: 0.87995337995338"
[1] "TRAIN +precision: 0.886503067484663"
[1] "TRAIN -precision: 0.755813953488372"
[1] "TRAIN specifity: 0.26"
[1] "TRAIN sensitivity: 0.985675306957708"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       14        4
            positive       78      477
[1] "TEST accuracy: 0.856893542757417"
[1] "TEST +precision: 0.859459459459459"
[1] "TEST -precision: 0.777777777777778"
[1] "TEST specifity: 0.152173913043478"
[1] "TEST sensitivity: 0.991683991683992"

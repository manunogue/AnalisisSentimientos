[1] "DATASET NAME: MSM_Uni_IR_10"
[1] "TRAIN INSTANCES: 1965"
[1] "TEST INSTANCES: 600"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 8.34862685203552"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

1965 samples
 656 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1572, 1572, 1572, 1571, 1573 
Resampling results:

  ROC        Sens       Spec     
  0.9953133  0.9476329  0.9925105

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     11.0      0.7
  positive      0.6     87.7
                            
 Accuracy (average) : 0.9873

[1] "TRAIN accuracy: 0.987277353689567"
[1] "TRAIN +precision: 0.993083573487032"
[1] "TRAIN -precision: 0.943478260869565"
[1] "TRAIN specifity: 0.947598253275109"
[1] "TRAIN sensitivity: 0.992511520737327"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        8        6
            positive       12      574
[1] "TEST accuracy: 0.97"
[1] "TEST +precision: 0.979522184300341"
[1] "TEST -precision: 0.571428571428571"
[1] "TEST specifity: 0.4"
[1] "TEST sensitivity: 0.989655172413793"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 2.32075311740239"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

1965 samples
 656 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1572, 1572, 1572, 1572, 1572 
Resampling results across tuning parameters:

  C      M  ROC        Sens       Spec     
  0.010  1  0.8343170  0.6594203  0.9758024
  0.010  2  0.8328906  0.6549758  0.9758024
  0.010  3  0.8229116  0.6288889  0.9735003
  0.255  1  0.9417386  0.8819324  0.9752211
  0.255  2  0.9331841  0.8382609  0.9757975
  0.255  3  0.9344201  0.8293720  0.9758008
  0.500  1  0.9577338  0.9124638  0.9740684
  0.500  2  0.9509388  0.8730435  0.9746447
  0.500  3  0.9430781  0.8467633  0.9758008

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.5 and M = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     10.6      2.3
  positive      1.0     86.1
                            
 Accuracy (average) : 0.9669

[1] "TRAIN accuracy: 0.966921119592875"
[1] "TRAIN +precision: 0.988310929281122"
[1] "TRAIN -precision: 0.822834645669291"
[1] "TRAIN specifity: 0.912663755458515"
[1] "TRAIN sensitivity: 0.974078341013825"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        9       12
            positive       11      568
[1] "TEST accuracy: 0.961666666666667"
[1] "TEST +precision: 0.981001727115717"
[1] "TEST -precision: 0.428571428571429"
[1] "TEST specifity: 0.45"
[1] "TEST sensitivity: 0.979310344827586"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 5.89172516663869"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

1965 samples
 656 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1572, 1573, 1572, 1571, 1572 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens       Spec     
  0.3  1          0.6               0.50        50      0.9601740  0.4499517  0.9942396
  0.3  1          0.6               0.50       100      0.9785620  0.6594203  0.9884792
  0.3  1          0.6               0.50       150      0.9849640  0.7685990  0.9890573
  0.3  1          0.6               0.75        50      0.9704994  0.4632850  0.9953907
  0.3  1          0.6               0.75       100      0.9806103  0.6815459  0.9907847
  0.3  1          0.6               0.75       150      0.9859579  0.7555556  0.9902067
  0.3  1          0.6               1.00        50      0.9648853  0.4460870  0.9953924
  0.3  1          0.6               1.00       100      0.9789498  0.6511111  0.9925122
  0.3  1          0.6               1.00       150      0.9825949  0.7469565  0.9925122
  0.3  1          0.8               0.50        50      0.9600954  0.4371014  0.9959687
  0.3  1          0.8               0.50       100      0.9814991  0.6944928  0.9913594
  0.3  1          0.8               0.50       150      0.9871170  0.7643478  0.9913594
  0.3  1          0.8               0.75        50      0.9669567  0.4763285  0.9953907
  0.3  1          0.8               0.75       100      0.9830539  0.6857971  0.9925122
  0.3  1          0.8               0.75       150      0.9856105  0.7729469  0.9907831
  0.3  1          0.8               1.00        50      0.9670102  0.4633816  0.9965434
  0.3  1          0.8               1.00       100      0.9797810  0.6337198  0.9936633
  0.3  1          0.8               1.00       150      0.9825266  0.7557488  0.9913594
  0.3  2          0.6               0.50        50      0.9790633  0.6946860  0.9913611
  0.3  2          0.6               0.50       100      0.9879164  0.8560386  0.9902100
  0.3  2          0.6               0.50       150      0.9920548  0.9039614  0.9925122
  0.3  2          0.6               0.75        50      0.9845862  0.6859903  0.9942413
  0.3  2          0.6               0.75       100      0.9901976  0.8647343  0.9936649
  0.3  2          0.6               0.75       150      0.9940905  0.9039614  0.9913611
  0.3  2          0.6               1.00        50      0.9856065  0.6946860  0.9953907
  0.3  2          0.6               1.00       100      0.9918311  0.8734300  0.9936649
  0.3  2          0.6               1.00       150      0.9946964  0.9082126  0.9936649
  0.3  2          0.8               0.50        50      0.9844647  0.7033816  0.9948160
  0.3  2          0.8               0.50       100      0.9907532  0.8603865  0.9902100
  0.3  2          0.8               0.50       150      0.9937738  0.9126570  0.9902100
  0.3  2          0.8               0.75        50      0.9846924  0.6772947  0.9948160
  0.3  2          0.8               0.75       100      0.9907827  0.8822222  0.9925105
  0.3  2          0.8               0.75       150      0.9942177  0.9257005  0.9930885
  0.3  2          0.8               1.00        50      0.9871167  0.6945894  0.9936649
  0.3  2          0.8               1.00       100      0.9915459  0.8777778  0.9942429
  0.3  2          0.8               1.00       150      0.9944391  0.9039614  0.9942413
  0.3  3          0.6               0.50        50      0.9890045  0.7557488  0.9936666
  0.3  3          0.6               0.50       100      0.9938496  0.9170048  0.9913661
  0.3  3          0.6               0.50       150      0.9960721  0.9387440  0.9902117
  0.3  3          0.6               0.75        50      0.9897646  0.8514010  0.9930869
  0.3  3          0.6               0.75       100      0.9940076  0.9300483  0.9942413
  0.3  3          0.6               0.75       150      0.9965399  0.9387440  0.9948193
  0.3  3          0.6               1.00        50      0.9926653  0.8604831  0.9953940
  0.3  3          0.6               1.00       100      0.9962767  0.9170048  0.9942413
  0.3  3          0.6               1.00       150      0.9973316  0.9257005  0.9936649
  0.3  3          0.8               0.50        50      0.9871087  0.8168116  0.9936649
  0.3  3          0.8               0.50       100      0.9929832  0.9213527  0.9913611
  0.3  3          0.8               0.50       150      0.9957219  0.9257005  0.9907847
  0.3  3          0.8               0.75        50      0.9902805  0.8603865  0.9953907
  0.3  3          0.8               0.75       100      0.9959653  0.9039614  0.9942396
  0.3  3          0.8               0.75       150      0.9979285  0.9343961  0.9930885
  0.3  3          0.8               1.00        50      0.9930823  0.8691787  0.9948160
  0.3  3          0.8               1.00       100      0.9968125  0.9257005  0.9942396
  0.3  3          0.8               1.00       150      0.9975804  0.9387440  0.9948176
  0.4  1          0.6               0.50        50      0.9702220  0.5636715  0.9930852
  0.4  1          0.6               0.50       100      0.9823811  0.7469565  0.9884776
  0.4  1          0.6               0.50       150      0.9861843  0.8384541  0.9890556
  0.4  1          0.6               0.75        50      0.9781351  0.5771014  0.9930885
  0.4  1          0.6               0.75       100      0.9828669  0.7293720  0.9930885
  0.4  1          0.6               0.75       150      0.9879741  0.8472464  0.9890556
  0.4  1          0.6               1.00        50      0.9716970  0.5505314  0.9942413
  0.4  1          0.6               1.00       100      0.9835615  0.7468599  0.9919358
  0.4  1          0.6               1.00       150      0.9842870  0.8515942  0.9907831
  0.4  1          0.8               0.50        50      0.9730063  0.5896618  0.9925089
  0.4  1          0.8               0.50       100      0.9854745  0.7340097  0.9896320
  0.4  1          0.8               0.50       150      0.9886417  0.8168116  0.9896303
  0.4  1          0.8               0.75        50      0.9773788  0.5811594  0.9948176
  0.4  1          0.8               0.75       100      0.9838775  0.7469565  0.9902067
  0.4  1          0.8               0.75       150      0.9869458  0.8734300  0.9890540
  0.4  1          0.8               1.00        50      0.9729842  0.5463768  0.9959671
  0.4  1          0.8               1.00       100      0.9808443  0.7250242  0.9930885
  0.4  1          0.8               1.00       150      0.9848535  0.8470531  0.9919375
  0.4  2          0.6               0.50        50      0.9829709  0.7557488  0.9919375
  0.4  2          0.6               0.50       100      0.9909669  0.8909179  0.9907897
  0.4  2          0.6               0.50       150      0.9944974  0.9213527  0.9867568
  0.4  2          0.6               0.75        50      0.9851896  0.7993237  0.9936666
  0.4  2          0.6               0.75       100      0.9932933  0.8952657  0.9919391
  0.4  2          0.6               0.75       150      0.9960206  0.9257005  0.9896336
  0.4  2          0.6               1.00        50      0.9909488  0.7775845  0.9953940
  0.4  2          0.6               1.00       100      0.9949223  0.8908213  0.9948176
  0.4  2          0.6               1.00       150      0.9965793  0.9257005  0.9936649
  0.4  2          0.8               0.50        50      0.9841353  0.7994203  0.9913594
  0.4  2          0.8               0.50       100      0.9899723  0.8822222  0.9925105
  0.4  2          0.8               0.50       150      0.9930350  0.9170048  0.9913611
  0.4  2          0.8               0.75        50      0.9885557  0.8034783  0.9959687
  0.4  2          0.8               0.75       100      0.9931829  0.9082126  0.9930885
  0.4  2          0.8               0.75       150      0.9958036  0.9300483  0.9913627
  0.4  2          0.8               1.00        50      0.9914302  0.8123671  0.9953940
  0.4  2          0.8               1.00       100      0.9953102  0.8865700  0.9948160
  0.4  2          0.8               1.00       150      0.9965244  0.9343961  0.9936649
  0.4  3          0.6               0.50        50      0.9897819  0.8734300  0.9925122
  0.4  3          0.6               0.50       100      0.9958510  0.9170048  0.9896303
  0.4  3          0.6               0.50       150      0.9969209  0.9343961  0.9919358
  0.4  3          0.6               0.75        50      0.9932317  0.8951691  0.9953924
  0.4  3          0.6               0.75       100      0.9962138  0.9430918  0.9919391
  0.4  3          0.6               0.75       150      0.9978750  0.9474396  0.9913627
  0.4  3          0.6               1.00        50      0.9922004  0.9039614  0.9959671
  0.4  3          0.6               1.00       100      0.9972293  0.9387440  0.9930885
  0.4  3          0.6               1.00       150      0.9976189  0.9387440  0.9919358
  0.4  3          0.8               0.50        50      0.9905234  0.9038647  0.9907880
  0.4  3          0.8               0.50       100      0.9960002  0.9213527  0.9879078
  0.4  3          0.8               0.50       150      0.9973417  0.9387440  0.9896353
  0.4  3          0.8               0.75        50      0.9934106  0.8951691  0.9930885
  0.4  3          0.8               0.75       100      0.9963495  0.9343961  0.9925138
  0.4  3          0.8               0.75       150      0.9979678  0.9474396  0.9925122
  0.4  3          0.8               1.00        50      0.9928548  0.8908213  0.9948176
  0.4  3          0.8               1.00       100      0.9974818  0.9387440  0.9948176
  0.4  3          0.8               1.00       150      0.9978188  0.9430918  0.9942396

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 150, max_depth = 3, eta = 0.4, gamma = 0, colsample_bytree = 0.8, min_child_weight = 1 and subsample = 0.75.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     11.0      0.7
  positive      0.6     87.7
                            
 Accuracy (average) : 0.9873

[1] "TRAIN accuracy: 0.987277353689567"
[1] "TRAIN +precision: 0.993083573487032"
[1] "TRAIN -precision: 0.943478260869565"
[1] "TRAIN specifity: 0.947598253275109"
[1] "TRAIN sensitivity: 0.992511520737327"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        9        8
            positive       11      572
[1] "TEST accuracy: 0.968333333333333"
[1] "TEST +precision: 0.981132075471698"
[1] "TEST -precision: 0.529411764705882"
[1] "TEST specifity: 0.45"
[1] "TEST sensitivity: 0.986206896551724"

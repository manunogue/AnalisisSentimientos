[1] "DATASET NAME: HRM_Uni_IR_10"
[1] "TRAIN INSTANCES: 361"
[1] "TEST INSTANCES: 112"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 1.80170297622681"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

361 samples
747 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 289, 288, 289, 289, 289 
Resampling results:

  ROC        Sens       Spec     
  0.9665066  0.8571429  0.9794272

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     16.6      1.7
  positive      2.8     78.9
                            
 Accuracy (average) : 0.9557

[1] "TRAIN accuracy: 0.955678670360111"
[1] "TRAIN +precision: 0.966101694915254"
[1] "TRAIN -precision: 0.909090909090909"
[1] "TRAIN specifity: 0.857142857142857"
[1] "TRAIN sensitivity: 0.979381443298969"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        9        3
            positive        9       91
[1] "TEST accuracy: 0.892857142857143"
[1] "TEST +precision: 0.91"
[1] "TEST -precision: 0.75"
[1] "TEST specifity: 0.5"
[1] "TEST sensitivity: 0.968085106382979"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 1.03014483451843"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

361 samples
747 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 288, 289, 289, 289, 289 
Resampling results across tuning parameters:

  C      M  ROC        Sens       Spec     
  0.010  1  0.6706291  0.4285714  0.9484512
  0.010  2  0.6851424  0.4285714  0.9553477
  0.010  3  0.6957231  0.4428571  0.9554646
  0.255  1  0.7435460  0.6142857  0.9209819
  0.255  2  0.7703745  0.6428571  0.9346581
  0.255  3  0.7370878  0.5285714  0.9518995
  0.500  1  0.8099712  0.7000000  0.9141438
  0.500  2  0.8062432  0.6571429  0.9346581
  0.500  3  0.8050680  0.6000000  0.9518995

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.5 and M = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     13.6      6.9
  positive      5.8     73.7
                            
 Accuracy (average) : 0.8726

[1] "TRAIN accuracy: 0.872576177285319"
[1] "TRAIN +precision: 0.926829268292683"
[1] "TRAIN -precision: 0.662162162162162"
[1] "TRAIN specifity: 0.7"
[1] "TRAIN sensitivity: 0.914089347079038"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        8        8
            positive       10       86
[1] "TEST accuracy: 0.839285714285714"
[1] "TEST +precision: 0.895833333333333"
[1] "TEST -precision: 0.5"
[1] "TEST specifity: 0.444444444444444"
[1] "TEST sensitivity: 0.914893617021277"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 1.94085069894791"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

361 samples
747 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 289, 288, 289, 289, 289 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens       Spec     
  0.3  1          0.6               0.50        50      0.9216206  0.5857143  0.9863822
  0.3  1          0.6               0.50       100      0.9331677  0.5714286  0.9862653
  0.3  1          0.6               0.50       150      0.9270018  0.6000000  0.9759205
  0.3  1          0.6               0.75        50      0.9304312  0.5000000  0.9932203
  0.3  1          0.6               0.75       100      0.9481381  0.6428571  0.9863238
  0.3  1          0.6               0.75       150      0.9473867  0.6714286  0.9793688
  0.3  1          0.6               1.00        50      0.9316816  0.5000000  0.9897136
  0.3  1          0.6               1.00       100      0.9513505  0.6428571  0.9759790
  0.3  1          0.6               1.00       150      0.9544210  0.7000000  0.9793688
  0.3  1          0.8               0.50        50      0.9276760  0.5714286  0.9794272
  0.3  1          0.8               0.50       100      0.9409744  0.6000000  0.9690824
  0.3  1          0.8               0.50       150      0.9392795  0.6285714  0.9724722
  0.3  1          0.8               0.75        50      0.9392461  0.5142857  0.9932203
  0.3  1          0.8               0.75       100      0.9498497  0.6142857  0.9759205
  0.3  1          0.8               0.75       150      0.9525632  0.7142857  0.9828171
  0.3  1          0.8               1.00        50      0.9309385  0.4857143  0.9862653
  0.3  1          0.8               1.00       100      0.9523441  0.6285714  0.9794272
  0.3  1          0.8               1.00       150      0.9547883  0.7000000  0.9759790
  0.3  2          0.6               0.50        50      0.9429740  0.6142857  0.9897136
  0.3  2          0.6               0.50       100      0.9442097  0.6857143  0.9690824
  0.3  2          0.6               0.50       150      0.9478667  0.7428571  0.9587376
  0.3  2          0.6               0.75        50      0.9464140  0.6142857  0.9828755
  0.3  2          0.6               0.75       100      0.9468940  0.7714286  0.9759790
  0.3  2          0.6               0.75       150      0.9461301  0.8000000  0.9725307
  0.3  2          0.6               1.00        50      0.9473032  0.6857143  0.9897136
  0.3  2          0.6               1.00       100      0.9439593  0.7857143  0.9759205
  0.3  2          0.6               1.00       150      0.9432412  0.8142857  0.9759205
  0.3  2          0.8               0.50        50      0.9349169  0.6142857  0.9863238
  0.3  2          0.8               0.50       100      0.9405903  0.7285714  0.9691409
  0.3  2          0.8               0.50       150      0.9383652  0.7714286  0.9519579
  0.3  2          0.8               0.75        50      0.9550221  0.6428571  0.9863238
  0.3  2          0.8               0.75       100      0.9567337  0.8142857  0.9828171
  0.3  2          0.8               0.75       150      0.9545086  0.8142857  0.9759205
  0.3  2          0.8               1.00        50      0.9447587  0.6714286  0.9828755
  0.3  2          0.8               1.00       100      0.9449570  0.7571429  0.9793688
  0.3  2          0.8               1.00       150      0.9412958  0.8000000  0.9793688
  0.3  3          0.6               0.50        50      0.9488186  0.6571429  0.9862069
  0.3  3          0.6               0.50       100      0.9488269  0.7000000  0.9724138
  0.3  3          0.6               0.50       150      0.9485806  0.7857143  0.9690240
  0.3  3          0.6               0.75        50      0.9559531  0.8000000  0.9862653
  0.3  3          0.6               0.75       100      0.9500584  0.8000000  0.9793688
  0.3  3          0.6               0.75       150      0.9534483  0.7857143  0.9724722
  0.3  3          0.6               1.00        50      0.9450071  0.7714286  0.9725891
  0.3  3          0.6               1.00       100      0.9457043  0.7857143  0.9794272
  0.3  3          0.6               1.00       150      0.9454246  0.8000000  0.9828171
  0.3  3          0.8               0.50        50      0.9399850  0.6571429  0.9725307
  0.3  3          0.8               0.50       100      0.9500584  0.7142857  0.9759790
  0.3  3          0.8               0.50       150      0.9478626  0.7857143  0.9759790
  0.3  3          0.8               0.75        50      0.9537948  0.7428571  0.9794272
  0.3  3          0.8               0.75       100      0.9488854  0.7857143  0.9793688
  0.3  3          0.8               0.75       150      0.9517701  0.7857143  0.9759205
  0.3  3          0.8               1.00        50      0.9498330  0.7142857  0.9897136
  0.3  3          0.8               1.00       100      0.9473658  0.8142857  0.9828171
  0.3  3          0.8               1.00       150      0.9480588  0.7857143  0.9793688
  0.4  1          0.6               0.50        50      0.9384737  0.5428571  0.9862653
  0.4  1          0.6               0.50       100      0.9407740  0.6000000  0.9759790
  0.4  1          0.6               0.50       150      0.9365868  0.6428571  0.9725307
  0.4  1          0.6               0.75        50      0.9429740  0.6142857  0.9863238
  0.4  1          0.6               0.75       100      0.9484825  0.6571429  0.9828171
  0.4  1          0.6               0.75       150      0.9390248  0.7142857  0.9690240
  0.4  1          0.6               1.00        50      0.9415004  0.6142857  0.9897136
  0.4  1          0.6               1.00       100      0.9531665  0.6714286  0.9725307
  0.4  1          0.6               1.00       150      0.9557610  0.7428571  0.9759790
  0.4  1          0.8               0.50        50      0.9341300  0.6142857  0.9793688
  0.4  1          0.8               0.50       100      0.9324163  0.6142857  0.9725307
  0.4  1          0.8               0.50       150      0.9328880  0.6428571  0.9656341
  0.4  1          0.8               0.75        50      0.9442243  0.6000000  0.9897136
  0.4  1          0.8               0.75       100      0.9520581  0.6428571  0.9724722
  0.4  1          0.8               0.75       150      0.9493947  0.7142857  0.9690824
  0.4  1          0.8               1.00        50      0.9424605  0.6142857  0.9897136
  0.4  1          0.8               1.00       100      0.9568381  0.6857143  0.9793688
  0.4  1          0.8               1.00       150      0.9572222  0.7428571  0.9793688
  0.4  2          0.6               0.50        50      0.9492444  0.6714286  0.9759790
  0.4  2          0.6               0.50       100      0.9563580  0.7285714  0.9724722
  0.4  2          0.6               0.50       150      0.9546464  0.7571429  0.9690240
  0.4  2          0.6               0.75        50      0.9540578  0.6857143  0.9828755
  0.4  2          0.6               0.75       100      0.9525591  0.8285714  0.9724722
  0.4  2          0.6               0.75       150      0.9527929  0.7857143  0.9690240
  0.4  2          0.6               1.00        50      0.9481464  0.7000000  0.9862653
  0.4  2          0.6               1.00       100      0.9452075  0.8000000  0.9828171
  0.4  2          0.6               1.00       150      0.9498330  0.8000000  0.9759205
  0.4  2          0.8               0.50        50      0.9380563  0.6571429  0.9793688
  0.4  2          0.8               0.50       100      0.9392252  0.6857143  0.9552309
  0.4  2          0.8               0.50       150      0.9411873  0.7428571  0.9586791
  0.4  2          0.8               0.75        50      0.9447817  0.7142857  0.9828171
  0.4  2          0.8               0.75       100      0.9444519  0.7857143  0.9759205
  0.4  2          0.8               0.75       150      0.9429448  0.7714286  0.9759205
  0.4  2          0.8               1.00        50      0.9454872  0.7142857  0.9794272
  0.4  2          0.8               1.00       100      0.9491275  0.8142857  0.9759790
  0.4  2          0.8               1.00       150      0.9456750  0.8142857  0.9793688
  0.4  3          0.6               0.50        50      0.9434040  0.6571429  0.9724722
  0.4  3          0.6               0.50       100      0.9485556  0.7428571  0.9793688
  0.4  3          0.6               0.50       150      0.9514737  0.7714286  0.9621274
  0.4  3          0.6               0.75        50      0.9505970  0.7857143  0.9724722
  0.4  3          0.6               0.75       100      0.9524672  0.7857143  0.9724722
  0.4  3          0.6               0.75       150      0.9529473  0.7857143  0.9655757
  0.4  3          0.6               1.00        50      0.9574142  0.7857143  0.9862653
  0.4  3          0.6               1.00       100      0.9564123  0.7714286  0.9828171
  0.4  3          0.6               1.00       150      0.9556650  0.7857143  0.9862653
  0.4  3          0.8               0.50        50      0.9380813  0.7714286  0.9690824
  0.4  3          0.8               0.50       100      0.9478375  0.7714286  0.9690240
  0.4  3          0.8               0.50       150      0.9446856  0.7857143  0.9655757
  0.4  3          0.8               0.75        50      0.9476455  0.7857143  0.9897136
  0.4  3          0.8               0.75       100      0.9478709  0.7857143  0.9724722
  0.4  3          0.8               0.75       150      0.9522501  0.8142857  0.9759205
  0.4  3          0.8               1.00        50      0.9491442  0.7857143  0.9862653
  0.4  3          0.8               1.00       100      0.9498622  0.7857143  0.9759205
  0.4  3          0.8               1.00       150      0.9520456  0.8000000  0.9759205

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 50, max_depth = 3, eta = 0.4, gamma = 0, colsample_bytree = 0.6, min_child_weight = 1 and subsample = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     15.2      1.1
  positive      4.2     79.5
                            
 Accuracy (average) : 0.9474

[1] "TRAIN accuracy: 0.947368421052632"
[1] "TRAIN +precision: 0.950331125827815"
[1] "TRAIN -precision: 0.932203389830508"
[1] "TRAIN specifity: 0.785714285714286"
[1] "TRAIN sensitivity: 0.986254295532646"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       10        1
            positive        8       93
[1] "TEST accuracy: 0.919642857142857"
[1] "TEST +precision: 0.920792079207921"
[1] "TEST -precision: 0.909090909090909"
[1] "TEST specifity: 0.555555555555556"
[1] "TEST sensitivity: 0.98936170212766"

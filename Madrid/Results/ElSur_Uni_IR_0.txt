[1] "DATASET NAME: ElSur_Uni_IR_0"
[1] "TRAIN INSTANCES: 1186"
[1] "TEST INSTANCES: 396"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 3.37160420417786"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

1186 samples
 684 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 949, 948, 950, 949, 948 
Resampling results:

  ROC        Sens  Spec     
  0.9563053  0.31  0.9965517

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative      0.6      0.3
  positive      1.4     97.6
                            
 Accuracy (average) : 0.9823

[1] "TRAIN accuracy: 0.982293423271501"
[1] "TRAIN +precision: 0.985531914893617"
[1] "TRAIN -precision: 0.636363636363636"
[1] "TRAIN specifity: 0.291666666666667"
[1] "TRAIN sensitivity: 0.996557659208262"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        5        1
            positive        0      390
[1] "TEST accuracy: 0.997474747474748"
[1] "TEST +precision: 1"
[1] "TEST -precision: 0.833333333333333"
[1] "TEST specifity: 1"
[1] "TEST sensitivity: 0.997442455242967"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 1.49226760069529"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

1186 samples
 684 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 949, 948, 950, 948, 949 
Resampling results across tuning parameters:

  C      M  ROC        Sens  Spec     
  0.010  1  0.5000000  0.00  1.0000000
  0.010  2  0.5000000  0.00  1.0000000
  0.010  3  0.5000000  0.00  1.0000000
  0.255  1  0.4965665  0.00  1.0000000
  0.255  2  0.5121888  0.00  0.9991416
  0.255  3  0.5000000  0.00  1.0000000
  0.500  1  0.4483771  0.05  0.9896626
  0.500  2  0.4601138  0.05  0.9879384
  0.500  3  0.5476162  0.00  0.9913941

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.5 and M = 3.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative      0.0      0.8
  positive      2.0     97.1
                            
 Accuracy (average) : 0.9713

[1] "TRAIN accuracy: 0.971332209106239"
[1] "TRAIN +precision: 0.979591836734694"
[1] "TRAIN -precision: 0"
[1] "TRAIN specifity: 0"
[1] "TRAIN sensitivity: 0.991394148020654"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        1        2
            positive        4      389
[1] "TEST accuracy: 0.984848484848485"
[1] "TEST +precision: 0.989821882951654"
[1] "TEST -precision: 0.333333333333333"
[1] "TEST specifity: 0.2"
[1] "TEST sensitivity: 0.994884910485934"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 4.11035766601563"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

1186 samples
 684 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 948, 950, 948, 949, 949 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens  Spec     
  0.3  1          0.6               0.50        50      0.9260065  0.13  0.9965517
  0.3  1          0.6               0.50       100      0.9264466  0.09  0.9948313
  0.3  1          0.6               0.50       150      0.9231604  0.13  0.9913830
  0.3  1          0.6               0.75        50      0.9319593  0.05  0.9982759
  0.3  1          0.6               0.75       100      0.9482326  0.13  0.9965517
  0.3  1          0.6               0.75       150      0.9420592  0.21  0.9948276
  0.3  1          0.6               1.00        50      0.9462485  0.00  0.9991379
  0.3  1          0.6               1.00       100      0.9514355  0.09  0.9974138
  0.3  1          0.6               1.00       150      0.9462892  0.16  0.9974138
  0.3  1          0.8               0.50        50      0.9281654  0.05  0.9965517
  0.3  1          0.8               0.50       100      0.9172701  0.13  0.9956897
  0.3  1          0.8               0.50       150      0.9105994  0.13  0.9922488
  0.3  1          0.8               0.75        50      0.9430868  0.00  0.9982759
  0.3  1          0.8               0.75       100      0.9487476  0.17  0.9965517
  0.3  1          0.8               0.75       150      0.9319557  0.13  0.9956897
  0.3  1          0.8               1.00        50      0.9478448  0.00  0.9991379
  0.3  1          0.8               1.00       100      0.9539764  0.13  0.9974138
  0.3  1          0.8               1.00       150      0.9478395  0.16  0.9974138
  0.3  2          0.6               0.50        50      0.9207213  0.09  0.9931108
  0.3  2          0.6               0.50       100      0.9187772  0.13  0.9931108
  0.3  2          0.6               0.50       150      0.9120240  0.13  0.9939729
  0.3  2          0.6               0.75        50      0.9397984  0.17  0.9974138
  0.3  2          0.6               0.75       100      0.9098794  0.17  0.9956897
  0.3  2          0.6               0.75       150      0.9145703  0.13  0.9956897
  0.3  2          0.6               1.00        50      0.9459309  0.13  0.9965554
  0.3  2          0.6               1.00       100      0.9210826  0.09  0.9956934
  0.3  2          0.6               1.00       150      0.9126336  0.09  0.9948313
  0.3  2          0.8               0.50        50      0.8974380  0.09  0.9948313
  0.3  2          0.8               0.50       100      0.8963902  0.13  0.9939692
  0.3  2          0.8               0.50       150      0.9032444  0.17  0.9939692
  0.3  2          0.8               0.75        50      0.9368714  0.09  0.9965554
  0.3  2          0.8               0.75       100      0.9217158  0.17  0.9948313
  0.3  2          0.8               0.75       150      0.9118867  0.09  0.9965517
  0.3  2          0.8               1.00        50      0.9440752  0.13  0.9991379
  0.3  2          0.8               1.00       100      0.9272427  0.13  0.9965517
  0.3  2          0.8               1.00       150      0.9148311  0.13  0.9965517
  0.3  3          0.6               0.50        50      0.8851898  0.13  0.9948313
  0.3  3          0.6               0.50       100      0.8664472  0.13  0.9939729
  0.3  3          0.6               0.50       150      0.8648590  0.13  0.9913904
  0.3  3          0.6               0.75        50      0.9217119  0.09  0.9965554
  0.3  3          0.6               0.75       100      0.9110186  0.13  0.9956934
  0.3  3          0.6               0.75       150      0.9115782  0.13  0.9956934
  0.3  3          0.6               1.00        50      0.9327681  0.09  0.9974175
  0.3  3          0.6               1.00       100      0.9159533  0.13  0.9948350
  0.3  3          0.6               1.00       150      0.9143570  0.13  0.9956934
  0.3  3          0.8               0.50        50      0.9167349  0.09  0.9956934
  0.3  3          0.8               0.50       100      0.9183521  0.09  0.9956934
  0.3  3          0.8               0.50       150      0.9051935  0.09  0.9965554
  0.3  3          0.8               0.75        50      0.9051432  0.21  0.9965517
  0.3  3          0.8               0.75       100      0.8901966  0.13  0.9922488
  0.3  3          0.8               0.75       150      0.8935974  0.13  0.9931071
  0.3  3          0.8               1.00        50      0.9324306  0.05  0.9982796
  0.3  3          0.8               1.00       100      0.9088151  0.09  0.9948350
  0.3  3          0.8               1.00       150      0.9126476  0.09  0.9956934
  0.4  1          0.6               0.50        50      0.9268843  0.09  0.9948313
  0.4  1          0.6               0.50       100      0.9129634  0.17  0.9956897
  0.4  1          0.6               0.50       150      0.9127821  0.21  0.9956897
  0.4  1          0.6               0.75        50      0.9247752  0.13  0.9974138
  0.4  1          0.6               0.75       100      0.9316459  0.13  0.9956897
  0.4  1          0.6               0.75       150      0.9253818  0.17  0.9956897
  0.4  1          0.6               1.00        50      0.9513530  0.05  0.9974138
  0.4  1          0.6               1.00       100      0.9506434  0.16  0.9974138
  0.4  1          0.6               1.00       150      0.9402867  0.16  0.9956897
  0.4  1          0.8               0.50        50      0.8985249  0.13  0.9956934
  0.4  1          0.8               0.50       100      0.9081231  0.17  0.9948276
  0.4  1          0.8               0.50       150      0.8977277  0.17  0.9939692
  0.4  1          0.8               0.75        50      0.9346002  0.00  0.9974138
  0.4  1          0.8               0.75       100      0.9344826  0.09  0.9965517
  0.4  1          0.8               0.75       150      0.9253380  0.17  0.9939655
  0.4  1          0.8               1.00        50      0.9485539  0.05  0.9982759
  0.4  1          0.8               1.00       100      0.9467284  0.21  0.9974138
  0.4  1          0.8               1.00       150      0.9387395  0.16  0.9956897
  0.4  2          0.6               0.50        50      0.9174713  0.17  0.9948276
  0.4  2          0.6               0.50       100      0.9044095  0.17  0.9948276
  0.4  2          0.6               0.50       150      0.9141235  0.17  0.9939692
  0.4  2          0.6               0.75        50      0.9201164  0.17  0.9965554
  0.4  2          0.6               0.75       100      0.9100213  0.09  0.9939692
  0.4  2          0.6               0.75       150      0.9016688  0.09  0.9956934
  0.4  2          0.6               1.00        50      0.9315808  0.04  0.9965554
  0.4  2          0.6               1.00       100      0.9138493  0.17  0.9948313
  0.4  2          0.6               1.00       150      0.9108745  0.17  0.9948313
  0.4  2          0.8               0.50        50      0.9203230  0.21  0.9974138
  0.4  2          0.8               0.50       100      0.9218168  0.20  0.9948276
  0.4  2          0.8               0.50       150      0.9299203  0.17  0.9956897
  0.4  2          0.8               0.75        50      0.9227740  0.17  0.9939729
  0.4  2          0.8               0.75       100      0.9114244  0.09  0.9931108
  0.4  2          0.8               0.75       150      0.9132444  0.13  0.9956934
  0.4  2          0.8               1.00        50      0.9417417  0.13  0.9982796
  0.4  2          0.8               1.00       100      0.9109703  0.17  0.9965554
  0.4  2          0.8               1.00       150      0.9102614  0.21  0.9948350
  0.4  3          0.6               0.50        50      0.8801868  0.21  0.9948313
  0.4  3          0.6               0.50       100      0.8977627  0.25  0.9948350
  0.4  3          0.6               0.50       150      0.8988329  0.21  0.9931071
  0.4  3          0.6               0.75        50      0.9120353  0.17  0.9965517
  0.4  3          0.6               0.75       100      0.9015612  0.13  0.9956934
  0.4  3          0.6               0.75       150      0.8983358  0.17  0.9939729
  0.4  3          0.6               1.00        50      0.9234607  0.13  0.9974138
  0.4  3          0.6               1.00       100      0.9079003  0.09  0.9948313
  0.4  3          0.6               1.00       150      0.9109153  0.13  0.9956897
  0.4  3          0.8               0.50        50      0.8936120  0.13  0.9956934
  0.4  3          0.8               0.50       100      0.9042981  0.13  0.9956934
  0.4  3          0.8               0.50       150      0.9002196  0.13  0.9948350
  0.4  3          0.8               0.75        50      0.9053319  0.13  0.9939729
  0.4  3          0.8               0.75       100      0.9045203  0.17  0.9948313
  0.4  3          0.8               0.75       150      0.9122693  0.17  0.9948350
  0.4  3          0.8               1.00        50      0.9104984  0.09  0.9965554
  0.4  3          0.8               1.00       100      0.9003809  0.13  0.9956934
  0.4  3          0.8               1.00       150      0.9056321  0.13  0.9956934

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 100, max_depth = 1, eta = 0.3, gamma = 0, colsample_bytree = 0.8, min_child_weight = 1 and subsample = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative      0.3      0.3
  positive      1.8     97.7
                            
 Accuracy (average) : 0.9798

[1] "TRAIN accuracy: 0.979763912310287"
[1] "TRAIN +precision: 0.982203389830508"
[1] "TRAIN -precision: 0.5"
[1] "TRAIN specifity: 0.125"
[1] "TRAIN sensitivity: 0.997418244406196"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        2        0
            positive        3      391
[1] "TEST accuracy: 0.992424242424242"
[1] "TEST +precision: 0.99238578680203"
[1] "TEST -precision: 1"
[1] "TEST specifity: 0.4"
[1] "TEST sensitivity: 1"

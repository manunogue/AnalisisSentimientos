[1] "DATASET NAME: Botin_Uni_IR_0"
[1] "TRAIN INSTANCES: 1716"
[1] "TEST INSTANCES: 573"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 12.8986420631409"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

1716 samples
 638 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1372, 1373, 1373, 1373, 1373 
Resampling results:

  ROC        Sens       Spec     
  0.9394855  0.6538085  0.9765517

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     10.1      2.0
  positive      5.4     82.5
                            
 Accuracy (average) : 0.9266

[1] "TRAIN accuracy: 0.926573426573427"
[1] "TRAIN +precision: 0.938992042440318"
[1] "TRAIN -precision: 0.836538461538462"
[1] "TRAIN specifity: 0.654135338345865"
[1] "TRAIN sensitivity: 0.976551724137931"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       44       18
            positive       32      479
[1] "TEST accuracy: 0.912739965095986"
[1] "TEST +precision: 0.937377690802348"
[1] "TEST -precision: 0.709677419354839"
[1] "TEST specifity: 0.578947368421053"
[1] "TEST sensitivity: 0.963782696177062"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 2.25017198324203"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

1716 samples
 638 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1373, 1373, 1372, 1373, 1373 
Resampling results across tuning parameters:

  C      M  ROC        Sens       Spec     
  0.010  1  0.6705634  0.4213836  0.9696552
  0.010  2  0.6676101  0.4062893  0.9703448
  0.010  3  0.6692530  0.3650594  0.9765517
  0.255  1  0.6794793  0.4813417  0.9503448
  0.255  2  0.7370694  0.4925227  0.9468966
  0.255  3  0.7323862  0.4775681  0.9434483
  0.500  1  0.7030395  0.5002096  0.9386207
  0.500  2  0.7508850  0.5113906  0.9379310
  0.500  3  0.7510326  0.4888889  0.9393103

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.5 and M = 3.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative      7.6      5.1
  positive      7.9     79.4
                            
 Accuracy (average) : 0.8695

[1] "TRAIN accuracy: 0.869463869463869"
[1] "TRAIN +precision: 0.909212283044059"
[1] "TRAIN -precision: 0.596330275229358"
[1] "TRAIN specifity: 0.488721804511278"
[1] "TRAIN sensitivity: 0.939310344827586"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       41       26
            positive       35      471
[1] "TEST accuracy: 0.893542757417103"
[1] "TEST +precision: 0.930830039525692"
[1] "TEST -precision: 0.611940298507463"
[1] "TEST specifity: 0.539473684210526"
[1] "TEST sensitivity: 0.947686116700201"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 5.8990004658699"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

1716 samples
 638 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1373, 1373, 1372, 1373, 1373 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens       Spec     
  0.3  1          0.6               0.50        50      0.9427005  0.4436758  0.9944828
  0.3  1          0.6               0.50       100      0.9585804  0.5751223  0.9931034
  0.3  1          0.6               0.50       150      0.9619304  0.6842767  0.9917241
  0.3  1          0.6               0.75        50      0.9426654  0.4475891  0.9965517
  0.3  1          0.6               0.75       100      0.9551840  0.5451433  0.9951724
  0.3  1          0.6               0.75       150      0.9632553  0.6316562  0.9924138
  0.3  1          0.6               1.00        50      0.9462192  0.4512928  0.9944828
  0.3  1          0.6               1.00       100      0.9587670  0.5527603  0.9944828
  0.3  1          0.6               1.00       150      0.9610048  0.5866527  0.9951724
  0.3  1          0.8               0.50        50      0.9378827  0.4360587  0.9958621
  0.3  1          0.8               0.50       100      0.9503834  0.5977638  0.9903448
  0.3  1          0.8               0.50       150      0.9590099  0.6466108  0.9882759
  0.3  1          0.8               0.75        50      0.9434904  0.4324948  0.9958621
  0.3  1          0.8               0.75       100      0.9586954  0.5639413  0.9931034
  0.3  1          0.8               0.75       150      0.9589320  0.6239693  0.9958621
  0.3  1          0.8               1.00        50      0.9465351  0.4400419  0.9951724
  0.3  1          0.8               1.00       100      0.9585932  0.5565339  0.9951724
  0.3  1          0.8               1.00       150      0.9610063  0.5828791  0.9944828
  0.3  2          0.6               0.50        50      0.9488058  0.5864430  0.9924138
  0.3  2          0.6               0.50       100      0.9545963  0.6767994  0.9931034
  0.3  2          0.6               0.50       150      0.9534165  0.7180294  0.9834483
  0.3  2          0.6               0.75        50      0.9549517  0.5529001  0.9931034
  0.3  2          0.6               0.75       100      0.9589378  0.6692523  0.9896552
  0.3  2          0.6               0.75       150      0.9592354  0.7180992  0.9889655
  0.3  2          0.6               1.00        50      0.9536825  0.5262055  0.9965517
  0.3  2          0.6               1.00       100      0.9619935  0.6427673  0.9951724
  0.3  2          0.6               1.00       150      0.9645852  0.7030049  0.9944828
  0.3  2          0.8               0.50        50      0.9521921  0.6013976  0.9931034
  0.3  2          0.8               0.50       100      0.9623352  0.6840671  0.9910345
  0.3  2          0.8               0.50       150      0.9629654  0.7518519  0.9862069
  0.3  2          0.8               0.75        50      0.9538565  0.5641509  0.9951724
  0.3  2          0.8               0.75       100      0.9607836  0.6618449  0.9924138
  0.3  2          0.8               0.75       150      0.9634109  0.6918938  0.9903448
  0.3  2          0.8               1.00        50      0.9495959  0.5338225  0.9958621
  0.3  2          0.8               1.00       100      0.9601921  0.6502446  0.9937931
  0.3  2          0.8               1.00       150      0.9606178  0.6991614  0.9910345
  0.3  3          0.6               0.50        50      0.9529615  0.6280922  0.9917241
  0.3  3          0.6               0.50       100      0.9537611  0.6881901  0.9862069
  0.3  3          0.6               0.50       150      0.9525777  0.7217331  0.9834483
  0.3  3          0.6               0.75        50      0.9591178  0.6166317  0.9917241
  0.3  3          0.6               0.75       100      0.9621176  0.6993012  0.9882759
  0.3  3          0.6               0.75       150      0.9602827  0.7180992  0.9868966
  0.3  3          0.6               1.00        50      0.9595978  0.5828092  0.9958621
  0.3  3          0.6               1.00       100      0.9591067  0.6765898  0.9924138
  0.3  3          0.6               1.00       150      0.9596858  0.6766597  0.9917241
  0.3  3          0.8               0.50        50      0.9511347  0.6391335  0.9889655
  0.3  3          0.8               0.50       100      0.9508901  0.6917540  0.9862069
  0.3  3          0.8               0.50       150      0.9505072  0.7106918  0.9800000
  0.3  3          0.8               0.75        50      0.9592915  0.6277428  0.9896552
  0.3  3          0.8               0.75       100      0.9628083  0.7179595  0.9896552
  0.3  3          0.8               0.75       150      0.9559045  0.7291405  0.9841379
  0.3  3          0.8               1.00        50      0.9575732  0.5901468  0.9951724
  0.3  3          0.8               1.00       100      0.9609142  0.6805031  0.9917241
  0.3  3          0.8               1.00       150      0.9604292  0.6880503  0.9910345
  0.4  1          0.6               0.50        50      0.9503553  0.5113208  0.9924138
  0.4  1          0.6               0.50       100      0.9582585  0.6579315  0.9889655
  0.4  1          0.6               0.50       150      0.9639100  0.6955975  0.9862069
  0.4  1          0.6               0.75        50      0.9431563  0.4888190  0.9931034
  0.4  1          0.6               0.75       100      0.9594675  0.6278127  0.9931034
  0.4  1          0.6               0.75       150      0.9616147  0.7030049  0.9931034
  0.4  1          0.6               1.00        50      0.9479363  0.4926625  0.9958621
  0.4  1          0.6               1.00       100      0.9614892  0.6053809  0.9951724
  0.4  1          0.6               1.00       150      0.9632263  0.6315863  0.9944828
  0.4  1          0.8               0.50        50      0.9428272  0.4697414  0.9917241
  0.4  1          0.8               0.50       100      0.9524627  0.6165618  0.9910345
  0.4  1          0.8               0.50       150      0.9534403  0.6991614  0.9855172
  0.4  1          0.8               0.75        50      0.9468445  0.4851153  0.9937931
  0.4  1          0.8               0.75       100      0.9582057  0.6129280  0.9944828
  0.4  1          0.8               0.75       150      0.9625856  0.6880503  0.9910345
  0.4  1          0.8               1.00        50      0.9471778  0.4963662  0.9972414
  0.4  1          0.8               1.00       100      0.9596289  0.5940601  0.9951724
  0.4  1          0.8               1.00       150      0.9630396  0.6352900  0.9951724
  0.4  2          0.6               0.50        50      0.9576221  0.6430468  0.9917241
  0.4  2          0.6               0.50       100      0.9600267  0.6956674  0.9896552
  0.4  2          0.6               0.50       150      0.9561483  0.7181691  0.9813793
  0.4  2          0.6               0.75        50      0.9552929  0.6316562  0.9931034
  0.4  2          0.6               0.75       100      0.9621863  0.7106918  0.9903448
  0.4  2          0.6               0.75       150      0.9614080  0.7445143  0.9875862
  0.4  2          0.6               1.00        50      0.9549729  0.5979036  0.9937931
  0.4  2          0.6               1.00       100      0.9604152  0.6654088  0.9903448
  0.4  2          0.6               1.00       150      0.9580881  0.6918239  0.9896552
  0.4  2          0.8               0.50        50      0.9574992  0.6465409  0.9896552
  0.4  2          0.8               0.50       100      0.9565286  0.7218029  0.9841379
  0.4  2          0.8               0.50       150      0.9565871  0.7256464  0.9841379
  0.4  2          0.8               0.75        50      0.9550237  0.6240391  0.9910345
  0.4  2          0.8               0.75       100      0.9580231  0.6993711  0.9875862
  0.4  2          0.8               0.75       150      0.9581334  0.7294200  0.9882759
  0.4  2          0.8               1.00        50      0.9591217  0.6012579  0.9931034
  0.4  2          0.8               1.00       100      0.9626618  0.6990217  0.9896552
  0.4  2          0.8               1.00       150      0.9585949  0.7329140  0.9896552
  0.4  3          0.6               0.50        50      0.9478749  0.6504542  0.9889655
  0.4  3          0.6               0.50       100      0.9485732  0.6879804  0.9855172
  0.4  3          0.6               0.50       150      0.9465980  0.6954577  0.9848276
  0.4  3          0.6               0.75        50      0.9580397  0.6843466  0.9910345
  0.4  3          0.6               0.75       100      0.9612728  0.7067785  0.9868966
  0.4  3          0.6               0.75       150      0.9548534  0.7181691  0.9813793
  0.4  3          0.6               1.00        50      0.9563079  0.6278127  0.9903448
  0.4  3          0.6               1.00       100      0.9587062  0.6881202  0.9862069
  0.4  3          0.6               1.00       150      0.9596891  0.7180992  0.9868966
  0.4  3          0.8               0.50        50      0.9525487  0.6653389  0.9834483
  0.4  3          0.8               0.50       100      0.9488431  0.7179595  0.9820690
  0.4  3          0.8               0.50       150      0.9396911  0.7067086  0.9813793
  0.4  3          0.8               0.75        50      0.9536011  0.6503145  0.9924138
  0.4  3          0.8               0.75       100      0.9558185  0.7031447  0.9868966
  0.4  3          0.8               0.75       150      0.9515747  0.6955276  0.9868966
  0.4  3          0.8               1.00        50      0.9561741  0.6240391  0.9903448
  0.4  3          0.8               1.00       100      0.9604275  0.6879106  0.9910345
  0.4  3          0.8               1.00       150      0.9595323  0.7142558  0.9862069

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 150, max_depth = 2, eta = 0.3, gamma = 0, colsample_bytree = 0.6, min_child_weight = 1 and subsample = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     10.9      0.5
  positive      4.6     84.0
                            
 Accuracy (average) : 0.9493

[1] "TRAIN accuracy: 0.949300699300699"
[1] "TRAIN +precision: 0.948060486522025"
[1] "TRAIN -precision: 0.958974358974359"
[1] "TRAIN specifity: 0.703007518796993"
[1] "TRAIN sensitivity: 0.99448275862069"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       50        4
            positive       26      493
[1] "TEST accuracy: 0.947643979057592"
[1] "TEST +precision: 0.94990366088632"
[1] "TEST -precision: 0.925925925925926"
[1] "TEST specifity: 0.657894736842105"
[1] "TEST sensitivity: 0.991951710261569"

[1] "DATASET NAME: TCT_Uni_IR_0"
[1] "TRAIN INSTANCES: 673"
[1] "TEST INSTANCES: 225"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 2.32162404060364"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

673 samples
725 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 539, 538, 539, 537, 539 
Resampling results:

  ROC        Sens       Spec     
  0.9721363  0.6111111  0.9952506

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative      3.7      0.4
  positive      2.4     93.5
                            
 Accuracy (average) : 0.9718

[1] "TRAIN accuracy: 0.971768202080238"
[1] "TRAIN +precision: 0.975193798449612"
[1] "TRAIN -precision: 0.892857142857143"
[1] "TRAIN specifity: 0.609756097560976"
[1] "TRAIN sensitivity: 0.995253164556962"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       14        3
            positive        5      203
[1] "TEST accuracy: 0.964444444444444"
[1] "TEST +precision: 0.975961538461538"
[1] "TEST -precision: 0.823529411764706"
[1] "TEST specifity: 0.736842105263158"
[1] "TEST sensitivity: 0.985436893203884"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 1.07336438496908"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

673 samples
725 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 539, 539, 538, 539, 537 
Resampling results across tuning parameters:

  C      M  ROC        Sens       Spec     
  0.010  1  0.5471230  0.0750000  0.9952381
  0.010  2  0.5471230  0.0750000  0.9952381
  0.010  3  0.5268849  0.0250000  0.9984127
  0.255  1  0.5532801  0.2611111  0.9810149
  0.255  2  0.6045205  0.2861111  0.9825897
  0.255  3  0.5847269  0.2138889  0.9873016
  0.500  1  0.6402223  0.3361111  0.9762530
  0.500  2  0.6846527  0.3361111  0.9810024
  0.500  3  0.6085786  0.2388889  0.9825522

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.5 and M = 2.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative      2.1      1.8
  positive      4.0     92.1
                            
 Accuracy (average) : 0.9421

[1] "TRAIN accuracy: 0.942050520059435"
[1] "TRAIN +precision: 0.958268933539413"
[1] "TRAIN -precision: 0.538461538461538"
[1] "TRAIN specifity: 0.341463414634146"
[1] "TRAIN sensitivity: 0.981012658227848"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        4        6
            positive       15      200
[1] "TEST accuracy: 0.906666666666667"
[1] "TEST +precision: 0.930232558139535"
[1] "TEST -precision: 0.4"
[1] "TEST specifity: 0.210526315789474"
[1] "TEST sensitivity: 0.970873786407767"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 2.5604820171992"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

673 samples
725 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 538, 539, 539, 538, 538 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens       Spec     
  0.3  1          0.6               0.50        50      0.9279278  0.3694444  0.9936633
  0.3  1          0.6               0.50       100      0.9345981  0.3944444  0.9889139
  0.3  1          0.6               0.50       150      0.9369129  0.4444444  0.9889139
  0.3  1          0.6               0.75        50      0.9349921  0.3472222  0.9952506
  0.3  1          0.6               0.75       100      0.9351430  0.4166667  0.9889139
  0.3  1          0.6               0.75       150      0.9346990  0.4416667  0.9905012
  0.3  1          0.6               1.00        50      0.9518006  0.2694444  0.9921010
  0.3  1          0.6               1.00       100      0.9497644  0.3194444  0.9936758
  0.3  1          0.6               1.00       150      0.9446088  0.3194444  0.9905012
  0.3  1          0.8               0.50        50      0.9331558  0.2444444  0.9921010
  0.3  1          0.8               0.50       100      0.9406940  0.3916667  0.9857518
  0.3  1          0.8               0.50       150      0.9353336  0.4444444  0.9857518
  0.3  1          0.8               0.75        50      0.9403469  0.2222222  0.9921010
  0.3  1          0.8               0.75       100      0.9400703  0.4166667  0.9889139
  0.3  1          0.8               0.75       150      0.9399915  0.4416667  0.9921010
  0.3  1          0.8               1.00        50      0.9498873  0.2472222  0.9952506
  0.3  1          0.8               1.00       100      0.9527281  0.3416667  0.9936758
  0.3  1          0.8               1.00       150      0.9439695  0.3444444  0.9920885
  0.3  2          0.6               0.50        50      0.9230574  0.3694444  0.9936758
  0.3  2          0.6               0.50       100      0.9294048  0.4194444  0.9873516
  0.3  2          0.6               0.50       150      0.9108420  0.4416667  0.9873391
  0.3  2          0.6               0.75        50      0.9355162  0.3916667  0.9936758
  0.3  2          0.6               0.75       100      0.9389460  0.3944444  0.9952506
  0.3  2          0.6               0.75       150      0.9389585  0.3944444  0.9936633
  0.3  2          0.6               1.00        50      0.9507801  0.2916667  0.9952506
  0.3  2          0.6               1.00       100      0.9418482  0.3638889  0.9968379
  0.3  2          0.6               1.00       150      0.9366152  0.4638889  0.9952506
  0.3  2          0.8               0.50        50      0.9309336  0.3472222  0.9905137
  0.3  2          0.8               0.50       100      0.9244928  0.3972222  0.9936758
  0.3  2          0.8               0.50       150      0.9129923  0.3694444  0.9920885
  0.3  2          0.8               0.75        50      0.9445974  0.3694444  0.9921010
  0.3  2          0.8               0.75       100      0.9348969  0.4416667  0.9936633
  0.3  2          0.8               0.75       150      0.9306389  0.3944444  0.9920760
  0.3  2          0.8               1.00        50      0.9464324  0.3166667  0.9952506
  0.3  2          0.8               1.00       100      0.9356549  0.3638889  0.9936758
  0.3  2          0.8               1.00       150      0.9322359  0.3916667  0.9952506
  0.3  3          0.6               0.50        50      0.9429785  0.3194444  0.9952631
  0.3  3          0.6               0.50       100      0.9274782  0.3972222  0.9936758
  0.3  3          0.6               0.50       150      0.9367142  0.4194444  0.9936758
  0.3  3          0.6               0.75        50      0.9352801  0.3444444  0.9936883
  0.3  3          0.6               0.75       100      0.9310579  0.3916667  0.9952631
  0.3  3          0.6               0.75       150      0.9311492  0.3666667  0.9905137
  0.3  3          0.6               1.00        50      0.9468700  0.2944444  0.9936633
  0.3  3          0.6               1.00       100      0.9434810  0.3916667  0.9952506
  0.3  3          0.6               1.00       150      0.9461706  0.3916667  0.9952506
  0.3  3          0.8               0.50        50      0.9210356  0.4416667  0.9936633
  0.3  3          0.8               0.50       100      0.9233710  0.4166667  0.9920760
  0.3  3          0.8               0.50       150      0.9213822  0.3916667  0.9905012
  0.3  3          0.8               0.75        50      0.9256032  0.3916667  0.9920885
  0.3  3          0.8               0.75       100      0.9269886  0.3666667  0.9952631
  0.3  3          0.8               0.75       150      0.9277901  0.3666667  0.9952631
  0.3  3          0.8               1.00        50      0.9393097  0.2444444  0.9936758
  0.3  3          0.8               1.00       100      0.9335317  0.3666667  0.9920760
  0.3  3          0.8               1.00       150      0.9397851  0.3416667  0.9920760
  0.4  1          0.6               0.50        50      0.9138304  0.2972222  0.9952631
  0.4  1          0.6               0.50       100      0.9313841  0.3194444  0.9920885
  0.4  1          0.6               0.50       150      0.9238694  0.3444444  0.9889139
  0.4  1          0.6               0.75        50      0.9292549  0.2972222  0.9920885
  0.4  1          0.6               0.75       100      0.9355921  0.4166667  0.9936758
  0.4  1          0.6               0.75       150      0.9379730  0.4694444  0.9920885
  0.4  1          0.6               1.00        50      0.9497803  0.2472222  0.9936758
  0.4  1          0.6               1.00       100      0.9415193  0.3666667  0.9905012
  0.4  1          0.6               1.00       150      0.9374927  0.4388889  0.9873266
  0.4  1          0.8               0.50        50      0.9220504  0.3222222  0.9905012
  0.4  1          0.8               0.50       100      0.9133183  0.3694444  0.9889139
  0.4  1          0.8               0.50       150      0.9095972  0.4444444  0.9825647
  0.4  1          0.8               0.75        50      0.9305352  0.2722222  0.9905137
  0.4  1          0.8               0.75       100      0.9399346  0.4666667  0.9920885
  0.4  1          0.8               0.75       150      0.9288124  0.4694444  0.9889389
  0.4  1          0.8               1.00        50      0.9525219  0.2916667  0.9936758
  0.4  1          0.8               1.00       100      0.9477331  0.2916667  0.9920885
  0.4  1          0.8               1.00       150      0.9410325  0.3666667  0.9905012
  0.4  2          0.6               0.50        50      0.8999983  0.2694444  0.9873266
  0.4  2          0.6               0.50       100      0.8896412  0.3916667  0.9825772
  0.4  2          0.6               0.50       150      0.8896065  0.3944444  0.9809774
  0.4  2          0.6               0.75        50      0.9439360  0.3916667  0.9936633
  0.4  2          0.6               0.75       100      0.9356388  0.3666667  0.9920760
  0.4  2          0.6               0.75       150      0.9263132  0.3694444  0.9904887
  0.4  2          0.6               1.00        50      0.9401981  0.3166667  0.9905012
  0.4  2          0.6               1.00       100      0.9310157  0.3694444  0.9936633
  0.4  2          0.6               1.00       150      0.9227199  0.4444444  0.9920760
  0.4  2          0.8               0.50        50      0.9276097  0.4000000  0.9857518
  0.4  2          0.8               0.50       100      0.9203659  0.4694444  0.9889139
  0.4  2          0.8               0.50       150      0.9216806  0.3944444  0.9857518
  0.4  2          0.8               0.75        50      0.9313041  0.3416667  0.9936758
  0.4  2          0.8               0.75       100      0.9291165  0.3444444  0.9952506
  0.4  2          0.8               0.75       150      0.9313888  0.3194444  0.9952506
  0.4  2          0.8               1.00        50      0.9485488  0.3416667  0.9952631
  0.4  2          0.8               1.00       100      0.9323762  0.3888889  0.9936758
  0.4  2          0.8               1.00       150      0.9318642  0.3944444  0.9920885
  0.4  3          0.6               0.50        50      0.9182458  0.3472222  0.9905137
  0.4  3          0.6               0.50       100      0.9140901  0.3944444  0.9952506
  0.4  3          0.6               0.50       150      0.9194219  0.4194444  0.9920760
  0.4  3          0.6               0.75        50      0.9291795  0.3944444  0.9936758
  0.4  3          0.6               0.75       100      0.9176145  0.3694444  0.9905012
  0.4  3          0.6               0.75       150      0.9155375  0.4194444  0.9920885
  0.4  3          0.6               1.00        50      0.9465190  0.3194444  0.9905262
  0.4  3          0.6               1.00       100      0.9462984  0.3666667  0.9905012
  0.4  3          0.6               1.00       150      0.9435190  0.3916667  0.9936758
  0.4  3          0.8               0.50        50      0.9187171  0.3694444  0.9889139
  0.4  3          0.8               0.50       100      0.9198006  0.3694444  0.9889264
  0.4  3          0.8               0.50       150      0.9159536  0.3472222  0.9873391
  0.4  3          0.8               0.75        50      0.9504321  0.3694444  0.9968379
  0.4  3          0.8               0.75       100      0.9489031  0.3444444  0.9920760
  0.4  3          0.8               0.75       150      0.9470700  0.3194444  0.9952506
  0.4  3          0.8               1.00        50      0.9268392  0.2944444  0.9968379
  0.4  3          0.8               1.00       100      0.9319474  0.3916667  0.9936633
  0.4  3          0.8               1.00       150      0.9342954  0.3666667  0.9952506

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 100, max_depth = 1, eta = 0.3, gamma = 0, colsample_bytree = 0.8, min_child_weight = 1 and subsample = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative      2.1      0.6
  positive      4.0     93.3
                            
 Accuracy (average) : 0.9539

[1] "TRAIN accuracy: 0.953937592867756"
[1] "TRAIN +precision: 0.958778625954198"
[1] "TRAIN -precision: 0.777777777777778"
[1] "TRAIN specifity: 0.341463414634146"
[1] "TRAIN sensitivity: 0.993670886075949"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        8        2
            positive       11      204
[1] "TEST accuracy: 0.942222222222222"
[1] "TEST +precision: 0.948837209302326"
[1] "TEST -precision: 0.8"
[1] "TEST specifity: 0.421052631578947"
[1] "TEST sensitivity: 0.990291262135922"

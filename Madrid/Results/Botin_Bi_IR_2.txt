[1] "DATASET NAME: Botin_Bi_IR_2"
[1] "TRAIN INSTANCES: 2324"
[1] "TEST INSTANCES: 573"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 8.9341459274292"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

2324 samples
 834 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1859, 1860, 1859, 1858, 1860 
Resampling results:

  ROC        Sens     Spec     
  0.9862569  0.95102  0.9672634

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     35.1      2.1
  positive      1.8     61.0
                            
 Accuracy (average) : 0.9613

[1] "TRAIN accuracy: 0.961273666092943"
[1] "TRAIN +precision: 0.971232876712329"
[1] "TRAIN -precision: 0.944444444444444"
[1] "TRAIN specifity: 0.951048951048951"
[1] "TRAIN sensitivity: 0.967257844474761"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       40       12
            positive       52      469
[1] "TEST accuracy: 0.888307155322862"
[1] "TEST +precision: 0.900191938579655"
[1] "TEST -precision: 0.769230769230769"
[1] "TEST specifity: 0.434782608695652"
[1] "TEST sensitivity: 0.975051975051975"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 4.94202198584874"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

2324 samples
 834 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1859, 1859, 1860, 1859, 1859 
Resampling results across tuning parameters:

  C      M  ROC        Sens       Spec     
  0.010  1  0.7523906  0.5059228  0.9263356
  0.010  2  0.7327331  0.4803142  0.9277101
  0.010  3  0.7333882  0.4803278  0.9242971
  0.255  1  0.9108060  0.7389229  0.9229273
  0.255  2  0.9082151  0.7203046  0.9201992
  0.255  3  0.8913913  0.6689582  0.9236075
  0.500  1  0.9178650  0.7494016  0.9338511
  0.500  2  0.9218671  0.7261118  0.9365698
  0.500  3  0.9006897  0.6713450  0.9352000

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.5 and M = 2.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     26.8      4.0
  positive     10.1     59.1
                            
 Accuracy (average) : 0.8589

[1] "TRAIN accuracy: 0.858864027538726"
[1] "TRAIN +precision: 0.853855721393035"
[1] "TRAIN -precision: 0.870111731843575"
[1] "TRAIN specifity: 0.726107226107226"
[1] "TRAIN sensitivity: 0.93656207366985"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        9       17
            positive       83      464
[1] "TEST accuracy: 0.825479930191972"
[1] "TEST +precision: 0.848263254113345"
[1] "TEST -precision: 0.346153846153846"
[1] "TEST specifity: 0.0978260869565217"
[1] "TEST sensitivity: 0.964656964656965"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 9.31793978611628"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

2324 samples
 834 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1860, 1858, 1859, 1859, 1860 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens       Spec     
  0.3  1          0.6               0.50        50      0.8476921  0.4522508  0.9549790
  0.3  1          0.6               0.50       100      0.9023751  0.5804366  0.9549767
  0.3  1          0.6               0.50       150      0.9239914  0.6655855  0.9515660
  0.3  1          0.6               0.75        50      0.8502999  0.4545764  0.9556616
  0.3  1          0.6               0.75       100      0.9052614  0.5850945  0.9563395
  0.3  1          0.6               0.75       150      0.9302180  0.6737318  0.9556569
  0.3  1          0.6               1.00        50      0.8604898  0.4534408  0.9556616
  0.3  1          0.6               1.00       100      0.9075482  0.5990412  0.9577047
  0.3  1          0.6               1.00       150      0.9287051  0.6375629  0.9617980
  0.3  1          0.8               0.50        50      0.8488196  0.4394533  0.9570268
  0.3  1          0.8               0.50       100      0.8984834  0.5722630  0.9508857
  0.3  1          0.8               0.50       150      0.9260505  0.6609071  0.9488403
  0.3  1          0.8               0.75        50      0.8567350  0.4533932  0.9542964
  0.3  1          0.8               0.75       100      0.9058850  0.6014552  0.9556616
  0.3  1          0.8               0.75       150      0.9240607  0.6713790  0.9556569
  0.3  1          0.8               1.00        50      0.8589727  0.4673943  0.9556616
  0.3  1          0.8               1.00       100      0.9086746  0.5943900  0.9590699
  0.3  1          0.8               1.00       150      0.9304305  0.6399021  0.9611177
  0.3  2          0.6               0.50        50      0.8990720  0.5559432  0.9570291
  0.3  2          0.6               0.50       100      0.9355001  0.6958112  0.9536092
  0.3  2          0.6               0.50       150      0.9483395  0.7715490  0.9515614
  0.3  2          0.6               0.75        50      0.9050409  0.5793010  0.9597571
  0.3  2          0.6               0.75       100      0.9437682  0.6947164  0.9611177
  0.3  2          0.6               0.75       150      0.9564786  0.7634435  0.9583873
  0.3  2          0.6               1.00        50      0.9104356  0.6025908  0.9597548
  0.3  2          0.6               1.00       100      0.9446644  0.6854005  0.9604374
  0.3  2          0.6               1.00       150      0.9607028  0.7541208  0.9597525
  0.3  2          0.8               0.50        50      0.8996055  0.5734530  0.9563442
  0.3  2          0.8               0.50       100      0.9386579  0.6969536  0.9577094
  0.3  2          0.8               0.50       150      0.9522924  0.7587175  0.9536092
  0.3  2          0.8               0.75        50      0.9064476  0.5956412  0.9556639
  0.3  2          0.8               0.75       100      0.9421162  0.6806814  0.9590746
  0.3  2          0.8               0.75       150      0.9562433  0.7634095  0.9617956
  0.3  2          0.8               1.00        50      0.9097592  0.6060792  0.9604374
  0.3  2          0.8               1.00       100      0.9481622  0.6899837  0.9604328
  0.3  2          0.8               1.00       150      0.9618595  0.7482796  0.9624806
  0.3  3          0.6               0.50        50      0.9223604  0.6328777  0.9549767
  0.3  3          0.6               0.50       100      0.9529683  0.7389365  0.9556569
  0.3  3          0.6               0.50       150      0.9616382  0.8321705  0.9604305
  0.3  3          0.6               0.75        50      0.9343089  0.6596967  0.9611154
  0.3  3          0.6               0.75       100      0.9564456  0.7668843  0.9556569
  0.3  3          0.6               0.75       150      0.9678584  0.8333129  0.9556569
  0.3  3          0.6               1.00        50      0.9363587  0.6562152  0.9652133
  0.3  3          0.6               1.00       100      0.9626929  0.7681083  0.9652086
  0.3  3          0.6               1.00       150      0.9720159  0.8193866  0.9638411
  0.3  3          0.8               0.50        50      0.9299680  0.6293826  0.9590746
  0.3  3          0.8               0.50       100      0.9558211  0.7599007  0.9590699
  0.3  3          0.8               0.50       150      0.9636702  0.8240242  0.9583850
  0.3  3          0.8               0.75        50      0.9322454  0.6492316  0.9624852
  0.3  3          0.8               0.75       100      0.9588260  0.7727390  0.9597548
  0.3  3          0.8               0.75       150      0.9673384  0.8507820  0.9604305
  0.3  3          0.8               1.00        50      0.9334977  0.6538828  0.9624806
  0.3  3          0.8               1.00       100      0.9628319  0.7447980  0.9638457
  0.3  3          0.8               1.00       150      0.9712781  0.8158575  0.9645307
  0.4  1          0.6               0.50        50      0.8657361  0.4790290  0.9549813
  0.4  1          0.6               0.50       100      0.9166240  0.6200870  0.9536138
  0.4  1          0.6               0.50       150      0.9362395  0.7004828  0.9536068
  0.4  1          0.6               0.75        50      0.8751092  0.5082075  0.9611200
  0.4  1          0.6               0.75       100      0.9194567  0.6177411  0.9570268
  0.4  1          0.6               0.75       150      0.9400100  0.7179655  0.9563372
  0.4  1          0.6               1.00        50      0.8722119  0.5104583  0.9583896
  0.4  1          0.6               1.00       100      0.9202008  0.6177411  0.9597525
  0.4  1          0.6               1.00       150      0.9439002  0.7016728  0.9577047
  0.4  1          0.8               0.50        50      0.8665108  0.5396097  0.9502055
  0.4  1          0.8               0.50       100      0.9146209  0.6153543  0.9542964
  0.4  1          0.8               0.50       150      0.9367968  0.6981912  0.9563419
  0.4  1          0.8               0.75        50      0.8741075  0.5023732  0.9549767
  0.4  1          0.8               0.75       100      0.9171744  0.6258602  0.9536115
  0.4  1          0.8               0.75       150      0.9353736  0.7168095  0.9522486
  0.4  1          0.8               1.00        50      0.8735625  0.5069903  0.9597548
  0.4  1          0.8               1.00       100      0.9196353  0.6177411  0.9611177
  0.4  1          0.8               1.00       150      0.9422219  0.6958112  0.9590699
  0.4  2          0.6               0.50        50      0.9132852  0.6107371  0.9618049
  0.4  2          0.6               0.50       100      0.9470471  0.7552632  0.9522579
  0.4  2          0.6               0.50       150      0.9588140  0.8065075  0.9556593
  0.4  2          0.6               0.75        50      0.9173736  0.6293962  0.9590699
  0.4  2          0.6               0.75       100      0.9492012  0.7599279  0.9570244
  0.4  2          0.6               0.75       150      0.9633023  0.8123215  0.9583850
  0.4  2          0.6               1.00        50      0.9229591  0.6142459  0.9631631
  0.4  2          0.6               1.00       100      0.9562590  0.7331701  0.9624806
  0.4  2          0.6               1.00       150      0.9687544  0.7937780  0.9652109
  0.4  2          0.8               0.50        50      0.9128328  0.6083707  0.9508834
  0.4  2          0.8               0.50       100      0.9489690  0.7528832  0.9549720
  0.4  2          0.8               0.50       150      0.9591579  0.8029920  0.9522393
  0.4  2          0.8               0.75        50      0.9220317  0.6165851  0.9611200
  0.4  2          0.8               0.75       100      0.9512271  0.7610975  0.9590699
  0.4  2          0.8               0.75       150      0.9625601  0.8193322  0.9611154
  0.4  2          0.8               1.00        50      0.9251045  0.6282198  0.9618026
  0.4  2          0.8               1.00       100      0.9576415  0.7622535  0.9577047
  0.4  2          0.8               1.00       150      0.9683329  0.7984088  0.9604351
  0.4  3          0.6               0.50        50      0.9380383  0.6911397  0.9542987
  0.4  3          0.6               0.50       100      0.9609037  0.8112199  0.9542964
  0.4  3          0.6               0.50       150      0.9660016  0.8589419  0.9536138
  0.4  3          0.6               0.75        50      0.9394285  0.6993132  0.9570198
  0.4  3          0.6               0.75       100      0.9634169  0.8158235  0.9590606
  0.4  3          0.6               0.75       150      0.9701110  0.8706242  0.9563349
  0.4  3          0.6               1.00        50      0.9445125  0.7028356  0.9652109
  0.4  3          0.6               1.00       100      0.9703032  0.8100299  0.9624829
  0.4  3          0.6               1.00       150      0.9766042  0.8636611  0.9638411
  0.4  3          0.8               0.50        50      0.9345164  0.6817965  0.9549767
  0.4  3          0.8               0.50       100      0.9576587  0.8158031  0.9495182
  0.4  3          0.8               0.50       150      0.9662668  0.8577723  0.9542894
  0.4  3          0.8               0.75        50      0.9430288  0.7074731  0.9549790
  0.4  3          0.8               0.75       100      0.9645680  0.8228410  0.9604328
  0.4  3          0.8               0.75       150      0.9691006  0.8822793  0.9583850
  0.4  3          0.8               1.00        50      0.9465943  0.7016728  0.9624829
  0.4  3          0.8               1.00       100      0.9696934  0.7995376  0.9652109
  0.4  3          0.8               1.00       150      0.9757263  0.8671427  0.9672517

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 150, max_depth = 3, eta = 0.4, gamma = 0, colsample_bytree = 0.6, min_child_weight = 1 and subsample = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     31.9      2.3
  positive      5.0     60.8
                            
 Accuracy (average) : 0.9269

[1] "TRAIN accuracy: 0.926850258175559"
[1] "TRAIN +precision: 0.923529411764706"
[1] "TRAIN -precision: 0.933249370277078"
[1] "TRAIN specifity: 0.863636363636364"
[1] "TRAIN sensitivity: 0.963847203274216"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       35       23
            positive       57      458
[1] "TEST accuracy: 0.860383944153578"
[1] "TEST +precision: 0.889320388349515"
[1] "TEST -precision: 0.603448275862069"
[1] "TEST specifity: 0.380434782608696"
[1] "TEST sensitivity: 0.952182952182952"

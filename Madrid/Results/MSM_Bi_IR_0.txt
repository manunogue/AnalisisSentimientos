[1] "DATASET NAME: MSM_Bi_IR_0"
[1] "TRAIN INSTANCES: 1798"
[1] "TEST INSTANCES: 600"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 8.2465238571167"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

1798 samples
 583 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1438, 1438, 1440, 1438, 1438 
Resampling results:

  ROC       Sens       Spec     
  0.864109  0.1564103  0.9994236

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative      0.6      0.1
  positive      3.0     96.4
                            
 Accuracy (average) : 0.9694

[1] "TRAIN accuracy: 0.969410456062291"
[1] "TRAIN +precision: 0.969781757134863"
[1] "TRAIN -precision: 0.909090909090909"
[1] "TRAIN specifity: 0.15625"
[1] "TRAIN sensitivity: 0.999423298731257"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        0        1
            positive       18      581
[1] "TEST accuracy: 0.968333333333333"
[1] "TEST +precision: 0.969949916527546"
[1] "TEST -precision: 0"
[1] "TEST specifity: 0"
[1] "TEST sensitivity: 0.998281786941581"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 1.94242053429286"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

1798 samples
 583 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1438, 1439, 1438, 1438, 1439 
Resampling results across tuning parameters:

  C      M  ROC        Sens       Spec     
  0.010  1  0.5675959  0.1730769  0.9925072
  0.010  2  0.5372922  0.1115385  0.9942363
  0.010  3  0.5000000  0.0000000  1.0000000
  0.255  1  0.5752882  0.2192308  0.9925072
  0.255  2  0.6129049  0.1730769  0.9925072
  0.255  3  0.6431513  0.1897436  0.9907781
  0.500  1  0.5965743  0.2192308  0.9913511
  0.500  2  0.6230117  0.1884615  0.9925072
  0.500  3  0.6431513  0.1897436  0.9907781

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.255 and M = 3.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative      0.7      0.9
  positive      2.9     95.6
                            
 Accuracy (average) : 0.9622

[1] "TRAIN accuracy: 0.962180200222469"
[1] "TRAIN +precision: 0.970621468926554"
[1] "TRAIN -precision: 0.428571428571429"
[1] "TRAIN specifity: 0.1875"
[1] "TRAIN sensitivity: 0.990772779700115"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        2        7
            positive       16      575
[1] "TEST accuracy: 0.961666666666667"
[1] "TEST +precision: 0.972927241962775"
[1] "TEST -precision: 0.222222222222222"
[1] "TEST specifity: 0.111111111111111"
[1] "TEST sensitivity: 0.987972508591065"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 5.62856318553289"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

1798 samples
 583 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1439, 1438, 1438, 1438, 1439 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens       Spec     
  0.3  1          0.6               0.50        50      0.7661007  0.1730769  0.9959604
  0.3  1          0.6               0.50       100      0.7709279  0.2833333  0.9953841
  0.3  1          0.6               0.50       150      0.7865592  0.2525641  0.9953841
  0.3  1          0.6               0.75        50      0.7787877  0.2525641  0.9953841
  0.3  1          0.6               0.75       100      0.7693120  0.2525641  0.9948060
  0.3  1          0.6               0.75       150      0.7737409  0.2833333  0.9959604
  0.3  1          0.6               1.00        50      0.7572636  0.1564103  0.9965385
  0.3  1          0.6               1.00       100      0.7613064  0.2525641  0.9965385
  0.3  1          0.6               1.00       150      0.7678154  0.2679487  0.9965385
  0.3  1          0.8               0.50        50      0.7537880  0.2833333  0.9959604
  0.3  1          0.8               0.50       100      0.7792032  0.2192308  0.9953857
  0.3  1          0.8               0.50       150      0.7836638  0.2833333  0.9948060
  0.3  1          0.8               0.75        50      0.7634348  0.2371795  0.9936516
  0.3  1          0.8               0.75       100      0.7693944  0.2679487  0.9948060
  0.3  1          0.8               0.75       150      0.7627492  0.2679487  0.9953824
  0.3  1          0.8               1.00        50      0.7624688  0.1564103  0.9959604
  0.3  1          0.8               1.00       100      0.7661556  0.2217949  0.9965385
  0.3  1          0.8               1.00       150      0.7713511  0.2679487  0.9965385
  0.3  2          0.6               0.50        50      0.7765706  0.2679487  0.9953841
  0.3  2          0.6               0.50       100      0.7743860  0.2346154  0.9959604
  0.3  2          0.6               0.50       150      0.7810237  0.2679487  0.9959621
  0.3  2          0.6               0.75        50      0.7796271  0.2679487  0.9948060
  0.3  2          0.6               0.75       100      0.7718886  0.2525641  0.9953841
  0.3  2          0.6               0.75       150      0.7742844  0.2525641  0.9959604
  0.3  2          0.6               1.00        50      0.7790908  0.2525641  0.9953841
  0.3  2          0.6               1.00       100      0.7802368  0.2679487  0.9953841
  0.3  2          0.6               1.00       150      0.7840125  0.2679487  0.9953841
  0.3  2          0.8               0.50        50      0.7569893  0.2833333  0.9930752
  0.3  2          0.8               0.50       100      0.7641106  0.2833333  0.9948060
  0.3  2          0.8               0.50       150      0.7624053  0.2679487  0.9942296
  0.3  2          0.8               0.75        50      0.7696835  0.2679487  0.9953841
  0.3  2          0.8               0.75       100      0.7724935  0.2679487  0.9953841
  0.3  2          0.8               0.75       150      0.7769728  0.2679487  0.9953841
  0.3  2          0.8               1.00        50      0.7694410  0.2525641  0.9953841
  0.3  2          0.8               1.00       100      0.7781053  0.2525641  0.9953841
  0.3  2          0.8               1.00       150      0.7810711  0.2525641  0.9948077
  0.3  3          0.6               0.50        50      0.7691302  0.2833333  0.9953841
  0.3  3          0.6               0.50       100      0.7703503  0.2833333  0.9953841
  0.3  3          0.6               0.50       150      0.7787407  0.2679487  0.9942313
  0.3  3          0.6               0.75        50      0.7839355  0.2679487  0.9953841
  0.3  3          0.6               0.75       100      0.7769151  0.2679487  0.9948060
  0.3  3          0.6               0.75       150      0.7856916  0.2679487  0.9948060
  0.3  3          0.6               1.00        50      0.7723330  0.2679487  0.9953841
  0.3  3          0.6               1.00       100      0.7811296  0.2679487  0.9959604
  0.3  3          0.6               1.00       150      0.7856014  0.2679487  0.9953841
  0.3  3          0.8               0.50        50      0.7588890  0.2679487  0.9942280
  0.3  3          0.8               0.50       100      0.7544754  0.2371795  0.9948060
  0.3  3          0.8               0.50       150      0.7662363  0.2512821  0.9948060
  0.3  3          0.8               0.75        50      0.7648812  0.2679487  0.9948060
  0.3  3          0.8               0.75       100      0.7687214  0.2525641  0.9948060
  0.3  3          0.8               0.75       150      0.7691003  0.2679487  0.9948060
  0.3  3          0.8               1.00        50      0.7734825  0.2525641  0.9959604
  0.3  3          0.8               1.00       100      0.7807845  0.2525641  0.9953841
  0.3  3          0.8               1.00       150      0.7798919  0.2525641  0.9959604
  0.4  1          0.6               0.50        50      0.7754311  0.2833333  0.9930769
  0.4  1          0.6               0.50       100      0.7800175  0.2833333  0.9953841
  0.4  1          0.6               0.50       150      0.7845103  0.2833333  0.9953841
  0.4  1          0.6               0.75        50      0.7744997  0.2371795  0.9948060
  0.4  1          0.6               0.75       100      0.7688686  0.2833333  0.9953824
  0.4  1          0.6               0.75       150      0.7663143  0.2833333  0.9953824
  0.4  1          0.6               1.00        50      0.7689263  0.1730769  0.9976912
  0.4  1          0.6               1.00       100      0.7668603  0.2679487  0.9965385
  0.4  1          0.6               1.00       150      0.7695281  0.2679487  0.9965385
  0.4  1          0.8               0.50        50      0.7846040  0.2679487  0.9948077
  0.4  1          0.8               0.50       100      0.7723271  0.2679487  0.9948060
  0.4  1          0.8               0.50       150      0.7895486  0.2833333  0.9953824
  0.4  1          0.8               0.75        50      0.7606585  0.2833333  0.9948060
  0.4  1          0.8               0.75       100      0.7744862  0.2833333  0.9953824
  0.4  1          0.8               0.75       150      0.7729320  0.2833333  0.9953824
  0.4  1          0.8               1.00        50      0.7593441  0.2371795  0.9965385
  0.4  1          0.8               1.00       100      0.7607567  0.2833333  0.9965385
  0.4  1          0.8               1.00       150      0.7709342  0.2833333  0.9965385
  0.4  2          0.6               0.50        50      0.7824607  0.2371795  0.9936533
  0.4  2          0.6               0.50       100      0.7798368  0.2679487  0.9965385
  0.4  2          0.6               0.50       150      0.7763544  0.2679487  0.9953841
  0.4  2          0.6               0.75        50      0.7769775  0.2679487  0.9948060
  0.4  2          0.6               0.75       100      0.7772071  0.2679487  0.9953841
  0.4  2          0.6               0.75       150      0.7776735  0.2679487  0.9948060
  0.4  2          0.6               1.00        50      0.7638741  0.2371795  0.9959604
  0.4  2          0.6               1.00       100      0.7748444  0.2371795  0.9953841
  0.4  2          0.6               1.00       150      0.7788589  0.2371795  0.9953841
  0.4  2          0.8               0.50        50      0.7763565  0.2217949  0.9948060
  0.4  2          0.8               0.50       100      0.7746151  0.2371795  0.9942296
  0.4  2          0.8               0.50       150      0.7665386  0.2679487  0.9942313
  0.4  2          0.8               0.75        50      0.7638565  0.2679487  0.9953841
  0.4  2          0.8               0.75       100      0.7777883  0.2679487  0.9948060
  0.4  2          0.8               0.75       150      0.7747664  0.2679487  0.9948060
  0.4  2          0.8               1.00        50      0.7710781  0.2679487  0.9959604
  0.4  2          0.8               1.00       100      0.7825743  0.2679487  0.9959604
  0.4  2          0.8               1.00       150      0.7854719  0.2679487  0.9942313
  0.4  3          0.6               0.50        50      0.7334783  0.2525641  0.9953841
  0.4  3          0.6               0.50       100      0.7486522  0.2525641  0.9953841
  0.4  3          0.6               0.50       150      0.7412848  0.2679487  0.9953841
  0.4  3          0.6               0.75        50      0.7852838  0.2679487  0.9953841
  0.4  3          0.6               0.75       100      0.7832781  0.2679487  0.9953841
  0.4  3          0.6               0.75       150      0.7795195  0.2679487  0.9953841
  0.4  3          0.6               1.00        50      0.7728877  0.2679487  0.9953841
  0.4  3          0.6               1.00       100      0.7827155  0.2525641  0.9953841
  0.4  3          0.6               1.00       150      0.7871933  0.2525641  0.9948077
  0.4  3          0.8               0.50        50      0.7713333  0.2679487  0.9948060
  0.4  3          0.8               0.50       100      0.7514235  0.2679487  0.9948077
  0.4  3          0.8               0.50       150      0.7610589  0.2666667  0.9953841
  0.4  3          0.8               0.75        50      0.7678994  0.2679487  0.9953841
  0.4  3          0.8               0.75       100      0.7751623  0.2679487  0.9953857
  0.4  3          0.8               0.75       150      0.7783423  0.2679487  0.9948093
  0.4  3          0.8               1.00        50      0.7706958  0.2679487  0.9953841
  0.4  3          0.8               1.00       100      0.7764252  0.2679487  0.9948077
  0.4  3          0.8               1.00       150      0.7844115  0.2525641  0.9942313

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 150, max_depth = 1, eta = 0.4, gamma = 0, colsample_bytree = 0.8, min_child_weight = 1 and subsample = 0.5.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative      1.0      0.4
  positive      2.6     96.0
                          
 Accuracy (average) : 0.97

[1] "TRAIN accuracy: 0.969966629588432"
[1] "TRAIN +precision: 0.974040632054176"
[1] "TRAIN -precision: 0.692307692307692"
[1] "TRAIN specifity: 0.28125"
[1] "TRAIN sensitivity: 0.995386389850058"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        2        3
            positive       16      579
[1] "TEST accuracy: 0.968333333333333"
[1] "TEST +precision: 0.973109243697479"
[1] "TEST -precision: 0.4"
[1] "TEST specifity: 0.111111111111111"
[1] "TEST sensitivity: 0.994845360824742"

[1] "DATASET NAME: Botin_Uni_IR_2"
[1] "TRAIN INSTANCES: 2308"
[1] "TEST INSTANCES: 573"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 17.7530279159546"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

2308 samples
 638 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1846, 1847, 1847, 1846, 1846 
Resampling results:

  ROC        Sens       Spec    
  0.9886464  0.9813681  0.957931

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     36.5      2.6
  positive      0.7     60.2
                            
 Accuracy (average) : 0.9666

[1] "TRAIN accuracy: 0.966637781629116"
[1] "TRAIN +precision: 0.988612099644128"
[1] "TRAIN -precision: 0.932447397563677"
[1] "TRAIN specifity: 0.981351981351981"
[1] "TRAIN sensitivity: 0.957931034482759"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       47       18
            positive       29      479
[1] "TEST accuracy: 0.917975567190227"
[1] "TEST +precision: 0.942913385826772"
[1] "TEST -precision: 0.723076923076923"
[1] "TEST specifity: 0.618421052631579"
[1] "TEST sensitivity: 0.963782696177062"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 3.24640466769536"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

2308 samples
 638 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1847, 1847, 1846, 1846, 1846 
Resampling results across tuning parameters:

  C      M  ROC        Sens       Spec     
  0.010  1  0.9098591  0.8636815  0.9158621
  0.010  2  0.9054404  0.8496600  0.9193103
  0.010  3  0.8974984  0.8205630  0.9186207
  0.255  1  0.9392122  0.9452468  0.9165517
  0.255  2  0.9410397  0.9288930  0.9172414
  0.255  3  0.9406739  0.8939344  0.9234483
  0.500  1  0.9391699  0.9534204  0.9179310
  0.500  2  0.9450258  0.9370665  0.9172414
  0.500  3  0.9423868  0.9009044  0.9227586

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.5 and M = 2.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     34.8      5.2
  positive      2.3     57.6
                            
 Accuracy (average) : 0.9246

[1] "TRAIN accuracy: 0.924610051993068"
[1] "TRAIN +precision: 0.960982658959538"
[1] "TRAIN -precision: 0.87012987012987"
[1] "TRAIN specifity: 0.937062937062937"
[1] "TRAIN sensitivity: 0.917241379310345"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       48       35
            positive       28      462
[1] "TEST accuracy: 0.890052356020942"
[1] "TEST +precision: 0.942857142857143"
[1] "TEST -precision: 0.578313253012048"
[1] "TEST specifity: 0.631578947368421"
[1] "TEST sensitivity: 0.929577464788732"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 7.44068973461787"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

2308 samples
 638 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1846, 1847, 1846, 1847, 1846 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens       Spec     
  0.3  1          0.6               0.50        50      0.9587840  0.7365837  0.9682759
  0.3  1          0.6               0.50       100      0.9753842  0.8333197  0.9662069
  0.3  1          0.6               0.50       150      0.9819725  0.8635863  0.9689655
  0.3  1          0.6               0.75        50      0.9600805  0.7318577  0.9689655
  0.3  1          0.6               0.75       100      0.9739541  0.8228002  0.9689655
  0.3  1          0.6               0.75       150      0.9811315  0.8729022  0.9689655
  0.3  1          0.6               1.00        50      0.9586682  0.7237658  0.9737931
  0.3  1          0.6               1.00       100      0.9733566  0.8169931  0.9724138
  0.3  1          0.6               1.00       150      0.9812326  0.8554196  0.9737931
  0.3  1          0.8               0.50        50      0.9569706  0.7482388  0.9689655
  0.3  1          0.8               0.50       100      0.9733032  0.8344485  0.9613793
  0.3  1          0.8               0.50       150      0.9809031  0.8682579  0.9703448
  0.3  1          0.8               0.75        50      0.9596725  0.7412077  0.9689655
  0.3  1          0.8               0.75       100      0.9738766  0.8263022  0.9717241
  0.3  1          0.8               0.75       150      0.9817009  0.8729090  0.9744828
  0.3  1          0.8               1.00        50      0.9588623  0.7179519  0.9731034
  0.3  1          0.8               1.00       100      0.9730995  0.8181695  0.9710345
  0.3  1          0.8               1.00       150      0.9808522  0.8519176  0.9731034
  0.3  2          0.6               0.50        50      0.9764521  0.8449408  0.9703448
  0.3  2          0.6               0.50       100      0.9883246  0.9067455  0.9662069
  0.3  2          0.6               0.50       150      0.9917307  0.9370325  0.9737931
  0.3  2          0.6               0.75        50      0.9755868  0.8461172  0.9703448
  0.3  2          0.6               0.75       100      0.9885052  0.9125527  0.9724138
  0.3  2          0.6               0.75       150      0.9913530  0.9417313  0.9751724
  0.3  2          0.6               1.00        50      0.9784041  0.8356317  0.9731034
  0.3  2          0.6               1.00       100      0.9890293  0.9113831  0.9772414
  0.3  2          0.6               1.00       150      0.9925178  0.9358765  0.9800000
  0.3  2          0.8               0.50        50      0.9764563  0.8531008  0.9689655
  0.3  2          0.8               0.50       100      0.9887289  0.9102135  0.9737931
  0.3  2          0.8               0.50       150      0.9914342  0.9347137  0.9731034
  0.3  2          0.8               0.75        50      0.9775372  0.8588943  0.9675862
  0.3  2          0.8               0.75       100      0.9884924  0.9207194  0.9724138
  0.3  2          0.8               0.75       150      0.9928241  0.9463688  0.9737931
  0.3  2          0.8               1.00        50      0.9771808  0.8460560  0.9710345
  0.3  2          0.8               1.00       100      0.9889146  0.9078879  0.9737931
  0.3  2          0.8               1.00       150      0.9922851  0.9405617  0.9779310
  0.3  3          0.6               0.50        50      0.9863491  0.9032368  0.9662069
  0.3  3          0.6               0.50       100      0.9925576  0.9416837  0.9731034
  0.3  3          0.6               0.50       150      0.9942067  0.9708690  0.9696552
  0.3  3          0.6               0.75        50      0.9865150  0.9044336  0.9751724
  0.3  3          0.6               0.75       100      0.9933000  0.9475384  0.9765517
  0.3  3          0.6               0.75       150      0.9948122  0.9662043  0.9772414
  0.3  3          0.6               1.00        50      0.9877080  0.9032436  0.9737931
  0.3  3          0.6               1.00       100      0.9946210  0.9475520  0.9793103
  0.3  3          0.6               1.00       150      0.9955959  0.9603631  0.9765517
  0.3  3          0.8               0.50        50      0.9855636  0.8973616  0.9689655
  0.3  3          0.8               0.50       100      0.9922625  0.9417041  0.9703448
  0.3  3          0.8               0.50       150      0.9931518  0.9662111  0.9689655
  0.3  3          0.8               0.75        50      0.9862450  0.9078947  0.9682759
  0.3  3          0.8               0.75       100      0.9929172  0.9487080  0.9751724
  0.3  3          0.8               0.75       150      0.9945891  0.9685435  0.9758621
  0.3  3          0.8               1.00        50      0.9872598  0.9078879  0.9703448
  0.3  3          0.8               1.00       100      0.9936312  0.9475452  0.9744828
  0.3  3          0.8               1.00       150      0.9950597  0.9661839  0.9765517
  0.4  1          0.6               0.50        50      0.9631622  0.7843057  0.9606897
  0.4  1          0.6               0.50       100      0.9769657  0.8577655  0.9662069
  0.4  1          0.6               0.50       150      0.9844536  0.8950632  0.9696552
  0.4  1          0.6               0.75        50      0.9649096  0.7785190  0.9696552
  0.4  1          0.6               0.75       100      0.9788825  0.8612403  0.9724138
  0.4  1          0.6               0.75       150      0.9852841  0.8892425  0.9724138
  0.4  1          0.6               1.00        50      0.9652813  0.7703930  0.9703448
  0.4  1          0.6               1.00       100      0.9781442  0.8402625  0.9724138
  0.4  1          0.6               1.00       150      0.9845525  0.8787366  0.9717241
  0.4  1          0.8               0.50        50      0.9666902  0.7913029  0.9675862
  0.4  1          0.8               0.50       100      0.9815926  0.8670883  0.9710345
  0.4  1          0.8               0.50       150      0.9859043  0.8985516  0.9703448
  0.4  1          0.8               0.75        50      0.9644330  0.7866789  0.9641379
  0.4  1          0.8               0.75       100      0.9797632  0.8717258  0.9765517
  0.4  1          0.8               0.75       150      0.9853604  0.8915681  0.9710345
  0.4  1          0.8               1.00        50      0.9660386  0.7727254  0.9717241
  0.4  1          0.8               1.00       100      0.9786793  0.8402761  0.9710345
  0.4  1          0.8               1.00       150      0.9849971  0.8705834  0.9737931
  0.4  2          0.6               0.50        50      0.9823307  0.8845777  0.9675862
  0.4  2          0.6               0.50       100      0.9902884  0.9323949  0.9696552
  0.4  2          0.6               0.50       150      0.9921066  0.9510200  0.9724138
  0.4  2          0.6               0.75        50      0.9818944  0.8764178  0.9682759
  0.4  2          0.6               0.75       100      0.9908169  0.9370461  0.9717241
  0.4  2          0.6               0.75       150      0.9932071  0.9522032  0.9731034
  0.4  2          0.6               1.00        50      0.9822803  0.8822453  0.9737931
  0.4  2          0.6               1.00       100      0.9912343  0.9265538  0.9786207
  0.4  2          0.6               1.00       150      0.9939091  0.9475384  0.9786207
  0.4  2          0.8               0.50        50      0.9810980  0.8764042  0.9586207
  0.4  2          0.8               0.50       100      0.9901772  0.9300490  0.9648276
  0.4  2          0.8               0.50       150      0.9915370  0.9533660  0.9703448
  0.4  2          0.8               0.75        50      0.9834576  0.8822385  0.9662069
  0.4  2          0.8               0.75       100      0.9910567  0.9382293  0.9703448
  0.4  2          0.8               0.75       150      0.9932306  0.9603631  0.9696552
  0.4  2          0.8               1.00        50      0.9854196  0.8915477  0.9731034
  0.4  2          0.8               1.00       100      0.9917837  0.9288794  0.9724138
  0.4  2          0.8               1.00       150      0.9938775  0.9463824  0.9737931
  0.4  3          0.6               0.50        50      0.9876225  0.9230654  0.9682759
  0.4  3          0.6               0.50       100      0.9931313  0.9545356  0.9724138
  0.4  3          0.6               0.50       150      0.9943949  0.9650415  0.9731034
  0.4  3          0.6               0.75        50      0.9890335  0.9230518  0.9724138
  0.4  3          0.6               0.75       100      0.9933752  0.9545220  0.9744828
  0.4  3          0.6               0.75       150      0.9944221  0.9732014  0.9758621
  0.4  3          0.6               1.00        50      0.9906860  0.9265538  0.9744828
  0.4  3          0.6               1.00       100      0.9953566  0.9650347  0.9737931
  0.4  3          0.6               1.00       150      0.9959560  0.9755406  0.9737931
  0.4  3          0.8               0.50        50      0.9865227  0.9113831  0.9655172
  0.4  3          0.8               0.50       100      0.9929141  0.9673807  0.9682759
  0.4  3          0.8               0.50       150      0.9934408  0.9755270  0.9724138
  0.4  3          0.8               0.75        50      0.9903789  0.9288726  0.9724138
  0.4  3          0.8               0.75       100      0.9944923  0.9626887  0.9737931
  0.4  3          0.8               0.75       150      0.9949726  0.9767102  0.9744828
  0.4  3          0.8               1.00        50      0.9895458  0.9300422  0.9710345
  0.4  3          0.8               1.00       100      0.9939983  0.9638719  0.9717241
  0.4  3          0.8               1.00       150      0.9952602  0.9790358  0.9744828

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 150, max_depth = 3, eta = 0.4, gamma = 0, colsample_bytree = 0.6, min_child_weight = 1 and subsample = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     36.3      1.6
  positive      0.9     61.2
                            
 Accuracy (average) : 0.9744

[1] "TRAIN accuracy: 0.974436741767764"
[1] "TRAIN +precision: 0.985345429169574"
[1] "TRAIN -precision: 0.956571428571429"
[1] "TRAIN specifity: 0.975524475524476"
[1] "TRAIN sensitivity: 0.973793103448276"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       56       16
            positive       20      481
[1] "TEST accuracy: 0.93717277486911"
[1] "TEST +precision: 0.960079840319361"
[1] "TEST -precision: 0.777777777777778"
[1] "TEST specifity: 0.736842105263158"
[1] "TEST sensitivity: 0.967806841046278"

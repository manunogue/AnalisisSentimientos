[1] "DATASET NAME: Botin_Bi_IR_5"
[1] "TRAIN INSTANCES: 1959"
[1] "TEST INSTANCES: 573"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 7.1757550239563"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

1959 samples
 834 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1567, 1566, 1568, 1568, 1567 
Resampling results:

  ROC        Sens       Spec     
  0.9578651  0.8154607  0.9665645

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     20.5      2.5
  positive      4.6     72.3
                            
 Accuracy (average) : 0.9285

[1] "TRAIN accuracy: 0.928534966819806"
[1] "TRAIN +precision: 0.939655172413793"
[1] "TRAIN -precision: 0.891352549889135"
[1] "TRAIN specifity: 0.815415821501014"
[1] "TRAIN sensitivity: 0.966575716234652"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       40       14
            positive       52      467
[1] "TEST accuracy: 0.884816753926702"
[1] "TEST +precision: 0.89980732177264"
[1] "TEST -precision: 0.740740740740741"
[1] "TEST specifity: 0.434782608695652"
[1] "TEST sensitivity: 0.970893970893971"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 3.88478779792786"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

1959 samples
 834 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1567, 1568, 1568, 1566, 1567 
Resampling results across tuning parameters:

  C      M  ROC        Sens       Spec     
  0.010  1  0.5949884  0.2272315  0.9631515
  0.010  2  0.5849295  0.2151103  0.9631515
  0.010  3  0.5871671  0.2191507  0.9597455
  0.255  1  0.8075482  0.4402804  0.9556500
  0.255  2  0.7841774  0.3895898  0.9556616
  0.255  3  0.7472808  0.3265925  0.9563465
  0.500  1  0.8096242  0.4687281  0.9583827
  0.500  2  0.8120131  0.4160585  0.9638457
  0.500  3  0.7940919  0.3876314  0.9529312

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.5 and M = 2.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     10.5      2.7
  positive     14.7     72.1
                            
 Accuracy (average) : 0.8259

[1] "TRAIN accuracy: 0.825931597753956"
[1] "TRAIN +precision: 0.830687830687831"
[1] "TRAIN -precision: 0.794573643410853"
[1] "TRAIN specifity: 0.415821501014199"
[1] "TRAIN sensitivity: 0.963847203274216"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       19       23
            positive       73      458
[1] "TEST accuracy: 0.832460732984293"
[1] "TEST +precision: 0.862523540489642"
[1] "TEST -precision: 0.452380952380952"
[1] "TEST specifity: 0.206521739130435"
[1] "TEST sensitivity: 0.952182952182952"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 8.57292335033417"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

1959 samples
 834 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1568, 1566, 1567, 1567, 1568 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens       Spec     
  0.3  1          0.6               0.50        50      0.8404640  0.2840033  0.9788512
  0.3  1          0.6               0.50       100      0.8760265  0.3710781  0.9788582
  0.3  1          0.6               0.50       150      0.8939638  0.4685632  0.9754429
  0.3  1          0.6               0.75        50      0.8418746  0.2655329  0.9829514
  0.3  1          0.6               0.75       100      0.8835979  0.4218718  0.9761231
  0.3  1          0.6               0.75       150      0.9029210  0.4846836  0.9768057
  0.3  1          0.6               1.00        50      0.8427591  0.2655123  0.9877133
  0.3  1          0.6               1.00       100      0.8873569  0.3629767  0.9870330
  0.3  1          0.6               1.00       150      0.9024497  0.4481962  0.9843143
  0.3  1          0.8               0.50        50      0.8288473  0.2799423  0.9774907
  0.3  1          0.8               0.50       100      0.8692958  0.3975469  0.9754498
  0.3  1          0.8               0.50       150      0.8904755  0.4747062  0.9740800
  0.3  1          0.8               0.75        50      0.8407642  0.2574933  0.9877133
  0.3  1          0.8               0.75       100      0.8780044  0.4056483  0.9740800
  0.3  1          0.8               0.75       150      0.8988769  0.4705215  0.9781732
  0.3  1          0.8               1.00        50      0.8424182  0.2716966  0.9877133
  0.3  1          0.8               1.00       100      0.8880221  0.3771800  0.9883982
  0.3  1          0.8               1.00       150      0.9019001  0.4522573  0.9843143
  0.3  2          0.6               0.50        50      0.8664445  0.4076273  0.9740754
  0.3  2          0.6               0.50       100      0.9021737  0.5212121  0.9720392
  0.3  2          0.6               0.50       150      0.9125024  0.5780251  0.9706647
  0.3  2          0.6               0.75        50      0.8802814  0.4259122  0.9808967
  0.3  2          0.6               0.75       100      0.9114502  0.5414554  0.9774930
  0.3  2          0.6               0.75       150      0.9253433  0.5862090  0.9713520
  0.3  2          0.6               1.00        50      0.8935950  0.4117502  0.9829444
  0.3  2          0.6               1.00       100      0.9203320  0.5211709  0.9822642
  0.3  2          0.6               1.00       150      0.9324590  0.5799629  0.9829491
  0.3  2          0.8               0.50        50      0.8663407  0.3995053  0.9754406
  0.3  2          0.8               0.50       100      0.9024764  0.5192125  0.9693065
  0.3  2          0.8               0.50       150      0.9136083  0.5658421  0.9693065
  0.3  2          0.8               0.75        50      0.8836606  0.4016079  0.9768104
  0.3  2          0.8               0.75       100      0.9155529  0.5172336  0.9815793
  0.3  2          0.8               0.75       150      0.9248323  0.5881262  0.9761208
  0.3  2          0.8               1.00        50      0.8890411  0.4198928  0.9808990
  0.3  2          0.8               1.00       100      0.9138294  0.5252319  0.9815816
  0.3  2          0.8               1.00       150      0.9318053  0.5941662  0.9795338
  0.3  3          0.6               0.50        50      0.8901686  0.4848073  0.9761231
  0.3  3          0.6               0.50       100      0.9149753  0.5658833  0.9747626
  0.3  3          0.6               0.50       150      0.9220468  0.6165121  0.9679436
  0.3  3          0.6               0.75        50      0.8961822  0.5070913  0.9774860
  0.3  3          0.6               0.75       100      0.9282947  0.5820862  0.9754406
  0.3  3          0.6               0.75       150      0.9331212  0.6286951  0.9733928
  0.3  3          0.6               1.00        50      0.9081733  0.4927644  0.9843096
  0.3  3          0.6               1.00       100      0.9319654  0.5880643  0.9822572
  0.3  3          0.6               1.00       150      0.9430815  0.6428159  0.9788489
  0.3  3          0.8               0.50        50      0.8884152  0.4746444  0.9740730
  0.3  3          0.8               0.50       100      0.9113762  0.5618223  0.9686169
  0.3  3          0.8               0.50       150      0.9182866  0.6205731  0.9679390
  0.3  3          0.8               0.75        50      0.9010777  0.4968048  0.9829421
  0.3  3          0.8               0.75       100      0.9250908  0.5820243  0.9822595
  0.3  3          0.8               0.75       150      0.9345717  0.6327149  0.9774860
  0.3  3          0.8               1.00        50      0.9062550  0.4988868  0.9808990
  0.3  3          0.8               1.00       100      0.9284785  0.5861266  0.9781686
  0.3  3          0.8               1.00       150      0.9426642  0.6367347  0.9795315
  0.4  1          0.6               0.50        50      0.8380686  0.3530612  0.9747626
  0.4  1          0.6               0.50       100      0.8768808  0.4502989  0.9774930
  0.4  1          0.6               0.50       150      0.8943938  0.5213152  0.9720345
  0.4  1          0.6               0.75        50      0.8624603  0.3632035  0.9788489
  0.4  1          0.6               0.75       100      0.8888562  0.4705628  0.9788535
  0.4  1          0.6               0.75       150      0.9110350  0.5353741  0.9761208
  0.4  1          0.6               1.00        50      0.8551305  0.3529582  0.9822549
  0.4  1          0.6               1.00       100      0.8990292  0.4077098  0.9884029
  0.4  1          0.6               1.00       150      0.9097687  0.5069264  0.9836294
  0.4  1          0.8               0.50        50      0.8452326  0.3650794  0.9754382
  0.4  1          0.8               0.50       100      0.8812491  0.4602968  0.9720276
  0.4  1          0.8               0.50       150      0.8993527  0.5212533  0.9706670
  0.4  1          0.8               0.75        50      0.8539695  0.3508967  0.9761231
  0.4  1          0.8               0.75       100      0.8990144  0.4584622  0.9808943
  0.4  1          0.8               0.75       150      0.9140941  0.5293754  0.9774883
  0.4  1          0.8               1.00        50      0.8582774  0.3286745  0.9870330
  0.4  1          0.8               1.00       100      0.8989706  0.4279324  0.9870330
  0.4  1          0.8               1.00       150      0.9113829  0.5029272  0.9836294
  0.4  2          0.6               0.50        50      0.8820455  0.4401361  0.9754406
  0.4  2          0.6               0.50       100      0.9072102  0.5678417  0.9699844
  0.4  2          0.6               0.50       150      0.9180882  0.6003711  0.9693042
  0.4  2          0.6               0.75        50      0.8925679  0.4928262  0.9768034
  0.4  2          0.6               0.75       100      0.9244425  0.5881262  0.9768057
  0.4  2          0.6               0.75       150      0.9334469  0.6307772  0.9713473
  0.4  2          0.6               1.00        50      0.8997242  0.4908266  0.9836247
  0.4  2          0.6               1.00       100      0.9241559  0.5698619  0.9829468
  0.4  2          0.6               1.00       150      0.9388786  0.6205525  0.9788535
  0.4  2          0.8               0.50        50      0.8834076  0.4787054  0.9699868
  0.4  2          0.8               0.50       100      0.9098580  0.5820449  0.9652133
  0.4  2          0.8               0.50       150      0.9165637  0.6224902  0.9699868
  0.4  2          0.8               0.75        50      0.8876468  0.4806844  0.9768034
  0.4  2          0.8               0.75       100      0.9228776  0.5841064  0.9795361
  0.4  2          0.8               0.75       150      0.9307429  0.6226139  0.9733928
  0.4  2          0.8               1.00        50      0.8975186  0.4786642  0.9822618
  0.4  2          0.8               1.00       100      0.9274068  0.5738404  0.9815839
  0.4  2          0.8               1.00       150      0.9388378  0.6265925  0.9795361
  0.4  3          0.6               0.50        50      0.8945012  0.5150897  0.9761231
  0.4  3          0.6               0.50       100      0.9135893  0.6044527  0.9686239
  0.4  3          0.6               0.50       150      0.9214182  0.6530200  0.9658958
  0.4  3          0.6               0.75        50      0.9112008  0.5293342  0.9768127
  0.4  3          0.6               0.75       100      0.9330166  0.6226551  0.9727148
  0.4  3          0.6               0.75       150      0.9354378  0.6733869  0.9699821
  0.4  3          0.6               1.00        50      0.9158394  0.5211915  0.9822618
  0.4  3          0.6               1.00       100      0.9390924  0.6328180  0.9795338
  0.4  3          0.6               1.00       150      0.9475443  0.6794063  0.9795291
  0.4  3          0.8               0.50        50      0.9020934  0.5170686  0.9768104
  0.4  3          0.8               0.50       100      0.9135061  0.6205112  0.9686285
  0.4  3          0.8               0.50       150      0.9197839  0.6570398  0.9686216
  0.4  3          0.8               0.75        50      0.9130536  0.5232942  0.9747556
  0.4  3          0.8               0.75       100      0.9340705  0.6225727  0.9733928
  0.4  3          0.8               0.75       150      0.9377500  0.6652237  0.9699821
  0.4  3          0.8               1.00        50      0.9178383  0.5232529  0.9822618
  0.4  3          0.8               1.00       100      0.9408996  0.6226139  0.9788512
  0.4  3          0.8               1.00       150      0.9478353  0.6772830  0.9761162

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 150, max_depth = 3, eta = 0.4, gamma = 0, colsample_bytree = 0.8, min_child_weight = 1 and subsample = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     17.0      1.8
  positive      8.1     73.0
                           
 Accuracy (average) : 0.901

[1] "TRAIN accuracy: 0.90096988259316"
[1] "TRAIN +precision: 0.9"
[1] "TRAIN -precision: 0.905149051490515"
[1] "TRAIN specifity: 0.677484787018255"
[1] "TRAIN sensitivity: 0.97612551159618"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       26       13
            positive       66      468
[1] "TEST accuracy: 0.862129144851658"
[1] "TEST +precision: 0.876404494382023"
[1] "TEST -precision: 0.666666666666667"
[1] "TEST specifity: 0.282608695652174"
[1] "TEST sensitivity: 0.972972972972973"

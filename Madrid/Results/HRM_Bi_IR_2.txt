[1] "DATASET NAME: HRM_Bi_IR_2"
[1] "TRAIN INSTANCES: 456"
[1] "TEST INSTANCES: 112"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 2.16879987716675"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

456 samples
964 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 365, 365, 364, 366, 364 
Resampling results:

  ROC        Sens       Spec    
  0.9735619  0.9103387  0.993043

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     33.6      0.4
  positive      3.3     62.7
                            
 Accuracy (average) : 0.9627

[1] "TRAIN accuracy: 0.962719298245614"
[1] "TRAIN +precision: 0.950166112956811"
[1] "TRAIN -precision: 0.987096774193548"
[1] "TRAIN specifity: 0.910714285714286"
[1] "TRAIN sensitivity: 0.993055555555556"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        4        0
            positive       11       97
[1] "TEST accuracy: 0.901785714285714"
[1] "TEST +precision: 0.898148148148148"
[1] "TEST -precision: 1"
[1] "TEST specifity: 0.266666666666667"
[1] "TEST sensitivity: 1"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 1.63999975124995"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

456 samples
964 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 364, 365, 365, 364, 366 
Resampling results across tuning parameters:

  C      M  ROC        Sens       Spec     
  0.010  1  0.8014819  0.4821747  0.9929825
  0.010  2  0.7902145  0.4762923  0.9895342
  0.010  3  0.7386145  0.4404635  0.9722928
  0.255  1  0.8795995  0.5957219  0.9859649
  0.255  2  0.8572789  0.5720143  0.9826376
  0.255  3  0.8213629  0.5360071  0.9688445
  0.500  1  0.8955593  0.6258467  0.9859649
  0.500  2  0.8719402  0.5896613  0.9791894
  0.500  3  0.8439586  0.5598930  0.9583787

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.5 and M = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     23.0      0.9
  positive     13.8     62.3
                            
 Accuracy (average) : 0.8531

[1] "TRAIN accuracy: 0.853070175438596"
[1] "TRAIN +precision: 0.818443804034582"
[1] "TRAIN -precision: 0.963302752293578"
[1] "TRAIN specifity: 0.625"
[1] "TRAIN sensitivity: 0.986111111111111"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        1        2
            positive       14       95
[1] "TEST accuracy: 0.857142857142857"
[1] "TEST +precision: 0.871559633027523"
[1] "TEST -precision: 0.333333333333333"
[1] "TEST specifity: 0.0666666666666667"
[1] "TEST sensitivity: 0.979381443298969"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 2.68380493323008"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

456 samples
964 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 365, 364, 365, 365, 365 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens       Spec     
  0.3  1          0.6               0.50        50      0.8318686  0.4046346  0.9686630
  0.3  1          0.6               0.50       100      0.8587774  0.5527629  0.9721718
  0.3  1          0.6               0.50       150      0.8703673  0.5529412  0.9721718
  0.3  1          0.6               0.75        50      0.8461388  0.4288770  0.9825771
  0.3  1          0.6               0.75       100      0.8918855  0.5411765  0.9791289
  0.3  1          0.6               0.75       150      0.9056103  0.6306595  0.9756201
  0.3  1          0.6               1.00        50      0.8478800  0.4875223  0.9721113
  0.3  1          0.6               1.00       100      0.8838000  0.5648841  0.9755596
  0.3  1          0.6               1.00       150      0.8997216  0.6247772  0.9721113
  0.3  1          0.8               0.50        50      0.8075094  0.4167558  0.9686630
  0.3  1          0.8               0.50       100      0.8756491  0.5647059  0.9791289
  0.3  1          0.8               0.50       150      0.8822832  0.5588235  0.9791289
  0.3  1          0.8               0.75        50      0.8454998  0.4454545  0.9721113
  0.3  1          0.8               0.75       100      0.8812465  0.5648841  0.9651543
  0.3  1          0.8               0.75       150      0.8993449  0.6245989  0.9721718
  0.3  1          0.8               1.00        50      0.8522130  0.4755793  0.9721113
  0.3  1          0.8               1.00       100      0.8823635  0.5588235  0.9721113
  0.3  1          0.8               1.00       150      0.9045954  0.6368984  0.9721113
  0.3  2          0.6               0.50        50      0.8592742  0.5001783  0.9756201
  0.3  2          0.6               0.50       100      0.8818390  0.5773619  0.9721718
  0.3  2          0.6               0.50       150      0.8809109  0.6008913  0.9652148
  0.3  2          0.6               0.75        50      0.8876039  0.5472371  0.9791289
  0.3  2          0.6               0.75       100      0.9035410  0.6187166  0.9756806
  0.3  2          0.6               0.75       150      0.9014158  0.6187166  0.9756806
  0.3  2          0.6               1.00        50      0.8826421  0.5830660  0.9721113
  0.3  2          0.6               1.00       100      0.9184354  0.6310160  0.9790684
  0.3  2          0.6               1.00       150      0.9103447  0.6245989  0.9722323
  0.3  2          0.8               0.50        50      0.8475163  0.4937611  0.9721718
  0.3  2          0.8               0.50       100      0.8824707  0.5590018  0.9722323
  0.3  2          0.8               0.50       150      0.8788743  0.5889483  0.9687235
  0.3  2          0.8               0.75        50      0.8920233  0.5770053  0.9721718
  0.3  2          0.8               0.75       100      0.9095246  0.6363636  0.9791289
  0.3  2          0.8               0.75       150      0.9108016  0.6363636  0.9791289
  0.3  2          0.8               1.00        50      0.8925515  0.5711230  0.9755596
  0.3  2          0.8               1.00       100      0.9144159  0.6486631  0.9790684
  0.3  2          0.8               1.00       150      0.9128278  0.6245989  0.9721718
  0.3  3          0.6               0.50        50      0.8673627  0.4998217  0.9513612
  0.3  3          0.6               0.50       100      0.8878401  0.5472371  0.9547489
  0.3  3          0.6               0.50       150      0.8855376  0.5590018  0.9617665
  0.3  3          0.6               0.75        50      0.9086496  0.6008913  0.9791289
  0.3  3          0.6               0.75       100      0.9093096  0.6245989  0.9722323
  0.3  3          0.6               0.75       150      0.9080149  0.6422460  0.9722323
  0.3  3          0.6               1.00        50      0.9119695  0.6130125  0.9756201
  0.3  3          0.6               1.00       100      0.9136731  0.6128342  0.9721718
  0.3  3          0.6               1.00       150      0.9168983  0.6304813  0.9722323
  0.3  3          0.8               0.50        50      0.8827017  0.5352941  0.9791289
  0.3  3          0.8               0.50       100      0.8908835  0.5413547  0.9756806
  0.3  3          0.8               0.50       150      0.8913971  0.6240642  0.9686630
  0.3  3          0.8               0.75        50      0.8990764  0.6007130  0.9756806
  0.3  3          0.8               0.75       100      0.9078960  0.6187166  0.9722323
  0.3  3          0.8               0.75       150      0.9107371  0.6304813  0.9687235
  0.3  3          0.8               1.00        50      0.9071264  0.6247772  0.9825771
  0.3  3          0.8               1.00       100      0.9128494  0.6604278  0.9721718
  0.3  3          0.8               1.00       150      0.9144063  0.6422460  0.9652148
  0.4  1          0.6               0.50        50      0.8436038  0.4996435  0.9687840
  0.4  1          0.6               0.50       100      0.8684560  0.5178253  0.9722323
  0.4  1          0.6               0.50       150      0.8834956  0.5591800  0.9652148
  0.4  1          0.6               0.75        50      0.8625360  0.5231729  0.9756201
  0.4  1          0.6               0.75       100      0.8946887  0.6128342  0.9791289
  0.4  1          0.6               0.75       150      0.9027317  0.6304813  0.9791289
  0.4  1          0.6               1.00        50      0.8704850  0.5351159  0.9721718
  0.4  1          0.6               1.00       100      0.8944086  0.6188948  0.9825771
  0.4  1          0.6               1.00       150      0.9087218  0.6427807  0.9790684
  0.4  1          0.8               0.50        50      0.8586473  0.4219251  0.9721113
  0.4  1          0.8               0.50       100      0.9059058  0.5354724  0.9825166
  0.4  1          0.8               0.50       150      0.9028752  0.6064171  0.9756201
  0.4  1          0.8               0.75        50      0.8480064  0.5351159  0.9756201
  0.4  1          0.8               0.75       100      0.8847298  0.6185383  0.9755596
  0.4  1          0.8               0.75       150      0.9079586  0.6422460  0.9686630
  0.4  1          0.8               1.00        50      0.8702579  0.5351159  0.9721718
  0.4  1          0.8               1.00       100      0.8973714  0.6247772  0.9756201
  0.4  1          0.8               1.00       150      0.9076733  0.6486631  0.9790684
  0.4  2          0.6               0.50        50      0.8832175  0.5288770  0.9791289
  0.4  2          0.6               0.50       100      0.8823940  0.5884135  0.9722323
  0.4  2          0.6               0.50       150      0.8808900  0.6005348  0.9652148
  0.4  2          0.6               0.75        50      0.8961509  0.5828877  0.9721113
  0.4  2          0.6               0.75       100      0.9017909  0.6245989  0.9722323
  0.4  2          0.6               0.75       150      0.9011590  0.6128342  0.9722323
  0.4  2          0.6               1.00        50      0.8932031  0.6124777  0.9756201
  0.4  2          0.6               1.00       100      0.9104467  0.6545455  0.9722323
  0.4  2          0.6               1.00       150      0.9056455  0.6604278  0.9722323
  0.4  2          0.8               0.50        50      0.8690302  0.5531194  0.9687235
  0.4  2          0.8               0.50       100      0.8779620  0.5590018  0.9652148
  0.4  2          0.8               0.50       150      0.8785092  0.5942959  0.9617060
  0.4  2          0.8               0.75        50      0.9067542  0.6245989  0.9790684
  0.4  2          0.8               0.75       100      0.9085770  0.6481283  0.9790684
  0.4  2          0.8               0.75       150      0.9038430  0.6540107  0.9756201
  0.4  2          0.8               1.00        50      0.8998974  0.6130125  0.9721113
  0.4  2          0.8               1.00       100      0.9145608  0.6545455  0.9721718
  0.4  2          0.8               1.00       150      0.9137376  0.6540107  0.9721718
  0.4  3          0.6               0.50        50      0.8876258  0.5413547  0.9512402
  0.4  3          0.6               0.50       100      0.8795363  0.5591800  0.9618270
  0.4  3          0.6               0.50       150      0.8804475  0.6064171  0.9477919
  0.4  3          0.6               0.75        50      0.9058214  0.6303030  0.9791289
  0.4  3          0.6               0.75       100      0.9049015  0.6361854  0.9756806
  0.4  3          0.6               0.75       150      0.9046229  0.6657754  0.9722323
  0.4  3          0.6               1.00        50      0.9181634  0.6486631  0.9790684
  0.4  3          0.6               1.00       100      0.9097842  0.6304813  0.9687840
  0.4  3          0.6               1.00       150      0.9115620  0.6304813  0.9653358
  0.4  3          0.8               0.50        50      0.8839886  0.5468806  0.9617665
  0.4  3          0.8               0.50       100      0.8879247  0.6245989  0.9687235
  0.4  3          0.8               0.50       150      0.8866210  0.6245989  0.9652148
  0.4  3          0.8               0.75        50      0.9099073  0.6363636  0.9756806
  0.4  3          0.8               0.75       100      0.9159878  0.6422460  0.9722323
  0.4  3          0.8               0.75       150      0.9177239  0.6422460  0.9722323
  0.4  3          0.8               1.00        50      0.9149401  0.6192513  0.9756201
  0.4  3          0.8               1.00       100      0.9108692  0.6304813  0.9652753
  0.4  3          0.8               1.00       150      0.9126390  0.6543672  0.9652753

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 100, max_depth = 2, eta = 0.3, gamma = 0, colsample_bytree = 0.6, min_child_weight = 1 and subsample = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     23.2      1.3
  positive     13.6     61.8
                            
 Accuracy (average) : 0.8509

[1] "TRAIN accuracy: 0.850877192982456"
[1] "TRAIN +precision: 0.819767441860465"
[1] "TRAIN -precision: 0.946428571428571"
[1] "TRAIN specifity: 0.630952380952381"
[1] "TRAIN sensitivity: 0.979166666666667"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        4        2
            positive       11       95
[1] "TEST accuracy: 0.883928571428571"
[1] "TEST +precision: 0.89622641509434"
[1] "TEST -precision: 0.666666666666667"
[1] "TEST specifity: 0.266666666666667"
[1] "TEST sensitivity: 0.979381443298969"

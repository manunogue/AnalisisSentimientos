[1] "DATASET NAME: MSM_Uni_IR_2"
[1] "TRAIN INSTANCES: 2635"
[1] "TEST INSTANCES: 600"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 9.66956210136414"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

2635 samples
 656 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 2108, 2108, 2108, 2108, 2108 
Resampling results:

  ROC        Sens  Spec     
  0.9994749  1     0.9948143

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     34.1      0.3
  positive      0.0     65.5
                            
 Accuracy (average) : 0.9966

[1] "TRAIN accuracy: 0.996584440227704"
[1] "TRAIN +precision: 1"
[1] "TRAIN -precision: 0.990088105726872"
[1] "TRAIN specifity: 1"
[1] "TRAIN sensitivity: 0.994815668202765"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        8        4
            positive       12      576
[1] "TEST accuracy: 0.973333333333333"
[1] "TEST +precision: 0.979591836734694"
[1] "TEST -precision: 0.666666666666667"
[1] "TEST specifity: 0.4"
[1] "TEST sensitivity: 0.993103448275862"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 3.33478875160217"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

2635 samples
 656 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 2108, 2108, 2108, 2108, 2108 
Resampling results across tuning parameters:

  C      M  ROC        Sens  Spec     
  0.010  1  0.9825663  1     0.9539170
  0.010  2  0.9826782  1     0.9533406
  0.010  3  0.9817804  1     0.9481599
  0.255  1  0.9875195  1     0.9654394
  0.255  2  0.9882354  1     0.9648630
  0.255  3  0.9882886  1     0.9608284
  0.500  1  0.9875216  1     0.9683212
  0.500  2  0.9886521  1     0.9677449
  0.500  3  0.9896751  1     0.9619812

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.5 and M = 3.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     34.1      2.5
  positive      0.0     63.4
                           
 Accuracy (average) : 0.975

[1] "TRAIN accuracy: 0.974952561669829"
[1] "TRAIN +precision: 1"
[1] "TRAIN -precision: 0.93160621761658"
[1] "TRAIN specifity: 1"
[1] "TRAIN sensitivity: 0.961981566820276"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        9       17
            positive       11      563
[1] "TEST accuracy: 0.953333333333333"
[1] "TEST +precision: 0.980836236933798"
[1] "TEST -precision: 0.346153846153846"
[1] "TEST specifity: 0.45"
[1] "TEST sensitivity: 0.970689655172414"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 7.74161798556646"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

2635 samples
 656 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 2108, 2108, 2108, 2108, 2108 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens       Spec     
  0.3  1          0.6               0.50        50      0.9770388  0.8198262  0.9729239
  0.3  1          0.6               0.50       100      0.9905891  0.9332961  0.9763788
  0.3  1          0.6               0.50       150      0.9951033  0.9655183  0.9786826
  0.3  1          0.6               0.75        50      0.9792609  0.8287461  0.9758041
  0.3  1          0.6               0.75       100      0.9909850  0.9221788  0.9763821
  0.3  1          0.6               0.75       150      0.9949673  0.9632961  0.9798337
  0.3  1          0.6               1.00        50      0.9785079  0.8075916  0.9758024
  0.3  1          0.6               1.00       100      0.9906599  0.9477343  0.9781096
  0.3  1          0.6               1.00       150      0.9955793  0.9655121  0.9798354
  0.3  1          0.8               0.50        50      0.9768957  0.8053569  0.9688943
  0.3  1          0.8               0.50       100      0.9907927  0.9343948  0.9775282
  0.3  1          0.8               0.50       150      0.9950954  0.9710739  0.9769535
  0.3  1          0.8               0.75        50      0.9795200  0.8376226  0.9758058
  0.3  1          0.8               0.75       100      0.9909443  0.9477343  0.9775316
  0.3  1          0.8               0.75       150      0.9952846  0.9699565  0.9804101
  0.3  1          0.8               1.00        50      0.9787118  0.8142582  0.9740783
  0.3  1          0.8               1.00       100      0.9907540  0.9399379  0.9758024
  0.3  1          0.8               1.00       150      0.9951021  0.9677343  0.9786826
  0.3  2          0.6               0.50        50      0.9953235  0.9499565  0.9758041
  0.3  2          0.6               0.50       100      0.9988216  1.0000000  0.9827139
  0.3  2          0.6               0.50       150      0.9994717  1.0000000  0.9855957
  0.3  2          0.6               0.75        50      0.9951048  0.9588454  0.9798354
  0.3  2          0.6               0.75       100      0.9989558  0.9932961  0.9861688
  0.3  2          0.6               0.75       150      0.9994749  1.0000000  0.9861738
  0.3  2          0.6               1.00        50      0.9951586  0.9677219  0.9792590
  0.3  2          0.6               1.00       100      0.9983889  0.9955556  0.9850194
  0.3  2          0.6               1.00       150      0.9992858  1.0000000  0.9861721
  0.3  2          0.8               0.50        50      0.9958952  0.9610739  0.9781046
  0.3  2          0.8               0.50       100      0.9986551  1.0000000  0.9867452
  0.3  2          0.8               0.50       150      0.9993436  1.0000000  0.9867468
  0.3  2          0.8               0.75        50      0.9954852  0.9588516  0.9815661
  0.3  2          0.8               0.75       100      0.9985334  1.0000000  0.9850194
  0.3  2          0.8               0.75       150      0.9990682  1.0000000  0.9884759
  0.3  2          0.8               1.00        50      0.9961128  0.9710739  0.9798337
  0.3  2          0.8               1.00       100      0.9988984  0.9955556  0.9873215
  0.3  2          0.8               1.00       150      0.9994268  1.0000000  0.9896254
  0.3  3          0.6               0.50        50      0.9985713  0.9899628  0.9827139
  0.3  3          0.6               0.50       100      0.9998206  1.0000000  0.9867468
  0.3  3          0.6               0.50       150      0.9998686  1.0000000  0.9878996
  0.3  3          0.6               0.75        50      0.9988037  0.9888516  0.9861705
  0.3  3          0.6               0.75       100      0.9996638  1.0000000  0.9890506
  0.3  3          0.6               0.75       150      0.9999039  1.0000000  0.9890523
  0.3  3          0.6               1.00        50      0.9992475  1.0000000  0.9873232
  0.3  3          0.6               1.00       100      0.9996093  1.0000000  0.9913561
  0.3  3          0.6               1.00       150      0.9995261  1.0000000  0.9913561
  0.3  3          0.8               0.50        50      0.9984243  0.9955556  0.9827156
  0.3  3          0.8               0.50       100      0.9994140  1.0000000  0.9867452
  0.3  3          0.8               0.50       150      0.9996958  1.0000000  0.9878996
  0.3  3          0.8               0.75        50      0.9983987  0.9888516  0.9838666
  0.3  3          0.8               0.75       100      0.9993627  1.0000000  0.9873249
  0.3  3          0.8               0.75       150      0.9994524  1.0000000  0.9890523
  0.3  3          0.8               1.00        50      0.9989303  0.9855183  0.9861705
  0.3  3          0.8               1.00       100      0.9997983  1.0000000  0.9907814
  0.3  3          0.8               1.00       150      0.9998911  1.0000000  0.9907814
  0.4  1          0.6               0.50        50      0.9839250  0.8732278  0.9683163
  0.4  1          0.6               0.50       100      0.9938958  0.9621850  0.9752310
  0.4  1          0.6               0.50       150      0.9967427  0.9799628  0.9804117
  0.4  1          0.6               0.75        50      0.9855114  0.8743203  0.9781079
  0.4  1          0.6               0.75       100      0.9940781  0.9644072  0.9769535
  0.4  1          0.6               0.75       150      0.9964957  0.9788516  0.9821392
  0.4  1          0.6               1.00        50      0.9839373  0.8720919  0.9746530
  0.4  1          0.6               1.00       100      0.9943429  0.9666294  0.9792607
  0.4  1          0.6               1.00       150      0.9965292  0.9788516  0.9804101
  0.4  1          0.8               0.50        50      0.9844420  0.8899131  0.9717712
  0.4  1          0.8               0.50       100      0.9939441  0.9655183  0.9735003
  0.4  1          0.8               0.50       150      0.9961879  0.9788516  0.9781112
  0.4  1          0.8               0.75        50      0.9857220  0.8643389  0.9763805
  0.4  1          0.8               0.75       100      0.9940658  0.9666294  0.9781129
  0.4  1          0.8               0.75       150      0.9969521  0.9710739  0.9804167
  0.4  1          0.8               1.00        50      0.9864207  0.8854252  0.9775332
  0.4  1          0.8               1.00       100      0.9943544  0.9677343  0.9786793
  0.4  1          0.8               1.00       150      0.9965533  0.9710739  0.9792590
  0.4  2          0.6               0.50        50      0.9966050  0.9744072  0.9792573
  0.4  2          0.6               0.50       100      0.9985141  1.0000000  0.9821359
  0.4  2          0.6               0.50       150      0.9986962  1.0000000  0.9844430
  0.4  2          0.6               0.75        50      0.9966640  0.9655183  0.9804117
  0.4  2          0.6               0.75       100      0.9987382  1.0000000  0.9838683
  0.4  2          0.6               0.75       150      0.9995228  1.0000000  0.9844447
  0.4  2          0.6               1.00        50      0.9976682  0.9788516  0.9832903
  0.4  2          0.6               1.00       100      0.9992283  1.0000000  0.9867468
  0.4  2          0.6               1.00       150      0.9997695  1.0000000  0.9873265
  0.4  2          0.8               0.50        50      0.9970532  0.9744072  0.9815645
  0.4  2          0.8               0.50       100      0.9988248  1.0000000  0.9855974
  0.4  2          0.8               0.50       150      0.9991770  1.0000000  0.9873265
  0.4  2          0.8               0.75        50      0.9971336  0.9799628  0.9804117
  0.4  2          0.8               0.75       100      0.9992827  1.0000000  0.9867468
  0.4  2          0.8               0.75       150      0.9994231  1.0000000  0.9861721
  0.4  2          0.8               1.00        50      0.9975287  0.9710739  0.9832886
  0.4  2          0.8               1.00       100      0.9993660  1.0000000  0.9873232
  0.4  2          0.8               1.00       150      0.9996734  1.0000000  0.9884792
  0.4  3          0.6               0.50        50      0.9990100  1.0000000  0.9855991
  0.4  3          0.6               0.50       100      0.9995420  1.0000000  0.9879029
  0.4  3          0.6               0.50       150      0.9995867  1.0000000  0.9879012
  0.4  3          0.6               0.75        50      0.9996893  1.0000000  0.9861705
  0.4  3          0.6               0.75       100      0.9998271  1.0000000  0.9902034
  0.4  3          0.6               0.75       150      0.9997150  1.0000000  0.9896287
  0.4  3          0.6               1.00        50      0.9996510  1.0000000  0.9890523
  0.4  3          0.6               1.00       100      0.9999904  1.0000000  0.9896270
  0.4  3          0.6               1.00       150      0.9999808  1.0000000  0.9890523
  0.4  3          0.8               0.50        50      0.9991321  1.0000000  0.9815595
  0.4  3          0.8               0.50       100      0.9998431  1.0000000  0.9867468
  0.4  3          0.8               0.50       150      0.9999296  1.0000000  0.9902034
  0.4  3          0.8               0.75        50      0.9991065  1.0000000  0.9861771
  0.4  3          0.8               0.75       100      0.9997310  1.0000000  0.9884776
  0.4  3          0.8               0.75       150      0.9998623  1.0000000  0.9879045
  0.4  3          0.8               1.00        50      0.9994460  1.0000000  0.9884743
  0.4  3          0.8               1.00       100      0.9995517  1.0000000  0.9919308
  0.4  3          0.8               1.00       150      0.9995197  1.0000000  0.9896270

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 100, max_depth = 3, eta = 0.4, gamma = 0, colsample_bytree = 0.6, min_child_weight = 1 and subsample = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     34.1      0.7
  positive      0.0     65.2
                            
 Accuracy (average) : 0.9932

[1] "TRAIN accuracy: 0.993168880455408"
[1] "TRAIN +precision: 1"
[1] "TRAIN -precision: 0.980370774263904"
[1] "TRAIN specifity: 1"
[1] "TRAIN sensitivity: 0.98963133640553"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       11        7
            positive        9      573
[1] "TEST accuracy: 0.973333333333333"
[1] "TEST +precision: 0.984536082474227"
[1] "TEST -precision: 0.611111111111111"
[1] "TEST specifity: 0.55"
[1] "TEST sensitivity: 0.987931034482759"

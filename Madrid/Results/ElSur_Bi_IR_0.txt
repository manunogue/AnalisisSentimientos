[1] "DATASET NAME: ElSur_Bi_IR_0"
[1] "TRAIN INSTANCES: 1186"
[1] "TEST INSTANCES: 396"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 2.66325402259827"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

1186 samples
 617 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 949, 948, 950, 948, 949 
Resampling results:

  ROC        Sens  Spec     
  0.7618107  0.05  0.9991416

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative      0.1      0.1
  positive      1.9     98.0
                            
 Accuracy (average) : 0.9806

[1] "TRAIN accuracy: 0.980607082630691"
[1] "TRAIN +precision: 0.981418918918919"
[1] "TRAIN -precision: 0.5"
[1] "TRAIN specifity: 0.0434782608695652"
[1] "TRAIN sensitivity: 0.999140154772141"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        0        0
            positive        6      390
[1] "TEST accuracy: 0.984848484848485"
[1] "TEST +precision: 0.984848484848485"
[1] "TEST -precision: NaN"
[1] "TEST specifity: 0"
[1] "TEST sensitivity: 1"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 1.3174874663353"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

1186 samples
 617 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 949, 948, 949, 949, 949 
Resampling results across tuning parameters:

  C      M  ROC  Sens  Spec
  0.010  1  0.5  0     1   
  0.010  2  0.5  0     1   
  0.010  3  0.5  0     1   
  0.255  1  0.5  0     1   
  0.255  2  0.5  0     1   
  0.255  3  0.5  0     1   
  0.500  1  0.5  0     1   
  0.500  2  0.5  0     1   
  0.500  3  0.5  0     1   

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.01 and M = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative      0.0      0.0
  positive      1.9     98.1
                            
 Accuracy (average) : 0.9806

[1] "TRAIN accuracy: 0.980607082630691"
[1] "TRAIN +precision: 0.980607082630691"
[1] "TRAIN -precision: NaN"
[1] "TRAIN specifity: 0"
[1] "TRAIN sensitivity: 1"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        0        0
            positive        6      390
[1] "TEST accuracy: 0.984848484848485"
[1] "TEST +precision: 0.984848484848485"
[1] "TEST -precision: NaN"
[1] "TEST specifity: 0"
[1] "TEST sensitivity: 1"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 3.23544848362605"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

1186 samples
 617 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 949, 949, 949, 948, 949 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens  Spec     
  0.3  1          0.6               0.50        50      0.6077904  0     1.0000000
  0.3  1          0.6               0.50       100      0.6162872  0     1.0000000
  0.3  1          0.6               0.50       150      0.6240199  0     1.0000000
  0.3  1          0.6               0.75        50      0.6680861  0     1.0000000
  0.3  1          0.6               0.75       100      0.6710023  0     1.0000000
  0.3  1          0.6               0.75       150      0.6792981  0     1.0000000
  0.3  1          0.6               1.00        50      0.6445268  0     1.0000000
  0.3  1          0.6               1.00       100      0.6457359  0     1.0000000
  0.3  1          0.6               1.00       150      0.6457359  0     1.0000000
  0.3  1          0.8               0.50        50      0.6481219  0     1.0000000
  0.3  1          0.8               0.50       100      0.6487169  0     1.0000000
  0.3  1          0.8               0.50       150      0.6514755  0     1.0000000
  0.3  1          0.8               0.75        50      0.6687350  0     1.0000000
  0.3  1          0.8               0.75       100      0.6678452  0     1.0000000
  0.3  1          0.8               0.75       150      0.6714659  0     1.0000000
  0.3  1          0.8               1.00        50      0.6381926  0     1.0000000
  0.3  1          0.8               1.00       100      0.6377139  0     1.0000000
  0.3  1          0.8               1.00       150      0.6377139  0     1.0000000
  0.3  2          0.6               0.50        50      0.6260815  0     1.0000000
  0.3  2          0.6               0.50       100      0.6327072  0     1.0000000
  0.3  2          0.6               0.50       150      0.6360637  0     1.0000000
  0.3  2          0.6               0.75        50      0.6574001  0     1.0000000
  0.3  2          0.6               0.75       100      0.6580935  0     1.0000000
  0.3  2          0.6               0.75       150      0.6609394  0     1.0000000
  0.3  2          0.6               1.00        50      0.6625614  0     1.0000000
  0.3  2          0.6               1.00       100      0.6673032  0     1.0000000
  0.3  2          0.6               1.00       150      0.6820693  0     1.0000000
  0.3  2          0.8               0.50        50      0.5823550  0     1.0000000
  0.3  2          0.8               0.50       100      0.5926887  0     1.0000000
  0.3  2          0.8               0.50       150      0.5932444  0     1.0000000
  0.3  2          0.8               0.75        50      0.6427727  0     1.0000000
  0.3  2          0.8               0.75       100      0.6445427  0     0.9991379
  0.3  2          0.8               0.75       150      0.6545642  0     1.0000000
  0.3  2          0.8               1.00        50      0.6506371  0     1.0000000
  0.3  2          0.8               1.00       100      0.6650277  0     1.0000000
  0.3  2          0.8               1.00       150      0.6756952  0     0.9991379
  0.3  3          0.6               0.50        50      0.5871352  0     1.0000000
  0.3  3          0.6               0.50       100      0.5987565  0     1.0000000
  0.3  3          0.6               0.50       150      0.5950862  0     1.0000000
  0.3  3          0.6               0.75        50      0.6682037  0     1.0000000
  0.3  3          0.6               0.75       100      0.6692286  0     1.0000000
  0.3  3          0.6               0.75       150      0.6730960  0     1.0000000
  0.3  3          0.6               1.00        50      0.6629525  0     1.0000000
  0.3  3          0.6               1.00       100      0.6702590  0     0.9991379
  0.3  3          0.6               1.00       150      0.6827105  0     0.9991379
  0.3  3          0.8               0.50        50      0.6006889  0     1.0000000
  0.3  3          0.8               0.50       100      0.5999993  0     1.0000000
  0.3  3          0.8               0.50       150      0.6033558  0     1.0000000
  0.3  3          0.8               0.75        50      0.6413908  0     1.0000000
  0.3  3          0.8               0.75       100      0.6422380  0     1.0000000
  0.3  3          0.8               0.75       150      0.6477952  0     0.9991379
  0.3  3          0.8               1.00        50      0.6623868  0     1.0000000
  0.3  3          0.8               1.00       100      0.6850736  0     0.9991379
  0.3  3          0.8               1.00       150      0.6872196  0     0.9991379
  0.4  1          0.6               0.50        50      0.6714222  0     1.0000000
  0.4  1          0.6               0.50       100      0.6813320  0     1.0000000
  0.4  1          0.6               0.50       150      0.6844354  0     1.0000000
  0.4  1          0.6               0.75        50      0.6540950  0     1.0000000
  0.4  1          0.6               0.75       100      0.6539226  0     1.0000000
  0.4  1          0.6               0.75       150      0.6589226  0     1.0000000
  0.4  1          0.6               1.00        50      0.6615784  0     1.0000000
  0.4  1          0.6               1.00       100      0.6645956  0     1.0000000
  0.4  1          0.6               1.00       150      0.6645956  0     1.0000000
  0.4  1          0.8               0.50        50      0.5942878  0     1.0000000
  0.4  1          0.8               0.50       100      0.6049301  0     1.0000000
  0.4  1          0.8               0.50       150      0.6192804  0     1.0000000
  0.4  1          0.8               0.75        50      0.6488479  0     1.0000000
  0.4  1          0.8               0.75       100      0.6594798  0     1.0000000
  0.4  1          0.8               0.75       150      0.6621326  0     1.0000000
  0.4  1          0.8               1.00        50      0.6147795  0     1.0000000
  0.4  1          0.8               1.00       100      0.6130554  0     1.0000000
  0.4  1          0.8               1.00       150      0.6130554  0     1.0000000
  0.4  2          0.6               0.50        50      0.6246703  0     1.0000000
  0.4  2          0.6               0.50       100      0.6278500  0     1.0000000
  0.4  2          0.6               0.50       150      0.6292696  0     1.0000000
  0.4  2          0.6               0.75        50      0.6619151  0     1.0000000
  0.4  2          0.6               0.75       100      0.6603522  0     0.9991379
  0.4  2          0.6               0.75       150      0.6586510  0     0.9991379
  0.4  2          0.6               1.00        50      0.6535404  0     1.0000000
  0.4  2          0.6               1.00       100      0.6611939  0     1.0000000
  0.4  2          0.6               1.00       150      0.6698035  0     0.9991379
  0.4  2          0.8               0.50        50      0.6369913  0     1.0000000
  0.4  2          0.8               0.50       100      0.6488697  0     1.0000000
  0.4  2          0.8               0.50       150      0.6563604  0     1.0000000
  0.4  2          0.8               0.75        50      0.6917360  0     1.0000000
  0.4  2          0.8               0.75       100      0.6993440  0     0.9991416
  0.4  2          0.8               0.75       150      0.6988098  0     0.9991416
  0.4  2          0.8               1.00        50      0.6567689  0     1.0000000
  0.4  2          0.8               1.00       100      0.6696600  0     0.9991416
  0.4  2          0.8               1.00       150      0.6851010  0     0.9991416
  0.4  3          0.6               0.50        50      0.6394306  0     1.0000000
  0.4  3          0.6               0.50       100      0.6452749  0     1.0000000
  0.4  3          0.6               0.50       150      0.6459653  0     1.0000000
  0.4  3          0.6               0.75        50      0.6555295  0     1.0000000
  0.4  3          0.6               0.75       100      0.6692885  0     1.0000000
  0.4  3          0.6               0.75       150      0.6728071  0     1.0000000
  0.4  3          0.6               1.00        50      0.6770945  0     1.0000000
  0.4  3          0.6               1.00       100      0.6894787  0     1.0000000
  0.4  3          0.6               1.00       150      0.6932185  0     1.0000000
  0.4  3          0.8               0.50        50      0.6259586  0     1.0000000
  0.4  3          0.8               0.50       100      0.6249811  0     1.0000000
  0.4  3          0.8               0.50       150      0.6299671  0     1.0000000
  0.4  3          0.8               0.75        50      0.6514348  0     1.0000000
  0.4  3          0.8               0.75       100      0.6719269  0     1.0000000
  0.4  3          0.8               0.75       150      0.6720616  0     1.0000000
  0.4  3          0.8               1.00        50      0.6569831  0     1.0000000
  0.4  3          0.8               1.00       100      0.6836436  0     0.9991416
  0.4  3          0.8               1.00       150      0.6846744  0     0.9982796

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 100, max_depth = 2, eta = 0.4, gamma = 0, colsample_bytree = 0.8, min_child_weight = 1 and subsample = 0.75.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative      0.0      0.1
  positive      1.9     98.0
                            
 Accuracy (average) : 0.9798

[1] "TRAIN accuracy: 0.979763912310287"
[1] "TRAIN +precision: 0.980590717299578"
[1] "TRAIN -precision: 0"
[1] "TRAIN specifity: 0"
[1] "TRAIN sensitivity: 0.999140154772141"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        0        0
            positive        6      390
[1] "TEST accuracy: 0.984848484848485"
[1] "TEST +precision: 0.984848484848485"
[1] "TEST -precision: NaN"
[1] "TEST specifity: 0"
[1] "TEST sensitivity: 1"

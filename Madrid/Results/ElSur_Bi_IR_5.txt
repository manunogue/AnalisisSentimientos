[1] "DATASET NAME: ElSur_Bi_IR_5"
[1] "TRAIN INSTANCES: 1414"
[1] "TEST INSTANCES: 396"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 3.18706893920898"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

1414 samples
 617 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1131, 1131, 1132, 1131, 1131 
Resampling results:

  ROC        Sens  Spec     
  0.9965628  1     0.9939803

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     17.8      0.5
  positive      0.0     81.8
                           
 Accuracy (average) : 0.995

[1] "TRAIN accuracy: 0.995049504950495"
[1] "TRAIN +precision: 1"
[1] "TRAIN -precision: 0.972868217054264"
[1] "TRAIN specifity: 1"
[1] "TRAIN sensitivity: 0.993981083404987"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        0        1
            positive        6      389
[1] "TEST accuracy: 0.982323232323232"
[1] "TEST +precision: 0.984810126582279"
[1] "TEST -precision: 0"
[1] "TEST specifity: 0"
[1] "TEST sensitivity: 0.997435897435897"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 1.56960066556931"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

1414 samples
 617 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1131, 1131, 1132, 1130, 1132 
Resampling results across tuning parameters:

  C      M  ROC        Sens       Spec     
  0.010  1  0.7922697  0.5175686  0.9922488
  0.010  2  0.7922697  0.5175686  0.9922488
  0.010  3  0.7937956  0.5175686  0.9948350
  0.255  1  0.8037829  0.5255686  0.9974212
  0.255  2  0.8037829  0.5255686  0.9974212
  0.255  3  0.8037829  0.5255686  0.9974212
  0.500  1  0.8107226  0.5255686  0.9974212
  0.500  2  0.8107226  0.5255686  0.9974212
  0.500  3  0.8107226  0.5255686  0.9974212

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.5 and M = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative      9.3      0.2
  positive      8.4     82.0
                            
 Accuracy (average) : 0.9137

[1] "TRAIN accuracy: 0.913719943422914"
[1] "TRAIN +precision: 0.906958561376075"
[1] "TRAIN -precision: 0.977777777777778"
[1] "TRAIN specifity: 0.525896414342629"
[1] "TRAIN sensitivity: 0.997420464316423"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        0        1
            positive        6      389
[1] "TEST accuracy: 0.982323232323232"
[1] "TEST +precision: 0.984810126582279"
[1] "TEST -precision: 0"
[1] "TEST specifity: 0"
[1] "TEST sensitivity: 0.997435897435897"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 4.48541591962179"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

1414 samples
 617 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1131, 1131, 1132, 1132, 1130 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens       Spec     
  0.3  1          0.6               0.50        50      0.9392255  0.5778824  0.9957045
  0.3  1          0.6               0.50       100      0.9632839  0.8526275  0.9939840
  0.3  1          0.6               0.50       150      0.9699494  0.9046275  0.9931182
  0.3  1          0.6               0.75        50      0.9489117  0.5976471  0.9956971
  0.3  1          0.6               0.75       100      0.9740891  0.8406275  0.9914052
  0.3  1          0.6               0.75       150      0.9782965  0.9323922  0.9914052
  0.3  1          0.6               1.00        50      0.9634820  0.6013333  0.9965628
  0.3  1          0.6               1.00       100      0.9806492  0.8363922  0.9905468
  0.3  1          0.6               1.00       150      0.9834175  0.9001569  0.9879717
  0.3  1          0.8               0.50        50      0.9383706  0.6017255  0.9939803
  0.3  1          0.8               0.50       100      0.9627400  0.8006275  0.9931219
  0.3  1          0.8               0.50       150      0.9696113  0.9084706  0.9922636
  0.3  1          0.8               0.75        50      0.9556333  0.6012549  0.9939803
  0.3  1          0.8               0.75       100      0.9763176  0.8566275  0.9931219
  0.3  1          0.8               0.75       150      0.9809931  0.9281569  0.9922636
  0.3  1          0.8               1.00        50      0.9614570  0.5893333  0.9957045
  0.3  1          0.8               1.00       100      0.9805724  0.8484706  0.9905468
  0.3  1          0.8               1.00       150      0.9843817  0.9001569  0.9888301
  0.3  2          0.6               0.50        50      0.9611684  0.8644706  0.9939803
  0.3  2          0.6               0.50       100      0.9737631  0.8923922  0.9922636
  0.3  2          0.6               0.50       150      0.9735739  0.9123922  0.9931219
  0.3  2          0.6               0.75        50      0.9724745  0.8605490  0.9896885
  0.3  2          0.6               0.75       100      0.9793933  0.9400784  0.9896885
  0.3  2          0.6               0.75       150      0.9811738  0.9400784  0.9888301
  0.3  2          0.6               1.00        50      0.9822380  0.8763922  0.9905468
  0.3  2          0.6               1.00       100      0.9867202  0.9640000  0.9879717
  0.3  2          0.6               1.00       150      0.9884542  0.9640000  0.9871134
  0.3  2          0.8               0.50        50      0.9620153  0.8363137  0.9931219
  0.3  2          0.8               0.50       100      0.9725001  0.9083137  0.9922636
  0.3  2          0.8               0.50       150      0.9739409  0.9163137  0.9896885
  0.3  2          0.8               0.75        50      0.9711280  0.8684706  0.9914015
  0.3  2          0.8               0.75       100      0.9774296  0.9321569  0.9922636
  0.3  2          0.8               0.75       150      0.9795135  0.9321569  0.9914052
  0.3  2          0.8               1.00        50      0.9809689  0.8683922  0.9888301
  0.3  2          0.8               1.00       100      0.9857325  0.9400784  0.9871097
  0.3  2          0.8               1.00       150      0.9877521  0.9640784  0.9862513
  0.3  3          0.6               0.50        50      0.9704112  0.9045490  0.9922636
  0.3  3          0.6               0.50       100      0.9749820  0.9125490  0.9914052
  0.3  3          0.6               0.50       150      0.9753377  0.9203922  0.9931256
  0.3  3          0.6               0.75        50      0.9840110  0.9243922  0.9905431
  0.3  3          0.6               0.75       100      0.9858842  0.9561569  0.9896848
  0.3  3          0.6               0.75       150      0.9863962  0.9561569  0.9914052
  0.3  3          0.6               1.00        50      0.9877105  0.9363137  0.9888301
  0.3  3          0.6               1.00       100      0.9902904  0.9720784  0.9871134
  0.3  3          0.6               1.00       150      0.9903167  0.9720784  0.9888301
  0.3  3          0.8               0.50        50      0.9711290  0.9045490  0.9914052
  0.3  3          0.8               0.50       100      0.9763538  0.9203922  0.9922636
  0.3  3          0.8               0.50       150      0.9777900  0.9203922  0.9922636
  0.3  3          0.8               0.75        50      0.9793049  0.9123137  0.9896885
  0.3  3          0.8               0.75       100      0.9826618  0.9441569  0.9888301
  0.3  3          0.8               0.75       150      0.9832790  0.9441569  0.9914089
  0.3  3          0.8               1.00        50      0.9861414  0.9363137  0.9888301
  0.3  3          0.8               1.00       100      0.9894472  0.9680784  0.9862513
  0.3  3          0.8               1.00       150      0.9898482  0.9680784  0.9879717
  0.4  1          0.6               0.50        50      0.9531663  0.6768627  0.9939803
  0.4  1          0.6               0.50       100      0.9654830  0.8923922  0.9896885
  0.4  1          0.6               0.50       150      0.9703238  0.9123922  0.9905468
  0.4  1          0.6               0.75        50      0.9609611  0.7167059  0.9948424
  0.4  1          0.6               0.75       100      0.9781328  0.9202353  0.9922636
  0.4  1          0.6               0.75       150      0.9814796  0.9481569  0.9914052
  0.4  1          0.6               1.00        50      0.9694763  0.6808627  0.9931256
  0.4  1          0.6               1.00       100      0.9803130  0.9203137  0.9896885
  0.4  1          0.6               1.00       150      0.9840267  0.9320784  0.9896885
  0.4  1          0.8               0.50        50      0.9546008  0.6891765  0.9931219
  0.4  1          0.8               0.50       100      0.9677258  0.9123922  0.9905468
  0.4  1          0.8               0.50       150      0.9721042  0.9123922  0.9896885
  0.4  1          0.8               0.75        50      0.9617248  0.7486275  0.9914052
  0.4  1          0.8               0.75       100      0.9751382  0.9121569  0.9888301
  0.4  1          0.8               0.75       150      0.9782452  0.9361569  0.9896885
  0.4  1          0.8               1.00        50      0.9682639  0.6886275  0.9922673
  0.4  1          0.8               1.00       100      0.9811049  0.8923137  0.9914052
  0.4  1          0.8               1.00       150      0.9836733  0.9520784  0.9905468
  0.4  2          0.6               0.50        50      0.9572926  0.8486275  0.9905394
  0.4  2          0.6               0.50       100      0.9698257  0.8726275  0.9922636
  0.4  2          0.6               0.50       150      0.9720407  0.8806275  0.9922599
  0.4  2          0.6               0.75        50      0.9760981  0.9201569  0.9905468
  0.4  2          0.6               0.75       100      0.9810043  0.9361569  0.9905468
  0.4  2          0.6               0.75       150      0.9813317  0.9361569  0.9888301
  0.4  2          0.6               1.00        50      0.9809191  0.9320784  0.9888301
  0.4  2          0.6               1.00       100      0.9849684  0.9560784  0.9879717
  0.4  2          0.6               1.00       150      0.9864540  0.9560784  0.9879717
  0.4  2          0.8               0.50        50      0.9646103  0.8566275  0.9896885
  0.4  2          0.8               0.50       100      0.9738578  0.9084706  0.9922636
  0.4  2          0.8               0.50       150      0.9755563  0.9164706  0.9922636
  0.4  2          0.8               0.75        50      0.9724756  0.9041569  0.9905431
  0.4  2          0.8               0.75       100      0.9787881  0.9281569  0.9896885
  0.4  2          0.8               0.75       150      0.9799527  0.9281569  0.9896885
  0.4  2          0.8               1.00        50      0.9841151  0.9283137  0.9914089
  0.4  2          0.8               1.00       100      0.9876858  0.9600784  0.9896885
  0.4  2          0.8               1.00       150      0.9881750  0.9600784  0.9888301
  0.4  3          0.6               0.50        50      0.9746728  0.8925490  0.9931182
  0.4  3          0.6               0.50       100      0.9766999  0.9043922  0.9939803
  0.4  3          0.6               0.50       150      0.9766929  0.9123922  0.9939803
  0.4  3          0.6               0.75        50      0.9768217  0.9283137  0.9905505
  0.4  3          0.6               0.75       100      0.9805712  0.9283137  0.9914089
  0.4  3          0.6               0.75       150      0.9813639  0.9283137  0.9922673
  0.4  3          0.6               1.00        50      0.9863187  0.9560784  0.9896885
  0.4  3          0.6               1.00       100      0.9874502  0.9560784  0.9888338
  0.4  3          0.6               1.00       150      0.9880663  0.9560784  0.9931256
  0.4  3          0.8               0.50        50      0.9735373  0.8884706  0.9896885
  0.4  3          0.8               0.50       100      0.9763518  0.9084706  0.9905468
  0.4  3          0.8               0.50       150      0.9769309  0.9084706  0.9905468
  0.4  3          0.8               0.75        50      0.9788913  0.9283922  0.9888301
  0.4  3          0.8               0.75       100      0.9800992  0.9283922  0.9905468
  0.4  3          0.8               0.75       150      0.9808480  0.9283922  0.9905505
  0.4  3          0.8               1.00        50      0.9855080  0.9600784  0.9871134
  0.4  3          0.8               1.00       100      0.9875591  0.9600784  0.9879717
  0.4  3          0.8               1.00       150      0.9875997  0.9600784  0.9905505

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 150, max_depth = 3, eta = 0.3, gamma = 0, colsample_bytree = 0.6, min_child_weight = 1 and subsample = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     17.3      0.9
  positive      0.5     81.3
                            
 Accuracy (average) : 0.9859

[1] "TRAIN accuracy: 0.985855728429986"
[1] "TRAIN +precision: 0.993949870354365"
[1] "TRAIN -precision: 0.949416342412451"
[1] "TRAIN specifity: 0.972111553784861"
[1] "TRAIN sensitivity: 0.988822012037833"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        0        2
            positive        6      388
[1] "TEST accuracy: 0.97979797979798"
[1] "TEST +precision: 0.984771573604061"
[1] "TEST -precision: 0"
[1] "TEST specifity: 0"
[1] "TEST sensitivity: 0.994871794871795"

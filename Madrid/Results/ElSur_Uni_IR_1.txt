[1] "DATASET NAME: ElSur_Uni_IR_1"
[1] "TRAIN INSTANCES: 2324"
[1] "TEST INSTANCES: 396"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 6.07234597206116"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

2324 samples
 684 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1858, 1859, 1860, 1859, 1860 
Resampling results:

  ROC  Sens  Spec     
  1    1     0.9982833

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     50.0      0.1
  positive      0.0     49.9
                            
 Accuracy (average) : 0.9991

[1] "TRAIN accuracy: 0.999139414802065"
[1] "TRAIN +precision: 1"
[1] "TRAIN -precision: 0.998281786941581"
[1] "TRAIN specifity: 1"
[1] "TRAIN sensitivity: 0.998278829604131"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        1        0
            positive        4      391
[1] "TEST accuracy: 0.98989898989899"
[1] "TEST +precision: 0.989873417721519"
[1] "TEST -precision: 1"
[1] "TEST specifity: 0.2"
[1] "TEST sensitivity: 1"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 2.53442813158035"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

2324 samples
 684 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1859, 1859, 1859, 1859, 1860 
Resampling results across tuning parameters:

  C      M  ROC        Sens  Spec     
  0.010  1  0.9895208  1     0.9724545
  0.010  2  0.9895208  1     0.9724545
  0.010  3  0.9895208  1     0.9724545
  0.255  1  0.9933553  1     0.9793399
  0.255  2  0.9933553  1     0.9793399
  0.255  3  0.9933553  1     0.9793399
  0.500  1  0.9922713  1     0.9810641
  0.500  2  0.9922713  1     0.9810641
  0.500  3  0.9936254  1     0.9810641

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.5 and M = 3.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     50.0      0.9
  positive      0.0     49.1
                            
 Accuracy (average) : 0.9905

[1] "TRAIN accuracy: 0.990533562822719"
[1] "TRAIN +precision: 1"
[1] "TRAIN -precision: 0.981418918918919"
[1] "TRAIN specifity: 1"
[1] "TRAIN sensitivity: 0.981067125645439"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        3        6
            positive        2      385
[1] "TEST accuracy: 0.97979797979798"
[1] "TEST +precision: 0.994832041343669"
[1] "TEST -precision: 0.333333333333333"
[1] "TEST specifity: 0.6"
[1] "TEST sensitivity: 0.9846547314578"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 7.82820403178533"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

2324 samples
 684 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1860, 1858, 1860, 1859, 1859 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens  Spec     
  0.3  1          0.6               0.50        50      0.9980483  1     0.9216738
  0.3  1          0.6               0.50       100      0.9990454  1     0.9724693
  0.3  1          0.6               0.50       150      0.9986236  1     0.9827956
  0.3  1          0.6               0.75        50      0.9992127  1     0.9165384
  0.3  1          0.6               0.75       100      0.9991116  1     0.9716072
  0.3  1          0.6               0.75       150      0.9990824  1     0.9853781
  0.3  1          0.6               1.00        50      0.9987353  1     0.9173857
  0.3  1          0.6               1.00       100      0.9993208  1     0.9776269
  0.3  1          0.6               1.00       150      0.9991120  1     0.9853744
  0.3  1          0.8               0.50        50      0.9977518  1     0.9191135
  0.3  1          0.8               0.50       100      0.9989784  1     0.9681663
  0.3  1          0.8               0.50       150      0.9985830  1     0.9802131
  0.3  1          0.8               0.75        50      0.9987838  1     0.9208339
  0.3  1          0.8               0.75       100      0.9990043  1     0.9741860
  0.3  1          0.8               0.75       150      0.9986236  1     0.9853744
  0.3  1          0.8               1.00        50      0.9988807  1     0.9191172
  0.3  1          0.8               1.00       100      0.9993022  1     0.9707600
  0.3  1          0.8               1.00       150      0.9989677  1     0.9853744
  0.3  2          0.6               0.50        50      0.9995671  1     0.9827919
  0.3  2          0.6               0.50       100      0.9990195  1     0.9870949
  0.3  2          0.6               0.50       150      0.9985719  1     0.9888116
  0.3  2          0.6               0.75        50      0.9993229  1     0.9845161
  0.3  2          0.6               0.75       100      0.9991675  1     0.9922525
  0.3  2          0.6               0.75       150      0.9988678  1     0.9888079
  0.3  2          0.6               1.00        50      0.9992156  1     0.9870949
  0.3  2          0.6               1.00       100      0.9991231  1     0.9905431
  0.3  2          0.6               1.00       150      0.9987901  1     0.9922599
  0.3  2          0.8               0.50        50      0.9995560  1     0.9759102
  0.3  2          0.8               0.50       100      0.9988382  1     0.9888153
  0.3  2          0.8               0.50       150      0.9984572  1     0.9896774
  0.3  2          0.8               0.75        50      0.9994968  1     0.9836577
  0.3  2          0.8               0.75       100      0.9994635  1     0.9888116
  0.3  2          0.8               0.75       150      0.9988456  1     0.9845087
  0.3  2          0.8               1.00        50      0.9991453  1     0.9836540
  0.3  2          0.8               1.00       100      0.9991120  1     0.9879495
  0.3  2          0.8               1.00       150      0.9988826  1     0.9888079
  0.3  3          0.6               0.50        50      1.0000000  1     0.9853744
  0.3  3          0.6               0.50       100      0.9998927  1     0.9896774
  0.3  3          0.6               0.50       150      0.9992119  1     0.9862291
  0.3  3          0.6               0.75        50      1.0000000  1     0.9913978
  0.3  3          0.6               0.75       100      0.9997077  1     0.9913941
  0.3  3          0.6               0.75       150      0.9995412  1     0.9913904
  0.3  3          0.6               1.00        50      1.0000000  1     0.9888190
  0.3  3          0.6               1.00       100      0.9998224  1     0.9879532
  0.3  3          0.6               1.00       150      0.9995745  1     0.9888116
  0.3  3          0.8               0.50        50      1.0000000  1     0.9931219
  0.3  3          0.8               0.50       100      0.9992526  1     0.9922562
  0.3  3          0.8               0.50       150      0.9991527  1     0.9931182
  0.3  3          0.8               0.75        50      0.9999630  1     0.9931219
  0.3  3          0.8               0.75       100      0.9993932  1     0.9905320
  0.3  3          0.8               0.75       150      0.9988234  1     0.9922525
  0.3  3          0.8               1.00        50      1.0000000  1     0.9913978
  0.3  3          0.8               1.00       100      0.9994450  1     0.9922562
  0.3  3          0.8               1.00       150      0.9992526  1     0.9931108
  0.4  1          0.6               0.50        50      0.9982582  1     0.9475137
  0.4  1          0.6               0.50       100      0.9992266  1     0.9793584
  0.4  1          0.6               0.50       150      0.9986347  1     0.9810752
  0.4  1          0.6               0.75        50      0.9988427  1     0.9492304
  0.4  1          0.6               0.75       100      0.9986902  1     0.9802094
  0.4  1          0.6               0.75       150      0.9987235  1     0.9870912
  0.4  1          0.6               1.00        50      0.9992128  1     0.9526787
  0.4  1          0.6               1.00       100      0.9997780  1     0.9836503
  0.4  1          0.6               1.00       150      0.9994746  1     0.9853744
  0.4  1          0.8               0.50        50      0.9985136  1     0.9432033
  0.4  1          0.8               0.50       100      0.9987605  1     0.9793547
  0.4  1          0.8               0.50       150      0.9983203  1     0.9845124
  0.4  1          0.8               0.75        50      0.9986603  1     0.9509546
  0.4  1          0.8               0.75       100      0.9988493  1     0.9784890
  0.4  1          0.8               0.75       150      0.9987272  1     0.9853670
  0.4  1          0.8               1.00        50      0.9989980  1     0.9466516
  0.4  1          0.8               1.00       100      0.9996485  1     0.9853744
  0.4  1          0.8               1.00       150      0.9991527  1     0.9862328
  0.4  2          0.6               0.50        50      0.9990787  1     0.9870986
  0.4  2          0.6               0.50       100      0.9984720  1     0.9870912
  0.4  2          0.6               0.50       150      0.9984313  1     0.9870875
  0.4  2          0.6               0.75        50      0.9991046  1     0.9862328
  0.4  2          0.6               0.75       100      0.9984239  1     0.9913904
  0.4  2          0.6               0.75       150      0.9984239  1     0.9896737
  0.4  2          0.6               1.00        50      0.9999297  1     0.9879569
  0.4  2          0.6               1.00       100      0.9997669  1     0.9905394
  0.4  2          0.6               1.00       150      0.9992526  1     0.9888116
  0.4  2          0.8               0.50        50      0.9994370  1     0.9870986
  0.4  2          0.8               0.50       100      0.9985830  1     0.9862291
  0.4  2          0.8               0.50       150      0.9984091  1     0.9827808
  0.4  2          0.8               0.75        50      1.0000000  1     0.9888190
  0.4  2          0.8               0.75       100      0.9993710  1     0.9879532
  0.4  2          0.8               0.75       150      0.9986347  1     0.9853670
  0.4  2          0.8               1.00        50      0.9993340  1     0.9879532
  0.4  2          0.8               1.00       100      0.9992156  1     0.9896700
  0.4  2          0.8               1.00       150      0.9991416  1     0.9870875
  0.4  3          0.6               0.50        50      0.9993044  1     0.9905357
  0.4  3          0.6               0.50       100      0.9991786  1     0.9922562
  0.4  3          0.6               0.50       150      0.9988456  1     0.9922562
  0.4  3          0.6               0.75        50      0.9994820  1     0.9939766
  0.4  3          0.6               0.75       100      0.9989788  1     0.9905320
  0.4  3          0.6               0.75       150      0.9990713  1     0.9939729
  0.4  3          0.6               1.00        50      1.0000000  1     0.9913941
  0.4  3          0.6               1.00       100      0.9994265  1     0.9913941
  0.4  3          0.6               1.00       150      0.9993192  1     0.9913941
  0.4  3          0.8               0.50        50      0.9998927  1     0.9905320
  0.4  3          0.8               0.50       100      0.9994783  1     0.9879421
  0.4  3          0.8               0.50       150      0.9993525  1     0.9888079
  0.4  3          0.8               0.75        50      1.0000000  1     0.9939766
  0.4  3          0.8               0.75       100      0.9993858  1     0.9913904
  0.4  3          0.8               0.75       150      0.9993303  1     0.9931108
  0.4  3          0.8               1.00        50      0.9999704  1     0.9931145
  0.4  3          0.8               1.00       100      0.9994043  1     0.9896737
  0.4  3          0.8               1.00       150      0.9992600  1     0.9913941

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 50, max_depth = 2, eta = 0.4, gamma = 0, colsample_bytree = 0.8, min_child_weight = 1 and subsample = 0.75.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     50.0      0.6
  positive      0.0     49.4
                            
 Accuracy (average) : 0.9944

[1] "TRAIN accuracy: 0.994406196213425"
[1] "TRAIN +precision: 1"
[1] "TRAIN -precision: 0.988936170212766"
[1] "TRAIN specifity: 1"
[1] "TRAIN sensitivity: 0.98881239242685"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        4        1
            positive        1      390
[1] "TEST accuracy: 0.994949494949495"
[1] "TEST +precision: 0.997442455242967"
[1] "TEST -precision: 0.8"
[1] "TEST specifity: 0.8"
[1] "TEST sensitivity: 0.997442455242967"

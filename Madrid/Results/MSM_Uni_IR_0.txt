[1] "DATASET NAME: MSM_Uni_IR_0"
[1] "TRAIN INSTANCES: 1798"
[1] "TEST INSTANCES: 600"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 7.07250690460205"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

1798 samples
 656 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1438, 1438, 1439, 1438, 1439 
Resampling results:

  ROC        Sens       Spec    
  0.9358086  0.2384615  0.995394

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative      0.8      0.4
  positive      2.6     96.1
                            
 Accuracy (average) : 0.9694

[1] "TRAIN accuracy: 0.969410456062291"
[1] "TRAIN +precision: 0.973521126760563"
[1] "TRAIN -precision: 0.652173913043478"
[1] "TRAIN specifity: 0.241935483870968"
[1] "TRAIN sensitivity: 0.995391705069124"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        9        6
            positive       11      574
[1] "TEST accuracy: 0.971666666666667"
[1] "TEST +precision: 0.981196581196581"
[1] "TEST -precision: 0.6"
[1] "TEST specifity: 0.45"
[1] "TEST sensitivity: 0.989655172413793"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 2.17263453006744"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

1798 samples
 656 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1438, 1439, 1438, 1438, 1439 
Resampling results across tuning parameters:

  C      M  ROC        Sens        Spec     
  0.010  1  0.5239599  0.04871795  0.9959654
  0.010  2  0.5155139  0.03333333  0.9971182
  0.010  3  0.5155139  0.03333333  0.9971182
  0.255  1  0.4718891  0.08205128  0.9948143
  0.255  2  0.5335033  0.08205128  0.9942380
  0.255  3  0.5644746  0.06666667  0.9953890
  0.500  1  0.5097539  0.09743590  0.9838666
  0.500  2  0.5873029  0.09743590  0.9873249
  0.500  3  0.5377214  0.06666667  0.9930852

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.5 and M = 2.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative      0.3      1.2
  positive      3.1     95.3
                            
 Accuracy (average) : 0.9566

[1] "TRAIN accuracy: 0.956618464961068"
[1] "TRAIN +precision: 0.968361581920904"
[1] "TRAIN -precision: 0.214285714285714"
[1] "TRAIN specifity: 0.0967741935483871"
[1] "TRAIN sensitivity: 0.987327188940092"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        6       12
            positive       14      568
[1] "TEST accuracy: 0.956666666666667"
[1] "TEST +precision: 0.975945017182131"
[1] "TEST -precision: 0.333333333333333"
[1] "TEST specifity: 0.3"
[1] "TEST sensitivity: 0.979310344827586"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 6.17938206593196"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

1798 samples
 656 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1438, 1439, 1439, 1437, 1439 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens       Spec     
  0.3  1          0.6               0.50        50      0.8788865  0.1282051  0.9988473
  0.3  1          0.6               0.50       100      0.8935545  0.3038462  0.9976945
  0.3  1          0.6               0.50       150      0.9007272  0.3051282  0.9953890
  0.3  1          0.6               0.75        50      0.8975418  0.2102564  0.9976945
  0.3  1          0.6               0.75       100      0.8981856  0.2884615  0.9948127
  0.3  1          0.6               0.75       150      0.9012448  0.3038462  0.9953890
  0.3  1          0.6               1.00        50      0.8963741  0.1294872  0.9988473
  0.3  1          0.6               1.00       100      0.9067624  0.2410256  0.9971182
  0.3  1          0.6               1.00       150      0.9105487  0.3192308  0.9948127
  0.3  1          0.8               0.50        50      0.8702392  0.1743590  0.9982709
  0.3  1          0.8               0.50       100      0.8924496  0.2243590  0.9971182
  0.3  1          0.8               0.50       150      0.8922674  0.2717949  0.9953924
  0.3  1          0.8               0.75        50      0.8968469  0.1769231  0.9976945
  0.3  1          0.8               0.75       100      0.9061204  0.2884615  0.9959654
  0.3  1          0.8               0.75       150      0.9055539  0.3038462  0.9948127
  0.3  1          0.8               1.00        50      0.8926351  0.1615385  0.9982709
  0.3  1          0.8               1.00       100      0.9077717  0.2423077  0.9971182
  0.3  1          0.8               1.00       150      0.9109974  0.3038462  0.9942363
  0.3  2          0.6               0.50        50      0.8955275  0.2397436  0.9976945
  0.3  2          0.6               0.50       100      0.9055693  0.2717949  0.9948143
  0.3  2          0.6               0.50       150      0.8959218  0.2717949  0.9925089
  0.3  2          0.6               0.75        50      0.9027865  0.3038462  0.9959654
  0.3  2          0.6               0.75       100      0.9183500  0.3512821  0.9942380
  0.3  2          0.6               0.75       150      0.9149946  0.3525641  0.9925089
  0.3  2          0.6               1.00        50      0.9273046  0.2076923  0.9976945
  0.3  2          0.6               1.00       100      0.9328200  0.2564103  0.9953890
  0.3  2          0.6               1.00       150      0.9307142  0.3038462  0.9936616
  0.3  2          0.8               0.50        50      0.8863998  0.2243590  0.9976945
  0.3  2          0.8               0.50       100      0.8993466  0.2730769  0.9942380
  0.3  2          0.8               0.50       150      0.8926229  0.3038462  0.9936616
  0.3  2          0.8               0.75        50      0.9067271  0.1923077  0.9965434
  0.3  2          0.8               0.75       100      0.9157704  0.3038462  0.9942380
  0.3  2          0.8               0.75       150      0.9112300  0.3500000  0.9930852
  0.3  2          0.8               1.00        50      0.9229381  0.1769231  0.9982709
  0.3  2          0.8               1.00       100      0.9273368  0.3192308  0.9953890
  0.3  2          0.8               1.00       150      0.9272824  0.3192308  0.9930852
  0.3  3          0.6               0.50        50      0.9079061  0.2551282  0.9942380
  0.3  3          0.6               0.50       100      0.8948715  0.2551282  0.9948127
  0.3  3          0.6               0.50       150      0.9046108  0.2551282  0.9919341
  0.3  3          0.6               0.75        50      0.9223561  0.2551282  0.9965434
  0.3  3          0.6               0.75       100      0.9146709  0.2717949  0.9959671
  0.3  3          0.6               0.75       150      0.9115420  0.3038462  0.9942380
  0.3  3          0.6               1.00        50      0.9138416  0.2538462  0.9971182
  0.3  3          0.6               1.00       100      0.9212734  0.3192308  0.9953907
  0.3  3          0.6               1.00       150      0.9180455  0.3358974  0.9953907
  0.3  3          0.8               0.50        50      0.8972198  0.3205128  0.9953907
  0.3  3          0.8               0.50       100      0.8917378  0.3038462  0.9925105
  0.3  3          0.8               0.50       150      0.8829283  0.2564103  0.9930852
  0.3  3          0.8               0.75        50      0.9032848  0.2551282  0.9948160
  0.3  3          0.8               0.75       100      0.8947779  0.2884615  0.9930852
  0.3  3          0.8               0.75       150      0.8948110  0.3038462  0.9936616
  0.3  3          0.8               1.00        50      0.9233719  0.2230769  0.9971182
  0.3  3          0.8               1.00       100      0.9248402  0.3038462  0.9948143
  0.3  3          0.8               1.00       150      0.9222254  0.3051282  0.9936616
  0.4  1          0.6               0.50        50      0.8845292  0.2410256  0.9976945
  0.4  1          0.6               0.50       100      0.8966674  0.2871795  0.9942363
  0.4  1          0.6               0.50       150      0.8985620  0.3038462  0.9913561
  0.4  1          0.6               0.75        50      0.8991445  0.2871795  0.9965418
  0.4  1          0.6               0.75       100      0.9085402  0.2717949  0.9953890
  0.4  1          0.6               0.75       150      0.9132704  0.3192308  0.9925089
  0.4  1          0.6               1.00        50      0.8963635  0.1935897  0.9976945
  0.4  1          0.6               1.00       100      0.9126164  0.3025641  0.9976945
  0.4  1          0.6               1.00       150      0.9154077  0.3025641  0.9953890
  0.4  1          0.8               0.50        50      0.8813296  0.1923077  0.9959654
  0.4  1          0.8               0.50       100      0.8888014  0.3358974  0.9913578
  0.4  1          0.8               0.50       150      0.8890183  0.3525641  0.9925072
  0.4  1          0.8               0.75        50      0.8966861  0.1923077  0.9965434
  0.4  1          0.8               0.75       100      0.9070426  0.3038462  0.9948127
  0.4  1          0.8               0.75       150      0.9059046  0.3192308  0.9942363
  0.4  1          0.8               1.00        50      0.8962446  0.1948718  0.9976945
  0.4  1          0.8               1.00       100      0.9096097  0.2871795  0.9953890
  0.4  1          0.8               1.00       150      0.9093598  0.3025641  0.9948127
  0.4  2          0.6               0.50        50      0.8966334  0.3179487  0.9948143
  0.4  2          0.6               0.50       100      0.8980197  0.3038462  0.9925089
  0.4  2          0.6               0.50       150      0.8893042  0.3038462  0.9913578
  0.4  2          0.6               0.75        50      0.9155030  0.2871795  0.9942363
  0.4  2          0.6               0.75       100      0.9108258  0.3051282  0.9913561
  0.4  2          0.6               0.75       150      0.9045418  0.3205128  0.9902050
  0.4  2          0.6               1.00        50      0.9225767  0.2884615  0.9959654
  0.4  2          0.6               1.00       100      0.9264141  0.3666667  0.9953890
  0.4  2          0.6               1.00       150      0.9225427  0.3512821  0.9925089
  0.4  2          0.8               0.50        50      0.8924376  0.2884615  0.9959671
  0.4  2          0.8               0.50       100      0.9018199  0.3025641  0.9936616
  0.4  2          0.8               0.50       150      0.8934124  0.2871795  0.9907798
  0.4  2          0.8               0.75        50      0.9155302  0.2717949  0.9976945
  0.4  2          0.8               0.75       100      0.9137802  0.3051282  0.9907798
  0.4  2          0.8               0.75       150      0.9066722  0.3371795  0.9884759
  0.4  2          0.8               1.00        50      0.9249545  0.2705128  0.9959654
  0.4  2          0.8               1.00       100      0.9230633  0.3346154  0.9936616
  0.4  2          0.8               1.00       150      0.9187101  0.3512821  0.9913561
  0.4  3          0.6               0.50        50      0.8856324  0.3038462  0.9930852
  0.4  3          0.6               0.50       100      0.8828110  0.2871795  0.9925089
  0.4  3          0.6               0.50       150      0.8793430  0.3025641  0.9925072
  0.4  3          0.6               0.75        50      0.9047910  0.3064103  0.9959671
  0.4  3          0.6               0.75       100      0.8954957  0.3051282  0.9953907
  0.4  3          0.6               0.75       150      0.8967009  0.3217949  0.9930852
  0.4  3          0.6               1.00        50      0.9221484  0.2705128  0.9965418
  0.4  3          0.6               1.00       100      0.9254909  0.3346154  0.9942380
  0.4  3          0.6               1.00       150      0.9130530  0.2884615  0.9930852
  0.4  3          0.8               0.50        50      0.8816897  0.3051282  0.9930852
  0.4  3          0.8               0.50       100      0.8853338  0.3358974  0.9930852
  0.4  3          0.8               0.50       150      0.8848693  0.3217949  0.9925105
  0.4  3          0.8               0.75        50      0.9028462  0.3692308  0.9942396
  0.4  3          0.8               0.75       100      0.8983878  0.3525641  0.9925089
  0.4  3          0.8               0.75       150      0.8957334  0.3525641  0.9919325
  0.4  3          0.8               1.00        50      0.9156649  0.2871795  0.9971182
  0.4  3          0.8               1.00       100      0.9070868  0.3179487  0.9925105
  0.4  3          0.8               1.00       150      0.9014637  0.3346154  0.9936616

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 100, max_depth = 2, eta = 0.3, gamma = 0, colsample_bytree = 0.6, min_child_weight = 1 and subsample = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative      0.9      0.4
  positive      2.6     96.1
                          
 Accuracy (average) : 0.97

[1] "TRAIN accuracy: 0.969966629588432"
[1] "TRAIN +precision: 0.974069898534386"
[1] "TRAIN -precision: 0.666666666666667"
[1] "TRAIN specifity: 0.258064516129032"
[1] "TRAIN sensitivity: 0.995391705069124"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        7        5
            positive       13      575
[1] "TEST accuracy: 0.97"
[1] "TEST +precision: 0.977891156462585"
[1] "TEST -precision: 0.583333333333333"
[1] "TEST specifity: 0.35"
[1] "TEST sensitivity: 0.991379310344828"

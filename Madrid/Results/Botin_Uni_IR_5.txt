[1] "DATASET NAME: Botin_Uni_IR_5"
[1] "TRAIN INSTANCES: 1953"
[1] "TEST INSTANCES: 573"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 16.2854430675507"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

1953 samples
 638 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1562, 1563, 1562, 1562, 1563 
Resampling results:

  ROC        Sens       Spec     
  0.9730718  0.9006337  0.9634483

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     23.2      2.7
  positive      2.6     71.5
                            
 Accuracy (average) : 0.9473

[1] "TRAIN accuracy: 0.94726062467998"
[1] "TRAIN +precision: 0.965445749827229"
[1] "TRAIN -precision: 0.895256916996047"
[1] "TRAIN specifity: 0.900596421471173"
[1] "TRAIN sensitivity: 0.963448275862069"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       46       20
            positive       30      477
[1] "TEST accuracy: 0.912739965095986"
[1] "TEST +precision: 0.940828402366864"
[1] "TEST -precision: 0.696969696969697"
[1] "TEST specifity: 0.605263157894737"
[1] "TEST sensitivity: 0.959758551307847"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 2.56173586845398"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

1953 samples
 638 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1562, 1562, 1562, 1563, 1563 
Resampling results across tuning parameters:

  C      M  ROC        Sens       Spec     
  0.010  1  0.8273719  0.7038614  0.9537931
  0.010  2  0.8374472  0.7018812  0.9537931
  0.010  3  0.8310835  0.6839802  0.9517241
  0.255  1  0.8690647  0.7933465  0.9372414
  0.255  2  0.8821946  0.7754653  0.9406897
  0.255  3  0.8786018  0.7575644  0.9351724
  0.500  1  0.8814804  0.8052673  0.9324138
  0.500  2  0.8935505  0.7854257  0.9372414
  0.500  3  0.8846489  0.7615446  0.9344828

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.5 and M = 2.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     20.2      4.7
  positive      5.5     69.6
                            
 Accuracy (average) : 0.8981

[1] "TRAIN accuracy: 0.89810547875064"
[1] "TRAIN +precision: 0.926380368098159"
[1] "TRAIN -precision: 0.812757201646091"
[1] "TRAIN specifity: 0.785288270377734"
[1] "TRAIN sensitivity: 0.937241379310345"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       44       25
            positive       32      472
[1] "TEST accuracy: 0.900523560209424"
[1] "TEST +precision: 0.936507936507937"
[1] "TEST -precision: 0.63768115942029"
[1] "TEST specifity: 0.578947368421053"
[1] "TEST sensitivity: 0.949698189134809"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 6.31929401556651"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

1953 samples
 638 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1563, 1562, 1562, 1562, 1563 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens       Spec     
  0.3  1          0.6               0.50        50      0.9569887  0.6042772  0.9875862
  0.3  1          0.6               0.50       100      0.9722200  0.7596040  0.9875862
  0.3  1          0.6               0.50       150      0.9771089  0.8052871  0.9827586
  0.3  1          0.6               0.75        50      0.9587405  0.6024752  0.9875862
  0.3  1          0.6               0.75       100      0.9712187  0.7277624  0.9862069
  0.3  1          0.6               0.75       150      0.9769327  0.7953465  0.9841379
  0.3  1          0.6               1.00        50      0.9580030  0.5885347  0.9889655
  0.3  1          0.6               1.00       100      0.9713382  0.6959406  0.9875862
  0.3  1          0.6               1.00       150      0.9763788  0.7655050  0.9827586
  0.3  1          0.8               0.50        50      0.9571391  0.6044158  0.9834483
  0.3  1          0.8               0.50       100      0.9703156  0.7397426  0.9820690
  0.3  1          0.8               0.50       150      0.9746144  0.8092673  0.9786207
  0.3  1          0.8               0.75        50      0.9592668  0.6103564  0.9889655
  0.3  1          0.8               0.75       100      0.9717887  0.7515446  0.9841379
  0.3  1          0.8               0.75       150      0.9764311  0.8052475  0.9793103
  0.3  1          0.8               1.00        50      0.9611013  0.5925545  0.9896552
  0.3  1          0.8               1.00       100      0.9706882  0.6999208  0.9868966
  0.3  1          0.8               1.00       150      0.9755655  0.7675050  0.9834483
  0.3  2          0.6               0.50        50      0.9706960  0.7675446  0.9855172
  0.3  2          0.6               0.50       100      0.9777588  0.8410693  0.9786207
  0.3  2          0.6               0.50       150      0.9809439  0.8708713  0.9779310
  0.3  2          0.6               0.75        50      0.9731087  0.7794653  0.9862069
  0.3  2          0.6               0.75       100      0.9794927  0.8489307  0.9820690
  0.3  2          0.6               0.75       150      0.9828014  0.8867525  0.9793103
  0.3  2          0.6               1.00        50      0.9732679  0.7515248  0.9841379
  0.3  2          0.6               1.00       100      0.9810668  0.8370693  0.9841379
  0.3  2          0.6               1.00       150      0.9835198  0.8609109  0.9806897
  0.3  2          0.8               0.50        50      0.9703875  0.7854059  0.9758621
  0.3  2          0.8               0.50       100      0.9778957  0.8390495  0.9800000
  0.3  2          0.8               0.50       150      0.9803681  0.8768515  0.9786207
  0.3  2          0.8               0.75        50      0.9752841  0.7834257  0.9820690
  0.3  2          0.8               0.75       100      0.9791089  0.8568911  0.9820690
  0.3  2          0.8               0.75       150      0.9805854  0.8787921  0.9793103
  0.3  2          0.8               1.00        50      0.9746005  0.7435644  0.9875862
  0.3  2          0.8               1.00       100      0.9811911  0.8410099  0.9813793
  0.3  2          0.8               1.00       150      0.9840949  0.8688911  0.9827586
  0.3  3          0.6               0.50        50      0.9762435  0.8053267  0.9806897
  0.3  3          0.6               0.50       100      0.9814291  0.8767921  0.9800000
  0.3  3          0.6               0.50       150      0.9831112  0.8927525  0.9793103
  0.3  3          0.6               0.75        50      0.9775773  0.8330297  0.9786207
  0.3  3          0.6               0.75       100      0.9825629  0.8827525  0.9827586
  0.3  3          0.6               0.75       150      0.9845422  0.8966931  0.9786207
  0.3  3          0.6               1.00        50      0.9769885  0.8211485  0.9820690
  0.3  3          0.6               1.00       100      0.9824594  0.8728713  0.9806897
  0.3  3          0.6               1.00       150      0.9845405  0.8907525  0.9827586
  0.3  3          0.8               0.50        50      0.9770886  0.8031881  0.9806897
  0.3  3          0.8               0.50       100      0.9805691  0.8748317  0.9744828
  0.3  3          0.8               0.50       150      0.9794280  0.8967327  0.9731034
  0.3  3          0.8               0.75        50      0.9796389  0.8211089  0.9813793
  0.3  3          0.8               0.75       100      0.9834218  0.8847327  0.9834483
  0.3  3          0.8               0.75       150      0.9843940  0.8967129  0.9813793
  0.3  3          0.8               1.00        50      0.9760284  0.8271287  0.9875862
  0.3  3          0.8               1.00       100      0.9829906  0.8747921  0.9848276
  0.3  3          0.8               1.00       150      0.9849482  0.8887327  0.9820690
  0.4  1          0.6               0.50        50      0.9633060  0.6721980  0.9855172
  0.4  1          0.6               0.50       100      0.9727305  0.7873465  0.9779310
  0.4  1          0.6               0.50       150      0.9769795  0.8291485  0.9779310
  0.4  1          0.6               0.75        50      0.9592899  0.6799208  0.9841379
  0.4  1          0.6               0.75       100      0.9733682  0.7913861  0.9806897
  0.4  1          0.6               0.75       150      0.9783400  0.8271485  0.9806897
  0.4  1          0.6               1.00        50      0.9645727  0.6561386  0.9896552
  0.4  1          0.6               1.00       100      0.9747495  0.7635842  0.9834483
  0.4  1          0.6               1.00       150      0.9797271  0.8112673  0.9827586
  0.4  1          0.8               0.50        50      0.9636499  0.6959208  0.9827586
  0.4  1          0.8               0.50       100      0.9736614  0.7813861  0.9855172
  0.4  1          0.8               0.50       150      0.9759955  0.8072475  0.9758621
  0.4  1          0.8               0.75        50      0.9610042  0.6820000  0.9834483
  0.4  1          0.8               0.75       100      0.9731873  0.7834455  0.9786207
  0.4  1          0.8               0.75       150      0.9779922  0.8390495  0.9779310
  0.4  1          0.8               1.00        50      0.9649491  0.6541188  0.9875862
  0.4  1          0.8               1.00       100      0.9751570  0.7535842  0.9841379
  0.4  1          0.8               1.00       150      0.9793772  0.8132079  0.9800000
  0.4  2          0.6               0.50        50      0.9713199  0.8092475  0.9779310
  0.4  2          0.6               0.50       100      0.9785000  0.8490099  0.9772414
  0.4  2          0.6               0.50       150      0.9797885  0.8708911  0.9724138
  0.4  2          0.6               0.75        50      0.9743271  0.7993069  0.9820690
  0.4  2          0.6               0.75       100      0.9806535  0.8727525  0.9779310
  0.4  2          0.6               0.75       150      0.9842565  0.8907723  0.9820690
  0.4  2          0.6               1.00        50      0.9742393  0.8013663  0.9841379
  0.4  2          0.6               1.00       100      0.9816156  0.8510297  0.9806897
  0.4  2          0.6               1.00       150      0.9841461  0.8827921  0.9834483
  0.4  2          0.8               0.50        50      0.9727645  0.8072475  0.9800000
  0.4  2          0.8               0.50       100      0.9763442  0.8609109  0.9751724
  0.4  2          0.8               0.50       150      0.9797394  0.8768713  0.9717241
  0.4  2          0.8               0.75        50      0.9727407  0.8052475  0.9806897
  0.4  2          0.8               0.75       100      0.9813086  0.8648911  0.9772414
  0.4  2          0.8               0.75       150      0.9826524  0.8987129  0.9786207
  0.4  2          0.8               1.00        50      0.9775531  0.7913069  0.9834483
  0.4  2          0.8               1.00       100      0.9832136  0.8648317  0.9813793
  0.4  2          0.8               1.00       150      0.9852032  0.8907327  0.9848276
  0.4  3          0.6               0.50        50      0.9783934  0.8529307  0.9806897
  0.4  3          0.6               0.50       100      0.9833078  0.8847723  0.9772414
  0.4  3          0.6               0.50       150      0.9816168  0.8888119  0.9751724
  0.4  3          0.6               0.75        50      0.9784003  0.8350891  0.9779310
  0.4  3          0.6               0.75       100      0.9845000  0.8967525  0.9806897
  0.4  3          0.6               0.75       150      0.9849063  0.9027129  0.9793103
  0.4  3          0.6               1.00        50      0.9797103  0.8549703  0.9813793
  0.4  3          0.6               1.00       100      0.9842445  0.8967129  0.9800000
  0.4  3          0.6               1.00       150      0.9846334  0.9086337  0.9786207
  0.4  3          0.8               0.50        50      0.9783660  0.8509307  0.9786207
  0.4  3          0.8               0.50       100      0.9802675  0.8888119  0.9772414
  0.4  3          0.8               0.50       150      0.9799984  0.8947525  0.9765517
  0.4  3          0.8               0.75        50      0.9767468  0.8430495  0.9806897
  0.4  3          0.8               0.75       100      0.9824283  0.8927723  0.9800000
  0.4  3          0.8               0.75       150      0.9828332  0.9066535  0.9779310
  0.4  3          0.8               1.00        50      0.9790609  0.8549109  0.9834483
  0.4  3          0.8               1.00       100      0.9835410  0.8867723  0.9827586
  0.4  3          0.8               1.00       150      0.9842771  0.9006733  0.9806897

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 150, max_depth = 2, eta = 0.4, gamma = 0, colsample_bytree = 0.8, min_child_weight = 1 and subsample = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     22.9      1.1
  positive      2.8     73.1
                            
 Accuracy (average) : 0.9606

[1] "TRAIN accuracy: 0.960573476702509"
[1] "TRAIN +precision: 0.962913014160485"
[1] "TRAIN -precision: 0.953191489361702"
[1] "TRAIN specifity: 0.89065606361829"
[1] "TRAIN sensitivity: 0.984827586206897"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       52       14
            positive       24      483
[1] "TEST accuracy: 0.933682373472949"
[1] "TEST +precision: 0.952662721893491"
[1] "TEST -precision: 0.787878787878788"
[1] "TEST specifity: 0.684210526315789"
[1] "TEST sensitivity: 0.971830985915493"

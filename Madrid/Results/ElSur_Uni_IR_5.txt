[1] "DATASET NAME: ElSur_Uni_IR_5"
[1] "TRAIN INSTANCES: 1414"
[1] "TEST INSTANCES: 396"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 3.64973592758179"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

1414 samples
 684 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1132, 1132, 1130, 1131, 1131 
Resampling results:

  ROC  Sens  Spec     
  1    1     0.9991379

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     17.8      0.1
  positive      0.0     82.1
                            
 Accuracy (average) : 0.9993

[1] "TRAIN accuracy: 0.999292786421499"
[1] "TRAIN +precision: 1"
[1] "TRAIN -precision: 0.99604743083004"
[1] "TRAIN specifity: 1"
[1] "TRAIN sensitivity: 0.999139414802065"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        1        0
            positive        4      391
[1] "TEST accuracy: 0.98989898989899"
[1] "TEST +precision: 0.989873417721519"
[1] "TEST -precision: 1"
[1] "TEST specifity: 0.2"
[1] "TEST sensitivity: 1"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 1.74014846881231"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

1414 samples
 684 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1132, 1131, 1130, 1132, 1131 
Resampling results across tuning parameters:

  C      M  ROC        Sens       Spec     
  0.010  1  0.9862931  0.9882353  0.9784779
  0.010  2  0.9861638  0.9882353  0.9784779
  0.010  3  0.9883752  0.9882353  0.9793473
  0.255  1  0.9925275  1.0000000  0.9819188
  0.255  2  0.9923982  1.0000000  0.9819188
  0.255  3  0.9934032  1.0000000  0.9844976
  0.500  1  0.9925275  1.0000000  0.9819188
  0.500  2  0.9923982  1.0000000  0.9819188
  0.500  3  0.9934032  1.0000000  0.9844976

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.255 and M = 3.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     17.8      1.3
  positive      0.0     80.9
                            
 Accuracy (average) : 0.9873

[1] "TRAIN accuracy: 0.987270155586987"
[1] "TRAIN +precision: 1"
[1] "TRAIN -precision: 0.933333333333333"
[1] "TRAIN specifity: 1"
[1] "TRAIN sensitivity: 0.984509466437177"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        1       12
            positive        4      379
[1] "TEST accuracy: 0.95959595959596"
[1] "TEST +precision: 0.989556135770235"
[1] "TEST -precision: 0.0769230769230769"
[1] "TEST specifity: 0.2"
[1] "TEST sensitivity: 0.969309462915601"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 4.93946323394775"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

1414 samples
 684 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1131, 1132, 1130, 1131, 1132 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens       Spec     
  0.3  1          0.6               0.50        50      0.9964060  0.8894118  0.9896663
  0.3  1          0.6               0.50       100      0.9982987  0.9843137  0.9922488
  0.3  1          0.6               0.50       150      0.9983686  1.0000000  0.9922488
  0.3  1          0.6               0.75        50      0.9971219  0.9408627  0.9922488
  0.3  1          0.6               0.75       100      0.9985418  0.9843137  0.9922451
  0.3  1          0.6               0.75       150      0.9986573  1.0000000  0.9905283
  0.3  1          0.6               1.00        50      0.9965419  0.9248627  0.9922451
  0.3  1          0.6               1.00       100      0.9986947  1.0000000  0.9913867
  0.3  1          0.6               1.00       150      0.9987947  1.0000000  0.9931108
  0.3  1          0.8               0.50        50      0.9958941  0.9247843  0.9913867
  0.3  1          0.8               0.50       100      0.9979056  0.9843137  0.9913867
  0.3  1          0.8               0.50       150      0.9982464  1.0000000  0.9913904
  0.3  1          0.8               0.75        50      0.9968315  0.9289412  0.9922488
  0.3  1          0.8               0.75       100      0.9983355  0.9843137  0.9913867
  0.3  1          0.8               0.75       150      0.9984344  1.0000000  0.9922525
  0.3  1          0.8               1.00        50      0.9972291  0.9092549  0.9905209
  0.3  1          0.8               1.00       100      0.9986432  1.0000000  0.9922451
  0.3  1          0.8               1.00       150      0.9988119  1.0000000  0.9913867
  0.3  2          0.6               0.50        50      0.9987112  1.0000000  0.9922488
  0.3  2          0.6               0.50       100      0.9983494  1.0000000  0.9905320
  0.3  2          0.6               0.50       150      0.9982968  1.0000000  0.9905320
  0.3  2          0.6               0.75        50      0.9984726  1.0000000  0.9922488
  0.3  2          0.6               0.75       100      0.9984513  1.0000000  0.9948387
  0.3  2          0.6               0.75       150      0.9984342  1.0000000  0.9931182
  0.3  2          0.6               1.00        50      0.9992962  1.0000000  0.9913867
  0.3  2          0.6               1.00       100      0.9987947  1.0000000  0.9939729
  0.3  2          0.6               1.00       150      0.9983827  1.0000000  0.9931182
  0.3  2          0.8               0.50        50      0.9984932  0.9843137  0.9922525
  0.3  2          0.8               0.50       100      0.9983314  1.0000000  0.9948350
  0.3  2          0.8               0.50       150      0.9983483  1.0000000  0.9922562
  0.3  2          0.8               0.75        50      0.9982990  1.0000000  0.9922525
  0.3  2          0.8               0.75       100      0.9984342  1.0000000  0.9939729
  0.3  2          0.8               0.75       150      0.9983827  1.0000000  0.9956971
  0.3  2          0.8               1.00        50      0.9989016  1.0000000  0.9931108
  0.3  2          0.8               1.00       100      0.9986926  1.0000000  0.9931108
  0.3  2          0.8               1.00       150      0.9983827  1.0000000  0.9939803
  0.3  3          0.6               0.50        50      0.9987312  0.9843137  0.9931108
  0.3  3          0.6               0.50       100      0.9983666  1.0000000  0.9931145
  0.3  3          0.6               0.50       150      0.9984342  1.0000000  0.9939766
  0.3  3          0.6               0.75        50      0.9997247  1.0000000  0.9965554
  0.3  3          0.6               0.75       100      0.9992259  1.0000000  0.9948350
  0.3  3          0.6               0.75       150      0.9986921  1.0000000  0.9931145
  0.3  3          0.6               1.00        50      0.9996218  1.0000000  0.9956971
  0.3  3          0.6               1.00       100      0.9992611  1.0000000  0.9956971
  0.3  3          0.6               1.00       150      0.9988825  1.0000000  0.9948350
  0.3  3          0.8               0.50        50      0.9993305  1.0000000  0.9948313
  0.3  3          0.8               0.50       100      0.9992101  1.0000000  0.9922525
  0.3  3          0.8               0.50       150      0.9991411  1.0000000  0.9939729
  0.3  3          0.8               0.75        50      0.9993624  1.0000000  0.9956934
  0.3  3          0.8               0.75       100      0.9992440  1.0000000  0.9956934
  0.3  3          0.8               0.75       150      0.9988825  1.0000000  0.9956934
  0.3  3          0.8               1.00        50      0.9993143  1.0000000  0.9948350
  0.3  3          0.8               1.00       100      0.9992618  1.0000000  0.9948350
  0.3  3          0.8               1.00       150      0.9992618  1.0000000  0.9948350
  0.4  1          0.6               0.50        50      0.9968496  0.9723137  0.9913867
  0.4  1          0.6               0.50       100      0.9983333  1.0000000  0.9905283
  0.4  1          0.6               0.50       150      0.9979693  1.0000000  0.9896737
  0.4  1          0.6               0.75        50      0.9977696  0.9723137  0.9888005
  0.4  1          0.6               0.75       100      0.9988131  1.0000000  0.9922488
  0.4  1          0.6               0.75       150      0.9987099  1.0000000  0.9939729
  0.4  1          0.6               1.00        50      0.9972404  0.9643137  0.9913867
  0.4  1          0.6               1.00       100      0.9987785  1.0000000  0.9922488
  0.4  1          0.6               1.00       150      0.9987088  1.0000000  0.9922488
  0.4  1          0.8               0.50        50      0.9974820  0.9803137  0.9888079
  0.4  1          0.8               0.50       100      0.9985578  1.0000000  0.9896626
  0.4  1          0.8               0.50       150      0.9983312  1.0000000  0.9913904
  0.4  1          0.8               0.75        50      0.9970643  0.9840000  0.9905246
  0.4  1          0.8               0.75       100      0.9983827  1.0000000  0.9913904
  0.4  1          0.8               0.75       150      0.9981938  1.0000000  0.9922488
  0.4  1          0.8               1.00        50      0.9975001  0.9644706  0.9879458
  0.4  1          0.8               1.00       100      0.9987959  1.0000000  0.9913904
  0.4  1          0.8               1.00       150      0.9987603  1.0000000  0.9922488
  0.4  2          0.6               0.50        50      0.9989330  1.0000000  0.9922525
  0.4  2          0.6               0.50       100      0.9984856  1.0000000  0.9913941
  0.4  2          0.6               0.50       150      0.9983997  1.0000000  0.9913941
  0.4  2          0.6               0.75        50      0.9986417  1.0000000  0.9931108
  0.4  2          0.6               0.75       100      0.9983827  1.0000000  0.9922599
  0.4  2          0.6               0.75       150      0.9983827  1.0000000  0.9913978
  0.4  2          0.6               1.00        50      0.9989013  1.0000000  0.9931108
  0.4  2          0.6               1.00       100      0.9983827  1.0000000  0.9896737
  0.4  2          0.6               1.00       150      0.9983830  1.0000000  0.9888153
  0.4  2          0.8               0.50        50      0.9983348  0.9843137  0.9931145
  0.4  2          0.8               0.50       100      0.9983827  1.0000000  0.9888153
  0.4  2          0.8               0.50       150      0.9982968  1.0000000  0.9888190
  0.4  2          0.8               0.75        50      0.9989160  1.0000000  0.9931108
  0.4  2          0.8               0.75       100      0.9983312  1.0000000  0.9905320
  0.4  2          0.8               0.75       150      0.9983827  1.0000000  0.9913941
  0.4  2          0.8               1.00        50      0.9994001  1.0000000  0.9939766
  0.4  2          0.8               1.00       100      0.9985204  1.0000000  0.9939766
  0.4  2          0.8               1.00       150      0.9985719  1.0000000  0.9948424
  0.4  3          0.6               0.50        50      0.9983689  1.0000000  0.9931145
  0.4  3          0.6               0.50       100      0.9982968  1.0000000  0.9913941
  0.4  3          0.6               0.50       150      0.9982968  1.0000000  0.9922525
  0.4  3          0.6               0.75        50      0.9991754  1.0000000  0.9956934
  0.4  3          0.6               0.75       100      0.9990724  1.0000000  0.9922488
  0.4  3          0.6               0.75       150      0.9990552  1.0000000  0.9913904
  0.4  3          0.6               1.00        50      1.0000000  1.0000000  0.9974175
  0.4  3          0.6               1.00       100      0.9995021  1.0000000  0.9956971
  0.4  3          0.6               1.00       150      0.9994162  1.0000000  0.9956971
  0.4  3          0.8               0.50        50      0.9996915  1.0000000  0.9931145
  0.4  3          0.8               0.50       100      0.9995707  1.0000000  0.9931145
  0.4  3          0.8               0.50       150      0.9993817  1.0000000  0.9939729
  0.4  3          0.8               0.75        50      0.9992941  1.0000000  0.9939729
  0.4  3          0.8               0.75       100      0.9989166  1.0000000  0.9931145
  0.4  3          0.8               0.75       150      0.9986925  1.0000000  0.9931145
  0.4  3          0.8               1.00        50      0.9994153  1.0000000  0.9956971
  0.4  3          0.8               1.00       100      0.9990377  1.0000000  0.9939766
  0.4  3          0.8               1.00       150      0.9987618  1.0000000  0.9905320

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 50, max_depth = 3, eta = 0.4, gamma = 0, colsample_bytree = 0.6, min_child_weight = 1 and subsample = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     17.8      0.2
  positive      0.0     82.0
                            
 Accuracy (average) : 0.9979

[1] "TRAIN accuracy: 0.997878359264498"
[1] "TRAIN +precision: 1"
[1] "TRAIN -precision: 0.988235294117647"
[1] "TRAIN specifity: 1"
[1] "TRAIN sensitivity: 0.997418244406196"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        2        1
            positive        3      390
[1] "TEST accuracy: 0.98989898989899"
[1] "TEST +precision: 0.99236641221374"
[1] "TEST -precision: 0.666666666666667"
[1] "TEST specifity: 0.4"
[1] "TEST sensitivity: 0.997442455242967"

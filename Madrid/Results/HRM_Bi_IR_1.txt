[1] "DATASET NAME: HRM_Bi_IR_1"
[1] "TRAIN INSTANCES: 576"
[1] "TEST INSTANCES: 112"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 2.90856409072876"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

576 samples
964 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 460, 461, 460, 461, 462 
Resampling results:

  ROC        Sens       Spec
  0.9985647  0.9826981  1   

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     49.1      0.0
  positive      0.9     50.0
                            
 Accuracy (average) : 0.9913

[1] "TRAIN accuracy: 0.991319444444444"
[1] "TRAIN +precision: 0.982935153583618"
[1] "TRAIN -precision: 1"
[1] "TRAIN specifity: 0.982638888888889"
[1] "TRAIN sensitivity: 1"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        3        0
            positive       12       97
[1] "TEST accuracy: 0.892857142857143"
[1] "TEST +precision: 0.889908256880734"
[1] "TEST -precision: 1"
[1] "TEST specifity: 0.2"
[1] "TEST sensitivity: 1"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 1.65971358617147"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

576 samples
964 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 462, 461, 461, 460, 460 
Resampling results across tuning parameters:

  C      M  ROC        Sens       Spec     
  0.010  1  0.9280852  0.7879613  0.9479129
  0.010  2  0.9297467  0.7879613  0.9272232
  0.010  3  0.9160299  0.7671506  0.9307925
  0.255  1  0.9353978  0.8019964  0.9651543
  0.255  2  0.9357259  0.8019964  0.9444646
  0.255  3  0.9395338  0.8019964  0.9620690
  0.500  1  0.9353978  0.8019964  0.9651543
  0.500  2  0.9410326  0.8019964  0.9655172
  0.500  3  0.9395338  0.8019964  0.9620690

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.5 and M = 2.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     40.1      1.7
  positive      9.9     48.3
                            
 Accuracy (average) : 0.8837

[1] "TRAIN accuracy: 0.883680555555556"
[1] "TRAIN +precision: 0.829850746268657"
[1] "TRAIN -precision: 0.95850622406639"
[1] "TRAIN specifity: 0.802083333333333"
[1] "TRAIN sensitivity: 0.965277777777778"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        1        5
            positive       14       92
[1] "TEST accuracy: 0.830357142857143"
[1] "TEST +precision: 0.867924528301887"
[1] "TEST -precision: 0.166666666666667"
[1] "TEST specifity: 0.0666666666666667"
[1] "TEST sensitivity: 0.948453608247423"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 3.0935360511144"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

576 samples
964 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 461, 461, 462, 460, 460 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens       Spec     
  0.3  1          0.6               0.50        50      0.8774796  0.6600726  0.9583787
  0.3  1          0.6               0.50       100      0.9473052  0.8473079  0.9584997
  0.3  1          0.6               0.50       150      0.9637521  0.8715668  0.9618875
  0.3  1          0.6               0.75        50      0.9029391  0.7571083  0.9687840
  0.3  1          0.6               0.75       100      0.9475165  0.8715668  0.9652148
  0.3  1          0.6               0.75       150      0.9549988  0.8715668  0.9652148
  0.3  1          0.6               1.00        50      0.8987933  0.7640653  0.9582577
  0.3  1          0.6               1.00       100      0.9452122  0.8612220  0.9652753
  0.3  1          0.6               1.00       150      0.9561030  0.8784634  0.9618270
  0.3  1          0.8               0.50        50      0.8955327  0.6947368  0.9722928
  0.3  1          0.8               0.50       100      0.9593486  0.8368421  0.9688445
  0.3  1          0.8               0.50       150      0.9652224  0.8784634  0.9688445
  0.3  1          0.8               0.75        50      0.9024939  0.7503327  0.9687840
  0.3  1          0.8               0.75       100      0.9497141  0.8366606  0.9687840
  0.3  1          0.8               0.75       150      0.9614908  0.8715668  0.9653358
  0.3  1          0.8               1.00        50      0.8978880  0.7606171  0.9582577
  0.3  1          0.8               1.00       100      0.9534457  0.8612220  0.9652753
  0.3  1          0.8               1.00       150      0.9570735  0.8784634  0.9618270
  0.3  2          0.6               0.50        50      0.9552121  0.8437387  0.9445251
  0.3  2          0.6               0.50       100      0.9629674  0.8646703  0.9722928
  0.3  2          0.6               0.50       150      0.9661347  0.8784634  0.9653962
  0.3  2          0.6               0.75        50      0.9498130  0.8575318  0.9618270
  0.3  2          0.6               0.75       100      0.9684690  0.8784634  0.9653358
  0.3  2          0.6               0.75       150      0.9734520  0.8889897  0.9618270
  0.3  2          0.6               1.00        50      0.9461395  0.8543255  0.9722928
  0.3  2          0.6               1.00       100      0.9651316  0.8784634  0.9583182
  0.3  2          0.6               1.00       150      0.9770146  0.8784634  0.9583182
  0.3  2          0.8               0.50        50      0.9528227  0.8191168  0.9065941
  0.3  2          0.8               0.50       100      0.9723351  0.8506352  0.9722323
  0.3  2          0.8               0.50       150      0.9717422  0.8680581  0.9653962
  0.3  2          0.8               0.75        50      0.9504508  0.8610405  0.9652753
  0.3  2          0.8               0.75       100      0.9683913  0.8784634  0.9722928
  0.3  2          0.8               0.75       150      0.9691500  0.8889897  0.9688445
  0.3  2          0.8               1.00        50      0.9476157  0.8543255  0.9618270
  0.3  2          0.8               1.00       100      0.9639030  0.8784634  0.9618270
  0.3  2          0.8               1.00       150      0.9789072  0.8784634  0.9583182
  0.3  3          0.6               0.50        50      0.9540039  0.8575318  0.9687840
  0.3  3          0.6               0.50       100      0.9607638  0.8644283  0.9653962
  0.3  3          0.6               0.50       150      0.9600765  0.8749546  0.9584997
  0.3  3          0.6               0.75        50      0.9607916  0.8784634  0.9618270
  0.3  3          0.6               0.75       100      0.9789618  0.8784634  0.9618270
  0.3  3          0.6               0.75       150      0.9737747  0.9027828  0.9583787
  0.3  3          0.6               1.00        50      0.9616374  0.8784634  0.9652753
  0.3  3          0.6               1.00       100      0.9774391  0.8922565  0.9618270
  0.3  3          0.6               1.00       150      0.9758660  0.9027828  0.9514217
  0.3  3          0.8               0.50        50      0.9555493  0.8090744  0.9652753
  0.3  3          0.8               0.50       100      0.9698086  0.8749546  0.9653358
  0.3  3          0.8               0.50       150      0.9690983  0.8820932  0.9515426
  0.3  3          0.8               0.75        50      0.9614808  0.8784634  0.9722928
  0.3  3          0.8               0.75       100      0.9726692  0.8922565  0.9687840
  0.3  3          0.8               0.75       150      0.9728221  0.9027828  0.9583787
  0.3  3          0.8               1.00        50      0.9547851  0.8784634  0.9583787
  0.3  3          0.8               1.00       100      0.9796348  0.8784634  0.9618270
  0.3  3          0.8               1.00       150      0.9771720  0.9027828  0.9514217
  0.4  1          0.6               0.50        50      0.9140717  0.7362976  0.9549909
  0.4  1          0.6               0.50       100      0.9594519  0.8575318  0.9444646
  0.4  1          0.6               0.50       150      0.9651500  0.8715668  0.9687840
  0.4  1          0.6               0.75        50      0.9209795  0.7882033  0.9548699
  0.4  1          0.6               0.75       100      0.9497390  0.8715668  0.9687840
  0.4  1          0.6               0.75       150      0.9657371  0.8784634  0.9722928
  0.4  1          0.6               1.00        50      0.9177620  0.8163944  0.9548699
  0.4  1          0.6               1.00       100      0.9554448  0.8784634  0.9618270
  0.4  1          0.6               1.00       150      0.9658453  0.8784634  0.9618270
  0.4  1          0.8               0.50        50      0.9105546  0.7469449  0.9584392
  0.4  1          0.8               0.50       100      0.9549752  0.8540835  0.9549909
  0.4  1          0.8               0.50       150      0.9661559  0.8644283  0.9722928
  0.4  1          0.8               0.75        50      0.9204079  0.8059891  0.9583787
  0.4  1          0.8               0.75       100      0.9551293  0.8681186  0.9722928
  0.4  1          0.8               0.75       150      0.9732402  0.8784634  0.9688445
  0.4  1          0.8               1.00        50      0.9155346  0.8060496  0.9548094
  0.4  1          0.8               1.00       100      0.9547710  0.8784634  0.9618270
  0.4  1          0.8               1.00       150      0.9669197  0.8784634  0.9618270
  0.4  2          0.6               0.50        50      0.9528423  0.8506352  0.9722323
  0.4  2          0.6               0.50       100      0.9658523  0.8435572  0.9584997
  0.4  2          0.6               0.50       150      0.9647075  0.8644283  0.9653962
  0.4  2          0.6               0.75        50      0.9640738  0.8612220  0.9687235
  0.4  2          0.6               0.75       100      0.9724998  0.8889897  0.9687840
  0.4  2          0.6               0.75       150      0.9734952  0.8889897  0.9583787
  0.4  2          0.6               1.00        50      0.9612644  0.8784634  0.9583787
  0.4  2          0.6               1.00       100      0.9761987  0.8922565  0.9618270
  0.4  2          0.6               1.00       150      0.9762993  0.8922565  0.9548699
  0.4  2          0.8               0.50        50      0.9543387  0.8230490  0.9652753
  0.4  2          0.8               0.50       100      0.9680526  0.8748336  0.9653358
  0.4  2          0.8               0.50       150      0.9677242  0.8853600  0.9549304
  0.4  2          0.8               0.75        50      0.9561289  0.8784634  0.9617665
  0.4  2          0.8               0.75       100      0.9761906  0.8784634  0.9618875
  0.4  2          0.8               0.75       150      0.9706243  0.8889897  0.9514822
  0.4  2          0.8               1.00        50      0.9576320  0.8715668  0.9583787
  0.4  2          0.8               1.00       100      0.9764027  0.8922565  0.9583787
  0.4  2          0.8               1.00       150      0.9766059  0.9027828  0.9548699
  0.4  3          0.6               0.50        50      0.9649015  0.8435572  0.9653358
  0.4  3          0.6               0.50       100      0.9664085  0.8680581  0.9584392
  0.4  3          0.6               0.50       150      0.9666481  0.8749546  0.9584392
  0.4  3          0.6               0.75        50      0.9670688  0.8784634  0.9688445
  0.4  3          0.6               0.75       100      0.9703657  0.8715668  0.9653358
  0.4  3          0.6               0.75       150      0.9711580  0.8820932  0.9653962
  0.4  3          0.6               1.00        50      0.9687629  0.8784634  0.9688445
  0.4  3          0.6               1.00       100      0.9745240  0.9027828  0.9653358
  0.4  3          0.6               1.00       150      0.9744851  0.9027828  0.9549304
  0.4  3          0.8               0.50        50      0.9534342  0.8333938  0.9687840
  0.4  3          0.8               0.50       100      0.9714398  0.8680581  0.9618875
  0.4  3          0.8               0.50       150      0.9701475  0.8680581  0.9514822
  0.4  3          0.8               0.75        50      0.9745771  0.8784634  0.9687840
  0.4  3          0.8               0.75       100      0.9715517  0.9027828  0.9688445
  0.4  3          0.8               0.75       150      0.9730115  0.9027828  0.9618875
  0.4  3          0.8               1.00        50      0.9694818  0.8784634  0.9618270
  0.4  3          0.8               1.00       100      0.9787047  0.9027828  0.9583182
  0.4  3          0.8               1.00       150      0.9780012  0.9027828  0.9548699

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 100, max_depth = 3, eta = 0.3, gamma = 0, colsample_bytree = 0.8, min_child_weight = 1 and subsample = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     43.9      1.9
  positive      6.1     48.1
                            
 Accuracy (average) : 0.9201

[1] "TRAIN accuracy: 0.920138888888889"
[1] "TRAIN +precision: 0.887820512820513"
[1] "TRAIN -precision: 0.958333333333333"
[1] "TRAIN specifity: 0.878472222222222"
[1] "TRAIN sensitivity: 0.961805555555556"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        4        4
            positive       11       93
[1] "TEST accuracy: 0.866071428571429"
[1] "TEST +precision: 0.894230769230769"
[1] "TEST -precision: 0.5"
[1] "TEST specifity: 0.266666666666667"
[1] "TEST sensitivity: 0.958762886597938"

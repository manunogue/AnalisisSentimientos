[1] "DATASET NAME: MSM_Uni_IR_5"
[1] "TRAIN INSTANCES: 2133"
[1] "TEST INSTANCES: 600"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 10.1045942306519"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

2133 samples
 656 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1707, 1706, 1707, 1707, 1705 
Resampling results:

  ROC        Sens       Spec     
  0.9988473  0.9924051  0.9942396

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     18.5      0.5
  positive      0.1     80.9
                            
 Accuracy (average) : 0.9939

[1] "TRAIN accuracy: 0.993905297702766"
[1] "TRAIN +precision: 0.998264893001735"
[1] "TRAIN -precision: 0.975247524752475"
[1] "TRAIN specifity: 0.992443324937028"
[1] "TRAIN sensitivity: 0.994239631336406"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        8        5
            positive       12      575
[1] "TEST accuracy: 0.971666666666667"
[1] "TEST +precision: 0.979557069846678"
[1] "TEST -precision: 0.615384615384615"
[1] "TEST specifity: 0.4"
[1] "TEST sensitivity: 0.991379310344828"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 2.55012883345286"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

2133 samples
 656 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1705, 1707, 1707, 1706, 1707 
Resampling results across tuning parameters:

  C      M  ROC        Sens       Spec     
  0.010  1  0.9110673  0.8335759  0.9694707
  0.010  2  0.9108380  0.8335759  0.9677416
  0.010  3  0.9078217  0.8210127  0.9677416
  0.255  1  0.9697648  0.9469304  0.9723525
  0.255  2  0.9698279  0.9443987  0.9729289
  0.255  3  0.9676076  0.9293354  0.9729272
  0.500  1  0.9786521  0.9697152  0.9717761
  0.500  2  0.9800895  0.9697152  0.9723525
  0.500  3  0.9742661  0.9444937  0.9717761

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.5 and M = 2.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     18.0      2.3
  positive      0.6     79.1
                            
 Accuracy (average) : 0.9719

[1] "TRAIN accuracy: 0.971870604781997"
[1] "TRAIN +precision: 0.992941176470588"
[1] "TRAIN -precision: 0.889145496535797"
[1] "TRAIN specifity: 0.969773299748111"
[1] "TRAIN sensitivity: 0.972350230414747"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        7       15
            positive       13      565
[1] "TEST accuracy: 0.953333333333333"
[1] "TEST +precision: 0.977508650519031"
[1] "TEST -precision: 0.318181818181818"
[1] "TEST specifity: 0.35"
[1] "TEST sensitivity: 0.974137931034483"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 6.51451460123062"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

2133 samples
 656 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1706, 1707, 1707, 1706, 1706 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens       Spec     
  0.3  1          0.6               0.50        50      0.9693101  0.6471835  0.9867518
  0.3  1          0.6               0.50       100      0.9854597  0.8464557  0.9873249
  0.3  1          0.6               0.50       150      0.9898606  0.9142405  0.9861721
  0.3  1          0.6               0.75        50      0.9729423  0.6249051  0.9913611
  0.3  1          0.6               0.75       100      0.9859757  0.8338608  0.9884759
  0.3  1          0.6               0.75       150      0.9899666  0.8891772  0.9873232
  0.3  1          0.6               1.00        50      0.9739302  0.6197785  0.9930885
  0.3  1          0.6               1.00       100      0.9856488  0.8212658  0.9890540
  0.3  1          0.6               1.00       150      0.9901746  0.8691456  0.9890540
  0.3  1          0.8               0.50        50      0.9752703  0.6425949  0.9913578
  0.3  1          0.8               0.50       100      0.9854840  0.8538608  0.9861721
  0.3  1          0.8               0.50       150      0.9888228  0.9168354  0.9850194
  0.3  1          0.8               0.75        50      0.9741414  0.6500633  0.9902050
  0.3  1          0.8               0.75       100      0.9856399  0.8337025  0.9867485
  0.3  1          0.8               0.75       150      0.9894867  0.9294304  0.9861721
  0.3  1          0.8               1.00        50      0.9752768  0.6248101  0.9919358
  0.3  1          0.8               1.00       100      0.9858004  0.8112658  0.9884776
  0.3  1          0.8               1.00       150      0.9904198  0.8766139  0.9884776
  0.3  2          0.6               0.50        50      0.9876720  0.8941456  0.9879029
  0.3  2          0.6               0.50       100      0.9933018  0.9496203  0.9884759
  0.3  2          0.6               0.50       150      0.9960503  0.9696519  0.9873232
  0.3  2          0.6               0.75        50      0.9911199  0.8714557  0.9913578
  0.3  2          0.6               0.75       100      0.9950424  0.9596519  0.9907814
  0.3  2          0.6               0.75       150      0.9967246  0.9747152  0.9884759
  0.3  2          0.6               1.00        50      0.9904381  0.8690190  0.9936616
  0.3  2          0.6               1.00       100      0.9957339  0.9545886  0.9919341
  0.3  2          0.6               1.00       150      0.9974961  0.9672152  0.9902067
  0.3  2          0.8               0.50        50      0.9894111  0.8940823  0.9878996
  0.3  2          0.8               0.50       100      0.9933364  0.9495886  0.9873232
  0.3  2          0.8               0.50       150      0.9959598  0.9747152  0.9884743
  0.3  2          0.8               0.75        50      0.9886424  0.8817089  0.9896303
  0.3  2          0.8               0.75       100      0.9949645  0.9546203  0.9896287
  0.3  2          0.8               0.75       150      0.9969445  0.9797468  0.9873232
  0.3  2          0.8               1.00        50      0.9909195  0.8891456  0.9913594
  0.3  2          0.8               1.00       100      0.9956962  0.9546203  0.9896287
  0.3  2          0.8               1.00       150      0.9970511  0.9696835  0.9902034
  0.3  3          0.6               0.50        50      0.9931716  0.9319937  0.9878996
  0.3  3          0.6               0.50       100      0.9979727  0.9722468  0.9873232
  0.3  3          0.6               0.50       150      0.9982988  0.9797468  0.9884759
  0.3  3          0.6               0.75        50      0.9950694  0.9495886  0.9942380
  0.3  3          0.6               0.75       100      0.9982932  0.9797468  0.9936616
  0.3  3          0.6               0.75       150      0.9993053  0.9797468  0.9913561
  0.3  3          0.6               1.00        50      0.9952518  0.9445886  0.9919341
  0.3  3          0.6               1.00       100      0.9985176  0.9772468  0.9907814
  0.3  3          0.6               1.00       150      0.9993324  0.9797468  0.9896287
  0.3  3          0.8               0.50        50      0.9950568  0.9471203  0.9907814
  0.3  3          0.8               0.50       100      0.9979839  0.9772468  0.9896303
  0.3  3          0.8               0.50       150      0.9991914  0.9797468  0.9879012
  0.3  3          0.8               0.75        50      0.9938228  0.9521203  0.9913578
  0.3  3          0.8               0.75       100      0.9984429  0.9772468  0.9890523
  0.3  3          0.8               0.75       150      0.9988959  0.9797468  0.9890506
  0.3  3          0.8               1.00        50      0.9952533  0.9470570  0.9913611
  0.3  3          0.8               1.00       100      0.9984136  0.9797468  0.9902067
  0.3  3          0.8               1.00       150      0.9993164  0.9797468  0.9902067
  0.4  1          0.6               0.50        50      0.9765293  0.7783861  0.9896320
  0.4  1          0.6               0.50       100      0.9890543  0.8916139  0.9850194
  0.4  1          0.6               0.50       150      0.9911092  0.9269304  0.9855957
  0.4  1          0.6               0.75        50      0.9795484  0.7683228  0.9907831
  0.4  1          0.6               0.75       100      0.9882137  0.9016772  0.9861705
  0.4  1          0.6               0.75       150      0.9918363  0.9369937  0.9873232
  0.4  1          0.6               1.00        50      0.9801764  0.7181329  0.9907831
  0.4  1          0.6               1.00       100      0.9884387  0.8615823  0.9867485
  0.4  1          0.6               1.00       150      0.9920897  0.9320570  0.9861721
  0.4  1          0.8               0.50        50      0.9760891  0.7279114  0.9855974
  0.4  1          0.8               0.50       100      0.9883327  0.8890190  0.9832919
  0.4  1          0.8               0.50       150      0.9899969  0.9344304  0.9838666
  0.4  1          0.8               0.75        50      0.9803917  0.7405380  0.9884776
  0.4  1          0.8               0.75       100      0.9894403  0.8967089  0.9878996
  0.4  1          0.8               0.75       150      0.9917778  0.9319304  0.9855941
  0.4  1          0.8               1.00        50      0.9800222  0.7103797  0.9907831
  0.4  1          0.8               1.00       100      0.9885722  0.8766456  0.9884776
  0.4  1          0.8               1.00       150      0.9918070  0.9395253  0.9879012
  0.4  2          0.6               0.50        50      0.9881210  0.8967089  0.9879045
  0.4  2          0.6               0.50       100      0.9940421  0.9571203  0.9902050
  0.4  2          0.6               0.50       150      0.9969616  0.9797468  0.9890540
  0.4  2          0.6               0.75        50      0.9923661  0.9295570  0.9896303
  0.4  2          0.6               0.75       100      0.9969179  0.9722152  0.9902050
  0.4  2          0.6               0.75       150      0.9979372  0.9797468  0.9861705
  0.4  2          0.6               1.00        50      0.9915771  0.9219620  0.9913578
  0.4  2          0.6               1.00       100      0.9962101  0.9596519  0.9919358
  0.4  2          0.6               1.00       150      0.9981856  0.9772468  0.9884776
  0.4  2          0.8               0.50        50      0.9920411  0.9243671  0.9919358
  0.4  2          0.8               0.50       100      0.9968008  0.9646835  0.9896287
  0.4  2          0.8               0.50       150      0.9980838  0.9772468  0.9867485
  0.4  2          0.8               0.75        50      0.9921317  0.9369937  0.9878996
  0.4  2          0.8               0.75       100      0.9966164  0.9696519  0.9890523
  0.4  2          0.8               0.75       150      0.9983873  0.9797468  0.9873232
  0.4  2          0.8               1.00        50      0.9922780  0.9245253  0.9884776
  0.4  2          0.8               1.00       100      0.9965983  0.9621519  0.9902067
  0.4  2          0.8               1.00       150      0.9982751  0.9797468  0.9890540
  0.4  3          0.6               0.50        50      0.9950044  0.9595886  0.9913578
  0.4  3          0.6               0.50       100      0.9980152  0.9797468  0.9867468
  0.4  3          0.6               0.50       150      0.9989340  0.9797468  0.9867501
  0.4  3          0.6               0.75        50      0.9956500  0.9621203  0.9907831
  0.4  3          0.6               0.75       100      0.9987569  0.9797468  0.9890540
  0.4  3          0.6               0.75       150      0.9994062  0.9898734  0.9902067
  0.4  3          0.6               1.00        50      0.9967999  0.9571203  0.9930869
  0.4  3          0.6               1.00       100      0.9992802  0.9797468  0.9919325
  0.4  3          0.6               1.00       150      0.9994996  0.9898734  0.9896270
  0.4  3          0.8               0.50        50      0.9953313  0.9571519  0.9902034
  0.4  3          0.8               0.50       100      0.9988455  0.9797468  0.9861721
  0.4  3          0.8               0.50       150      0.9993263  0.9898734  0.9890523
  0.4  3          0.8               0.75        50      0.9967212  0.9621519  0.9913561
  0.4  3          0.8               0.75       100      0.9988087  0.9797468  0.9902034
  0.4  3          0.8               0.75       150      0.9995283  1.0000000  0.9884776
  0.4  3          0.8               1.00        50      0.9962999  0.9696835  0.9907847
  0.4  3          0.8               1.00       100      0.9986876  0.9797468  0.9902067
  0.4  3          0.8               1.00       150      0.9990605  0.9898734  0.9907814

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 150, max_depth = 3, eta = 0.4, gamma = 0, colsample_bytree = 0.8, min_child_weight = 1 and subsample = 0.75.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     18.6      0.9
  positive      0.0     80.5
                            
 Accuracy (average) : 0.9906

[1] "TRAIN accuracy: 0.990623534927332"
[1] "TRAIN +precision: 1"
[1] "TRAIN -precision: 0.952038369304556"
[1] "TRAIN specifity: 1"
[1] "TRAIN sensitivity: 0.988479262672811"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        8       12
            positive       12      568
[1] "TEST accuracy: 0.96"
[1] "TEST +precision: 0.979310344827586"
[1] "TEST -precision: 0.4"
[1] "TEST specifity: 0.4"
[1] "TEST sensitivity: 0.979310344827586"

[1] "DATASET NAME: TCT_Uni_IR_1"
[1] "TRAIN INSTANCES: 1264"
[1] "TEST INSTANCES: 225"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 3.78461694717407"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

1264 samples
 725 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1012, 1012, 1011, 1010, 1011 
Resampling results:

  ROC  Sens  Spec     
  1    1     0.9952506

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     50.0      0.2
  positive      0.0     49.8
                            
 Accuracy (average) : 0.9976

[1] "TRAIN accuracy: 0.997626582278481"
[1] "TRAIN +precision: 1"
[1] "TRAIN -precision: 0.995275590551181"
[1] "TRAIN specifity: 1"
[1] "TRAIN sensitivity: 0.995253164556962"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       12        1
            positive        7      205
[1] "TEST accuracy: 0.964444444444444"
[1] "TEST +precision: 0.966981132075472"
[1] "TEST -precision: 0.923076923076923"
[1] "TEST specifity: 0.631578947368421"
[1] "TEST sensitivity: 0.995145631067961"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 1.60729301770528"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

1264 samples
 725 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1012, 1010, 1012, 1011, 1011 
Resampling results across tuning parameters:

  C      M  ROC        Sens  Spec     
  0.010  1  0.9770988  1     0.9367079
  0.010  2  0.9769477  1     0.9367079
  0.010  3  0.9771827  1     0.9367079
  0.255  1  0.9773885  1     0.9398700
  0.255  2  0.9772878  1     0.9398700
  0.255  3  0.9774095  1     0.9398700
  0.500  1  0.9794034  1     0.9445944
  0.500  2  0.9793026  1     0.9445944
  0.500  3  0.9765539  1     0.9398700

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.5 and M = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     50.0      2.8
  positive      0.0     47.2
                            
 Accuracy (average) : 0.9723

[1] "TRAIN accuracy: 0.972310126582278"
[1] "TRAIN +precision: 1"
[1] "TRAIN -precision: 0.947526236881559"
[1] "TRAIN specifity: 1"
[1] "TRAIN sensitivity: 0.944620253164557"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        9        5
            positive       10      201
[1] "TEST accuracy: 0.933333333333333"
[1] "TEST +precision: 0.95260663507109"
[1] "TEST -precision: 0.642857142857143"
[1] "TEST specifity: 0.473684210526316"
[1] "TEST sensitivity: 0.975728155339806"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 4.38749418258667"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

1264 samples
 725 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1010, 1012, 1011, 1012, 1011 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens       Spec     
  0.3  1          0.6               0.50        50      0.9895205  0.9588926  0.9509811
  0.3  1          0.6               0.50       100      0.9931102  1.0000000  0.9652168
  0.3  1          0.6               0.50       150      0.9940673  1.0000000  0.9699538
  0.3  1          0.6               0.75        50      0.9881502  0.9525434  0.9635921
  0.3  1          0.6               0.75       100      0.9931858  1.0000000  0.9652043
  0.3  1          0.6               0.75       150      0.9943933  1.0000000  0.9699538
  0.3  1          0.6               1.00        50      0.9884019  0.9573053  0.9604549
  0.3  1          0.6               1.00       100      0.9920952  1.0000000  0.9683665
  0.3  1          0.6               1.00       150      0.9926996  1.0000000  0.9762780
  0.3  1          0.8               0.50        50      0.9881652  0.9525434  0.9525309
  0.3  1          0.8               0.50       100      0.9933164  1.0000000  0.9683665
  0.3  1          0.8               0.50       150      0.9947781  1.0000000  0.9730909
  0.3  1          0.8               0.75        50      0.9883733  0.9588551  0.9557055
  0.3  1          0.8               0.75       100      0.9927337  1.0000000  0.9715411
  0.3  1          0.8               0.75       150      0.9933501  1.0000000  0.9746907
  0.3  1          0.8               1.00        50      0.9888861  0.9573053  0.9620422
  0.3  1          0.8               1.00       100      0.9918686  1.0000000  0.9667792
  0.3  1          0.8               1.00       150      0.9932680  1.0000000  0.9715161
  0.3  2          0.6               0.50        50      0.9948814  1.0000000  0.9762780
  0.3  2          0.6               0.50       100      0.9963880  1.0000000  0.9762780
  0.3  2          0.6               0.50       150      0.9971348  1.0000000  0.9810149
  0.3  2          0.6               0.75        50      0.9967804  1.0000000  0.9762780
  0.3  2          0.6               0.75       100      0.9974888  1.0000000  0.9794526
  0.3  2          0.6               0.75       150      0.9971574  1.0000000  0.9826272
  0.3  2          0.6               1.00        50      0.9959846  1.0000000  0.9762780
  0.3  2          0.6               1.00       100      0.9956712  1.0000000  0.9826147
  0.3  2          0.6               1.00       150      0.9952359  1.0000000  0.9841895
  0.3  2          0.8               0.50        50      0.9947541  1.0000000  0.9715161
  0.3  2          0.8               0.50       100      0.9960643  1.0000000  0.9810024
  0.3  2          0.8               0.50       150      0.9959793  1.0000000  0.9826147
  0.3  2          0.8               0.75        50      0.9946965  1.0000000  0.9747032
  0.3  2          0.8               0.75       100      0.9953046  1.0000000  0.9810274
  0.3  2          0.8               0.75       150      0.9964025  1.0000000  0.9873516
  0.3  2          0.8               1.00        50      0.9942340  1.0000000  0.9731284
  0.3  2          0.8               1.00       100      0.9954798  1.0000000  0.9794401
  0.3  2          0.8               1.00       150      0.9951030  1.0000000  0.9857643
  0.3  3          0.6               0.50        50      0.9973932  1.0000000  0.9794276
  0.3  3          0.6               0.50       100      0.9982516  1.0000000  0.9841770
  0.3  3          0.6               0.50       150      0.9988840  1.0000000  0.9841895
  0.3  3          0.6               0.75        50      0.9967204  1.0000000  0.9810274
  0.3  3          0.6               0.75       100      0.9980710  1.0000000  0.9873391
  0.3  3          0.6               0.75       150      0.9989155  1.0000000  0.9841770
  0.3  3          0.6               1.00        50      0.9978715  1.0000000  0.9810274
  0.3  3          0.6               1.00       100      0.9985587  1.0000000  0.9857643
  0.3  3          0.6               1.00       150      0.9993041  1.0000000  0.9841770
  0.3  3          0.8               0.50        50      0.9979510  1.0000000  0.9809899
  0.3  3          0.8               0.50       100      0.9983621  1.0000000  0.9809899
  0.3  3          0.8               0.50       150      0.9991191  1.0000000  0.9778403
  0.3  3          0.8               0.75        50      0.9967299  1.0000000  0.9810149
  0.3  3          0.8               0.75       100      0.9977526  1.0000000  0.9857643
  0.3  3          0.8               0.75       150      0.9986593  1.0000000  0.9841770
  0.3  3          0.8               1.00        50      0.9975782  1.0000000  0.9794276
  0.3  3          0.8               1.00       100      0.9980031  1.0000000  0.9841770
  0.3  3          0.8               1.00       150      0.9988716  1.0000000  0.9825897
  0.4  1          0.6               0.50        50      0.9927207  0.9795276  0.9604299
  0.4  1          0.6               0.50       100      0.9940432  1.0000000  0.9683540
  0.4  1          0.6               0.50       150      0.9945536  1.0000000  0.9699413
  0.4  1          0.6               0.75        50      0.9916695  0.9858143  0.9588426
  0.4  1          0.6               0.75       100      0.9947136  1.0000000  0.9683415
  0.4  1          0.6               0.75       150      0.9956829  1.0000000  0.9746782
  0.4  1          0.6               1.00        50      0.9914354  0.9857768  0.9636170
  0.4  1          0.6               1.00       100      0.9929016  1.0000000  0.9746907
  0.4  1          0.6               1.00       150      0.9922482  1.0000000  0.9778653
  0.4  1          0.8               0.50        50      0.9915305  0.9874016  0.9509436
  0.4  1          0.8               0.50       100      0.9948895  1.0000000  0.9731159
  0.4  1          0.8               0.50       150      0.9949760  1.0000000  0.9746782
  0.4  1          0.8               0.75        50      0.9916954  0.9810524  0.9636170
  0.4  1          0.8               0.75       100      0.9937543  1.0000000  0.9731034
  0.4  1          0.8               0.75       150      0.9945224  1.0000000  0.9731159
  0.4  1          0.8               1.00        50      0.9913533  0.9857768  0.9683790
  0.4  1          0.8               1.00       100      0.9922374  1.0000000  0.9699413
  0.4  1          0.8               1.00       150      0.9922257  1.0000000  0.9762780
  0.4  2          0.6               0.50        50      0.9961833  1.0000000  0.9746907
  0.4  2          0.6               0.50       100      0.9961367  1.0000000  0.9778528
  0.4  2          0.6               0.50       150      0.9967971  1.0000000  0.9778528
  0.4  2          0.6               0.75        50      0.9946067  1.0000000  0.9794401
  0.4  2          0.6               0.75       100      0.9955820  1.0000000  0.9826022
  0.4  2          0.6               0.75       150      0.9969318  1.0000000  0.9841895
  0.4  2          0.6               1.00        50      0.9964344  1.0000000  0.9762905
  0.4  2          0.6               1.00       100      0.9969825  1.0000000  0.9841895
  0.4  2          0.6               1.00       150      0.9979164  1.0000000  0.9857768
  0.4  2          0.8               0.50        50      0.9964855  1.0000000  0.9731159
  0.4  2          0.8               0.50       100      0.9962778  1.0000000  0.9825897
  0.4  2          0.8               0.50       150      0.9969496  1.0000000  0.9778528
  0.4  2          0.8               0.75        50      0.9958104  1.0000000  0.9778653
  0.4  2          0.8               0.75       100      0.9959926  1.0000000  0.9857768
  0.4  2          0.8               0.75       150      0.9965032  1.0000000  0.9826147
  0.4  2          0.8               1.00        50      0.9955310  1.0000000  0.9778778
  0.4  2          0.8               1.00       100      0.9956174  1.0000000  0.9810149
  0.4  2          0.8               1.00       150      0.9970826  1.0000000  0.9841770
  0.4  3          0.6               0.50        50      0.9962544  1.0000000  0.9794401
  0.4  3          0.6               0.50       100      0.9971728  1.0000000  0.9826022
  0.4  3          0.6               0.50       150      0.9973092  1.0000000  0.9778403
  0.4  3          0.6               0.75        50      0.9974702  1.0000000  0.9810274
  0.4  3          0.6               0.75       100      0.9988082  1.0000000  0.9841895
  0.4  3          0.6               0.75       150      0.9996894  1.0000000  0.9794401
  0.4  3          0.6               1.00        50      0.9967257  1.0000000  0.9810274
  0.4  3          0.6               1.00       100      0.9977060  1.0000000  0.9810399
  0.4  3          0.6               1.00       150      0.9983384  1.0000000  0.9794526
  0.4  3          0.8               0.50        50      0.9985244  1.0000000  0.9794526
  0.4  3          0.8               0.50       100      0.9984128  1.0000000  0.9826022
  0.4  3          0.8               0.50       150      0.9988096  1.0000000  0.9826147
  0.4  3          0.8               0.75        50      0.9982424  1.0000000  0.9826022
  0.4  3          0.8               0.75       100      0.9994128  1.0000000  0.9810024
  0.4  3          0.8               0.75       150      0.9998256  1.0000000  0.9841770
  0.4  3          0.8               1.00        50      0.9975771  1.0000000  0.9841770
  0.4  3          0.8               1.00       100      0.9982005  1.0000000  0.9857643
  0.4  3          0.8               1.00       150      0.9992180  1.0000000  0.9825897

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 150, max_depth = 3, eta = 0.4, gamma = 0, colsample_bytree = 0.8, min_child_weight = 1 and subsample = 0.75.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     50.0      0.8
  positive      0.0     49.2
                            
 Accuracy (average) : 0.9921

[1] "TRAIN accuracy: 0.992088607594937"
[1] "TRAIN +precision: 1"
[1] "TRAIN -precision: 0.984423676012461"
[1] "TRAIN specifity: 1"
[1] "TRAIN sensitivity: 0.984177215189873"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       12        3
            positive        7      203
[1] "TEST accuracy: 0.955555555555556"
[1] "TEST +precision: 0.966666666666667"
[1] "TEST -precision: 0.8"
[1] "TEST specifity: 0.631578947368421"
[1] "TEST sensitivity: 0.985436893203884"

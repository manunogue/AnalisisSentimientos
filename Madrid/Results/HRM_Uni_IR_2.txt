[1] "DATASET NAME: HRM_Uni_IR_2"
[1] "TRAIN INSTANCES: 459"
[1] "TEST INSTANCES: 112"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 2.02962613105774"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

459 samples
747 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 366, 368, 368, 367, 367 
Resampling results:

  ROC        Sens       Spec     
  0.9920067  0.9702317  0.9862653

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     35.5      0.9
  positive      1.1     62.5
                            
 Accuracy (average) : 0.9804

[1] "TRAIN accuracy: 0.980392156862745"
[1] "TRAIN +precision: 0.982876712328767"
[1] "TRAIN -precision: 0.976047904191617"
[1] "TRAIN specifity: 0.970238095238095"
[1] "TRAIN sensitivity: 0.986254295532646"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        8        2
            positive       10       92
[1] "TEST accuracy: 0.892857142857143"
[1] "TEST +precision: 0.901960784313726"
[1] "TEST -precision: 0.8"
[1] "TEST specifity: 0.444444444444444"
[1] "TEST sensitivity: 0.978723404255319"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 1.10411741733551"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

459 samples
747 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 368, 367, 367, 367, 367 
Resampling results across tuning parameters:

  C      M  ROC        Sens       Spec     
  0.010  1  0.9028509  0.8873440  0.9106955
  0.010  2  0.9104955  0.8812834  0.9106955
  0.010  3  0.8963160  0.8754011  0.9038574
  0.255  1  0.9490088  0.9702317  0.9210403
  0.255  2  0.9600001  0.9702317  0.9244886
  0.255  3  0.9463068  0.9584670  0.9038574
  0.500  1  0.9535067  0.9821747  0.9210403
  0.500  2  0.9658765  0.9821747  0.9244886
  0.500  3  0.9470900  0.9584670  0.9038574

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.5 and M = 2.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     35.9      4.8
  positive      0.7     58.6
                            
 Accuracy (average) : 0.9455

[1] "TRAIN accuracy: 0.945533769063181"
[1] "TRAIN +precision: 0.988970588235294"
[1] "TRAIN -precision: 0.882352941176471"
[1] "TRAIN specifity: 0.982142857142857"
[1] "TRAIN sensitivity: 0.924398625429553"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        7        5
            positive       11       89
[1] "TEST accuracy: 0.857142857142857"
[1] "TEST +precision: 0.89"
[1] "TEST -precision: 0.583333333333333"
[1] "TEST specifity: 0.388888888888889"
[1] "TEST sensitivity: 0.946808510638298"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 2.35769041776657"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

459 samples
747 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 367, 367, 368, 367, 367 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens       Spec     
  0.3  1          0.6               0.50        50      0.9789264  0.8454545  0.9691409
  0.3  1          0.6               0.50       100      0.9871684  0.9169340  0.9794272
  0.3  1          0.6               0.50       150      0.9891863  0.9404635  0.9760374
  0.3  1          0.6               0.75        50      0.9749262  0.8217469  0.9760374
  0.3  1          0.6               0.75       100      0.9893306  0.9463458  0.9690824
  0.3  1          0.6               0.75       150      0.9915645  0.9522282  0.9759205
  0.3  1          0.6               1.00        50      0.9785299  0.8151515  0.9828171
  0.3  1          0.6               1.00       100      0.9872047  0.9463458  0.9690240
  0.3  1          0.6               1.00       150      0.9904541  0.9522282  0.9690240
  0.3  1          0.8               0.50        50      0.9671298  0.8096257  0.9725307
  0.3  1          0.8               0.50       100      0.9894111  0.9402852  0.9794272
  0.3  1          0.8               0.50       150      0.9908555  0.9698752  0.9621859
  0.3  1          0.8               0.75        50      0.9718986  0.8153298  0.9759790
  0.3  1          0.8               0.75       100      0.9891401  0.9461676  0.9759205
  0.3  1          0.8               0.75       150      0.9923885  0.9582888  0.9793103
  0.3  1          0.8               1.00        50      0.9766491  0.8096257  0.9794272
  0.3  1          0.8               1.00       100      0.9875121  0.9402852  0.9656341
  0.3  1          0.8               1.00       150      0.9916724  0.9522282  0.9690240
  0.3  2          0.6               0.50        50      0.9872150  0.9108734  0.9691409
  0.3  2          0.6               0.50       100      0.9888047  0.9522282  0.9450614
  0.3  2          0.6               0.50       150      0.9904291  0.9698752  0.9450614
  0.3  2          0.6               0.75        50      0.9910614  0.9522282  0.9897136
  0.3  2          0.6               0.75       100      0.9940951  0.9698752  0.9759205
  0.3  2          0.6               0.75       150      0.9932812  0.9759358  0.9622443
  0.3  2          0.6               1.00        50      0.9872940  0.9522282  0.9691409
  0.3  2          0.6               1.00       100      0.9927815  0.9759358  0.9691409
  0.3  2          0.6               1.00       150      0.9932873  0.9818182  0.9725891
  0.3  2          0.8               0.50        50      0.9864493  0.9228164  0.9690824
  0.3  2          0.8               0.50       100      0.9900073  0.9698752  0.9588545
  0.3  2          0.8               0.50       150      0.9923662  0.9698752  0.9622443
  0.3  2          0.8               0.75        50      0.9912560  0.9342246  0.9691993
  0.3  2          0.8               0.75       100      0.9914407  0.9757576  0.9692577
  0.3  2          0.8               0.75       150      0.9919517  0.9818182  0.9656926
  0.3  2          0.8               1.00        50      0.9914718  0.9522282  0.9725891
  0.3  2          0.8               1.00       100      0.9929830  0.9818182  0.9725307
  0.3  2          0.8               1.00       150      0.9930774  0.9818182  0.9725307
  0.3  3          0.6               0.50        50      0.9914798  0.9461676  0.9760374
  0.3  3          0.6               0.50       100      0.9928777  0.9698752  0.9519579
  0.3  3          0.6               0.50       150      0.9930779  0.9818182  0.9485681
  0.3  3          0.6               0.75        50      0.9922586  0.9461676  0.9760374
  0.3  3          0.6               0.75       100      0.9933781  0.9757576  0.9725307
  0.3  3          0.6               0.75       150      0.9942935  0.9818182  0.9622443
  0.3  3          0.6               1.00        50      0.9931814  0.9698752  0.9691409
  0.3  3          0.6               1.00       100      0.9932688  0.9818182  0.9622443
  0.3  3          0.6               1.00       150      0.9937851  0.9818182  0.9656341
  0.3  3          0.8               0.50        50      0.9867229  0.9404635  0.9589129
  0.3  3          0.8               0.50       100      0.9916269  0.9759358  0.9520164
  0.3  3          0.8               0.50       150      0.9910210  0.9759358  0.9518995
  0.3  3          0.8               0.75        50      0.9925690  0.9698752  0.9760374
  0.3  3          0.8               0.75       100      0.9931784  0.9818182  0.9587960
  0.3  3          0.8               0.75       150      0.9942983  0.9818182  0.9587960
  0.3  3          0.8               1.00        50      0.9929760  0.9818182  0.9725891
  0.3  3          0.8               1.00       100      0.9934782  0.9818182  0.9656926
  0.3  3          0.8               1.00       150      0.9945043  0.9818182  0.9587376
  0.4  1          0.6               0.50        50      0.9763641  0.8581105  0.9656341
  0.4  1          0.6               0.50       100      0.9881879  0.9404635  0.9588545
  0.4  1          0.6               0.50       150      0.9903266  0.9698752  0.9691409
  0.4  1          0.6               0.75        50      0.9779423  0.8568627  0.9793688
  0.4  1          0.6               0.75       100      0.9917871  0.9463458  0.9655757
  0.4  1          0.6               0.75       150      0.9933843  0.9582888  0.9794272
  0.4  1          0.6               1.00        50      0.9831590  0.8636364  0.9794272
  0.4  1          0.6               1.00       100      0.9897398  0.9522282  0.9759205
  0.4  1          0.6               1.00       150      0.9919688  0.9522282  0.9655757
  0.4  1          0.8               0.50        50      0.9777365  0.8516934  0.9760374
  0.4  1          0.8               0.50       100      0.9883046  0.9345811  0.9759790
  0.4  1          0.8               0.50       150      0.9912428  0.9522282  0.9759790
  0.4  1          0.8               0.75        50      0.9798922  0.8575758  0.9828755
  0.4  1          0.8               0.75       100      0.9913450  0.9522282  0.9656341
  0.4  1          0.8               0.75       150      0.9922634  0.9522282  0.9690824
  0.4  1          0.8               1.00        50      0.9835639  0.8636364  0.9794272
  0.4  1          0.8               1.00       100      0.9906578  0.9522282  0.9690240
  0.4  1          0.8               1.00       150      0.9921791  0.9582888  0.9689655
  0.4  2          0.6               0.50        50      0.9891218  0.9283422  0.9794272
  0.4  2          0.6               0.50       100      0.9915544  0.9698752  0.9622443
  0.4  2          0.6               0.50       150      0.9927771  0.9698752  0.9621859
  0.4  2          0.6               0.75        50      0.9894173  0.9520499  0.9589129
  0.4  2          0.6               0.75       100      0.9916467  0.9757576  0.9555231
  0.4  2          0.6               0.75       150      0.9918412  0.9818182  0.9520164
  0.4  2          0.6               1.00        50      0.9915667  0.9522282  0.9759790
  0.4  2          0.6               1.00       100      0.9919543  0.9818182  0.9656926
  0.4  2          0.6               1.00       150      0.9918542  0.9818182  0.9622443
  0.4  2          0.8               0.50        50      0.9897395  0.9522282  0.9587960
  0.4  2          0.8               0.50       100      0.9909367  0.9698752  0.9587960
  0.4  2          0.8               0.50       150      0.9920561  0.9759358  0.9485096
  0.4  2          0.8               0.75        50      0.9925898  0.9638146  0.9759205
  0.4  2          0.8               0.75       100      0.9930863  0.9641711  0.9656926
  0.4  2          0.8               0.75       150      0.9932899  0.9818182  0.9587960
  0.4  2          0.8               1.00        50      0.9930005  0.9522282  0.9759790
  0.4  2          0.8               1.00       100      0.9941086  0.9818182  0.9725891
  0.4  2          0.8               1.00       150      0.9936033  0.9818182  0.9622443
  0.4  3          0.6               0.50        50      0.9883992  0.9698752  0.9690240
  0.4  3          0.6               0.50       100      0.9915582  0.9698752  0.9690824
  0.4  3          0.6               0.50       150      0.9919709  0.9757576  0.9587376
  0.4  3          0.6               0.75        50      0.9929742  0.9759358  0.9520164
  0.4  3          0.6               0.75       100      0.9934699  0.9818182  0.9587960
  0.4  3          0.6               0.75       150      0.9940867  0.9818182  0.9587376
  0.4  3          0.6               1.00        50      0.9934910  0.9818182  0.9726476
  0.4  3          0.6               1.00       100      0.9946031  0.9818182  0.9623612
  0.4  3          0.6               1.00       150      0.9956361  0.9818182  0.9623027
  0.4  3          0.8               0.50        50      0.9930691  0.9698752  0.9623027
  0.4  3          0.8               0.50       100      0.9931867  0.9759358  0.9587960
  0.4  3          0.8               0.50       150      0.9924786  0.9818182  0.9484512
  0.4  3          0.8               0.75        50      0.9918643  0.9818182  0.9724722
  0.4  3          0.8               0.75       100      0.9926594  0.9818182  0.9587376
  0.4  3          0.8               0.75       150      0.9949195  0.9818182  0.9518410
  0.4  3          0.8               1.00        50      0.9912194  0.9818182  0.9657510
  0.4  3          0.8               1.00       100      0.9929562  0.9818182  0.9520164
  0.4  3          0.8               1.00       150      0.9942935  0.9818182  0.9622443

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 150, max_depth = 3, eta = 0.4, gamma = 0, colsample_bytree = 0.6, min_child_weight = 1 and subsample = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     35.9      2.4
  positive      0.7     61.0
                            
 Accuracy (average) : 0.9695

[1] "TRAIN accuracy: 0.969498910675381"
[1] "TRAIN +precision: 0.989399293286219"
[1] "TRAIN -precision: 0.9375"
[1] "TRAIN specifity: 0.982142857142857"
[1] "TRAIN sensitivity: 0.962199312714777"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       11        5
            positive        7       89
[1] "TEST accuracy: 0.892857142857143"
[1] "TEST +precision: 0.927083333333333"
[1] "TEST -precision: 0.6875"
[1] "TEST specifity: 0.611111111111111"
[1] "TEST sensitivity: 0.946808510638298"

[1] "DATASET NAME: TCT_Bi_IR_10"
[1] "TRAIN INSTANCES: 732"
[1] "TEST INSTANCES: 225"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 2.77737498283386"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

732 samples
940 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 586, 585, 586, 586, 585 
Resampling results:

  ROC        Sens      Spec     
  0.9859883  0.832381  0.9952381

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     11.6      0.4
  positive      2.3     85.7
                            
 Accuracy (average) : 0.9727

[1] "TRAIN accuracy: 0.972677595628415"
[1] "TRAIN +precision: 0.97360248447205"
[1] "TRAIN -precision: 0.965909090909091"
[1] "TRAIN specifity: 0.833333333333333"
[1] "TRAIN sensitivity: 0.995238095238095"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        3        0
            positive       14      208
[1] "TEST accuracy: 0.937777777777778"
[1] "TEST +precision: 0.936936936936937"
[1] "TEST -precision: 1"
[1] "TEST specifity: 0.176470588235294"
[1] "TEST sensitivity: 1"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 1.6410081466039"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

732 samples
940 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 585, 586, 585, 586, 586 
Resampling results across tuning parameters:

  C      M  ROC        Sens        Spec     
  0.010  1  0.5245881  0.02952381  1.0000000
  0.010  2  0.5245881  0.02952381  1.0000000
  0.010  3  0.5100000  0.02000000  1.0000000
  0.255  1  0.7032143  0.26523810  0.9984127
  0.255  2  0.6774603  0.22523810  0.9984127
  0.255  3  0.6675586  0.21571429  0.9952381
  0.500  1  0.7399263  0.27523810  0.9984127
  0.500  2  0.6928798  0.24523810  0.9984127
  0.500  3  0.6675586  0.21571429  0.9952381

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.5 and M = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative      3.8      0.1
  positive     10.1     85.9
                            
 Accuracy (average) : 0.8975

[1] "TRAIN accuracy: 0.897540983606557"
[1] "TRAIN +precision: 0.894736842105263"
[1] "TRAIN -precision: 0.96551724137931"
[1] "TRAIN specifity: 0.274509803921569"
[1] "TRAIN sensitivity: 0.998412698412698"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        0        0
            positive       17      208
[1] "TEST accuracy: 0.924444444444444"
[1] "TEST +precision: 0.924444444444444"
[1] "TEST -precision: NaN"
[1] "TEST specifity: 0"
[1] "TEST sensitivity: 1"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 3.33108108441035"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

732 samples
940 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 585, 585, 586, 586, 586 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens       Spec     
  0.3  1          0.6               0.50        50      0.7889701  0.2261905  0.9936508
  0.3  1          0.6               0.50       100      0.8127022  0.2761905  0.9857143
  0.3  1          0.6               0.50       150      0.8156935  0.3147619  0.9825397
  0.3  1          0.6               0.75        50      0.7893122  0.2366667  0.9968254
  0.3  1          0.6               0.75       100      0.8235714  0.3157143  0.9936508
  0.3  1          0.6               0.75       150      0.8446240  0.4033333  0.9825397
  0.3  1          0.6               1.00        50      0.8103099  0.2257143  1.0000000
  0.3  1          0.6               1.00       100      0.8462944  0.3147619  0.9984127
  0.3  1          0.6               1.00       150      0.8779346  0.3647619  0.9952381
  0.3  1          0.8               0.50        50      0.7894747  0.2261905  0.9952381
  0.3  1          0.8               0.50       100      0.8247109  0.3152381  0.9904762
  0.3  1          0.8               0.50       150      0.8352853  0.3747619  0.9888889
  0.3  1          0.8               0.75        50      0.8154441  0.2961905  0.9984127
  0.3  1          0.8               0.75       100      0.8584958  0.3657143  0.9952381
  0.3  1          0.8               0.75       150      0.8681179  0.4138095  0.9857143
  0.3  1          0.8               1.00        50      0.8079856  0.2161905  1.0000000
  0.3  1          0.8               1.00       100      0.8519615  0.3252381  0.9984127
  0.3  1          0.8               1.00       150      0.8716856  0.3447619  0.9952381
  0.3  2          0.6               0.50        50      0.8097676  0.3061905  0.9888889
  0.3  2          0.6               0.50       100      0.8266119  0.3447619  0.9857143
  0.3  2          0.6               0.50       150      0.8344879  0.3847619  0.9857143
  0.3  2          0.6               0.75        50      0.8393103  0.3547619  0.9936508
  0.3  2          0.6               0.75       100      0.8630707  0.4038095  0.9888889
  0.3  2          0.6               0.75       150      0.8679800  0.4333333  0.9825397
  0.3  2          0.6               1.00        50      0.8777551  0.3542857  0.9968254
  0.3  2          0.6               1.00       100      0.9012642  0.4823810  0.9904762
  0.3  2          0.6               1.00       150      0.9023432  0.5023810  0.9873016
  0.3  2          0.8               0.50        50      0.8288360  0.3547619  0.9888889
  0.3  2          0.8               0.50       100      0.8376493  0.3938095  0.9809524
  0.3  2          0.8               0.50       150      0.8340306  0.4038095  0.9825397
  0.3  2          0.8               0.75        50      0.8603685  0.3447619  0.9920635
  0.3  2          0.8               0.75       100      0.8743651  0.4328571  0.9888889
  0.3  2          0.8               0.75       150      0.8790816  0.4328571  0.9873016
  0.3  2          0.8               1.00        50      0.8658919  0.3047619  0.9984127
  0.3  2          0.8               1.00       100      0.8908598  0.4328571  0.9904762
  0.3  2          0.8               1.00       150      0.8951682  0.4328571  0.9857143
  0.3  3          0.6               0.50        50      0.8352646  0.3447619  0.9888889
  0.3  3          0.6               0.50       100      0.8515098  0.4033333  0.9841270
  0.3  3          0.6               0.50       150      0.8538095  0.4033333  0.9841270
  0.3  3          0.6               0.75        50      0.8813511  0.4333333  0.9904762
  0.3  3          0.6               0.75       100      0.8911772  0.4723810  0.9809524
  0.3  3          0.6               0.75       150      0.8929422  0.4528571  0.9841270
  0.3  3          0.6               1.00        50      0.8952419  0.4428571  0.9952381
  0.3  3          0.6               1.00       100      0.8989569  0.5023810  0.9904762
  0.3  3          0.6               1.00       150      0.8965873  0.5023810  0.9857143
  0.3  3          0.8               0.50        50      0.8441534  0.3947619  0.9825397
  0.3  3          0.8               0.50       100      0.8564418  0.4238095  0.9793651
  0.3  3          0.8               0.50       150      0.8521032  0.4523810  0.9777778
  0.3  3          0.8               0.75        50      0.8730442  0.3933333  0.9936508
  0.3  3          0.8               0.75       100      0.8776965  0.4233333  0.9873016
  0.3  3          0.8               0.75       150      0.8808012  0.4433333  0.9841270
  0.3  3          0.8               1.00        50      0.8994218  0.4433333  0.9952381
  0.3  3          0.8               1.00       100      0.9013001  0.4828571  0.9904762
  0.3  3          0.8               1.00       150      0.8954913  0.4928571  0.9904762
  0.4  1          0.6               0.50        50      0.8013889  0.2947619  0.9936508
  0.4  1          0.6               0.50       100      0.8373866  0.3447619  0.9873016
  0.4  1          0.6               0.50       150      0.8449981  0.3442857  0.9825397
  0.4  1          0.6               0.75        50      0.8171221  0.2861905  0.9968254
  0.4  1          0.6               0.75       100      0.8554611  0.3728571  0.9904762
  0.4  1          0.6               0.75       150      0.8596977  0.4323810  0.9809524
  0.4  1          0.6               1.00        50      0.8309618  0.2661905  0.9984127
  0.4  1          0.6               1.00       100      0.8696674  0.3052381  0.9968254
  0.4  1          0.6               1.00       150      0.8806652  0.3742857  0.9888889
  0.4  1          0.8               0.50        50      0.7968027  0.2652381  0.9936508
  0.4  1          0.8               0.50       100      0.8312226  0.3352381  0.9920635
  0.4  1          0.8               0.50       150      0.8358201  0.3447619  0.9841270
  0.4  1          0.8               0.75        50      0.8167593  0.3052381  0.9968254
  0.4  1          0.8               0.75       100      0.8452816  0.3733333  0.9841270
  0.4  1          0.8               0.75       150      0.8519180  0.3738095  0.9825397
  0.4  1          0.8               1.00        50      0.8278647  0.2657143  0.9984127
  0.4  1          0.8               1.00       100      0.8671618  0.3547619  0.9968254
  0.4  1          0.8               1.00       150      0.8782237  0.3942857  0.9920635
  0.4  2          0.6               0.50        50      0.8402891  0.2757143  0.9888889
  0.4  2          0.6               0.50       100      0.8423526  0.3347619  0.9873016
  0.4  2          0.6               0.50       150      0.8496542  0.3733333  0.9809524
  0.4  2          0.6               0.75        50      0.8606217  0.3847619  0.9904762
  0.4  2          0.6               0.75       100      0.8649490  0.4233333  0.9857143
  0.4  2          0.6               0.75       150      0.8675661  0.4523810  0.9841270
  0.4  2          0.6               1.00        50      0.8634543  0.3738095  0.9968254
  0.4  2          0.6               1.00       100      0.8818934  0.4328571  0.9841270
  0.4  2          0.6               1.00       150      0.8839928  0.4528571  0.9825397
  0.4  2          0.8               0.50        50      0.8272392  0.3242857  0.9920635
  0.4  2          0.8               0.50       100      0.8422033  0.3542857  0.9825397
  0.4  2          0.8               0.50       150      0.8547506  0.3933333  0.9873016
  0.4  2          0.8               0.75        50      0.8613776  0.3738095  0.9952381
  0.4  2          0.8               0.75       100      0.8706293  0.4333333  0.9873016
  0.4  2          0.8               0.75       150      0.8819539  0.4423810  0.9873016
  0.4  2          0.8               1.00        50      0.8688416  0.3847619  0.9936508
  0.4  2          0.8               1.00       100      0.8850945  0.4819048  0.9809524
  0.4  2          0.8               1.00       150      0.8849320  0.4919048  0.9841270
  0.4  3          0.6               0.50        50      0.8404611  0.3833333  0.9793651
  0.4  3          0.6               0.50       100      0.8398715  0.3933333  0.9825397
  0.4  3          0.6               0.50       150      0.8436999  0.3933333  0.9793651
  0.4  3          0.6               0.75        50      0.8630820  0.3942857  0.9825397
  0.4  3          0.6               0.75       100      0.8720597  0.4238095  0.9888889
  0.4  3          0.6               0.75       150      0.8717800  0.4438095  0.9873016
  0.4  3          0.6               1.00        50      0.8911017  0.4723810  0.9904762
  0.4  3          0.6               1.00       100      0.8980178  0.4923810  0.9857143
  0.4  3          0.6               1.00       150      0.8983862  0.5023810  0.9841270
  0.4  3          0.8               0.50        50      0.8523828  0.3857143  0.9857143
  0.4  3          0.8               0.50       100      0.8566421  0.3938095  0.9777778
  0.4  3          0.8               0.50       150      0.8599187  0.4142857  0.9761905
  0.4  3          0.8               0.75        50      0.8761999  0.4238095  0.9857143
  0.4  3          0.8               0.75       100      0.8798772  0.4238095  0.9841270
  0.4  3          0.8               0.75       150      0.8812547  0.4338095  0.9825397
  0.4  3          0.8               1.00        50      0.8978628  0.4823810  0.9952381
  0.4  3          0.8               1.00       100      0.8980537  0.4923810  0.9888889
  0.4  3          0.8               1.00       150      0.8991969  0.5023810  0.9857143

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 150, max_depth = 2, eta = 0.3, gamma = 0, colsample_bytree = 0.6, min_child_weight = 1 and subsample = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative      7.0      1.1
  positive      7.0     85.0
                            
 Accuracy (average) : 0.9194

[1] "TRAIN accuracy: 0.919398907103825"
[1] "TRAIN +precision: 0.924219910846954"
[1] "TRAIN -precision: 0.864406779661017"
[1] "TRAIN specifity: 0.5"
[1] "TRAIN sensitivity: 0.987301587301587"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        1        2
            positive       16      206
[1] "TEST accuracy: 0.92"
[1] "TEST +precision: 0.927927927927928"
[1] "TEST -precision: 0.333333333333333"
[1] "TEST specifity: 0.0588235294117647"
[1] "TEST sensitivity: 0.990384615384615"

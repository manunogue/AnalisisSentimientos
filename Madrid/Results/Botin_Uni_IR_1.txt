[1] "DATASET NAME: Botin_Uni_IR_1"
[1] "TRAIN INSTANCES: 2900"
[1] "TEST INSTANCES: 573"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 29.4771978855133"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

2900 samples
 638 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 2320, 2320, 2320, 2320, 2320 
Resampling results:

  ROC      Sens       Spec     
  0.99044  0.9986207  0.9655172

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     49.9      1.7
  positive      0.1     48.3
                            
 Accuracy (average) : 0.9821

[1] "TRAIN accuracy: 0.982068965517241"
[1] "TRAIN +precision: 0.998573466476462"
[1] "TRAIN -precision: 0.966622162883845"
[1] "TRAIN specifity: 0.998620689655172"
[1] "TRAIN sensitivity: 0.96551724137931"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       47       18
            positive       29      479
[1] "TEST accuracy: 0.917975567190227"
[1] "TEST +precision: 0.942913385826772"
[1] "TEST -precision: 0.723076923076923"
[1] "TEST specifity: 0.618421052631579"
[1] "TEST sensitivity: 0.963782696177062"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 4.33770004908244"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

2900 samples
 638 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 2320, 2320, 2320, 2320, 2320 
Resampling results across tuning parameters:

  C      M  ROC        Sens       Spec     
  0.010  1  0.9256433  0.9379310  0.8910345
  0.010  2  0.9280214  0.9324138  0.8924138
  0.010  3  0.9310630  0.9213793  0.8882759
  0.255  1  0.9490048  0.9820690  0.9082759
  0.255  2  0.9494174  0.9717241  0.9048276
  0.255  3  0.9494875  0.9620690  0.8958621
  0.500  1  0.9507063  0.9882759  0.9089655
  0.500  2  0.9532200  0.9793103  0.9055172
  0.500  3  0.9512580  0.9648276  0.8979310

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.5 and M = 2.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     49.0      4.7
  positive      1.0     45.3
                            
 Accuracy (average) : 0.9424

[1] "TRAIN accuracy: 0.942413793103448"
[1] "TRAIN +precision: 0.977661950856292"
[1] "TRAIN -precision: 0.912010276172126"
[1] "TRAIN specifity: 0.979310344827586"
[1] "TRAIN sensitivity: 0.90551724137931"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       50       38
            positive       26      459
[1] "TEST accuracy: 0.888307155322862"
[1] "TEST +precision: 0.94639175257732"
[1] "TEST -precision: 0.568181818181818"
[1] "TEST specifity: 0.657894736842105"
[1] "TEST sensitivity: 0.92354124748491"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 9.37758361498515"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

2900 samples
 638 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 2320, 2320, 2320, 2320, 2320 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens       Spec     
  0.3  1          0.6               0.50        50      0.9581558  0.8675862  0.9172414
  0.3  1          0.6               0.50       100      0.9767111  0.8965517  0.9427586
  0.3  1          0.6               0.50       150      0.9836171  0.9255172  0.9448276
  0.3  1          0.6               0.75        50      0.9564233  0.8606897  0.9234483
  0.3  1          0.6               0.75       100      0.9749489  0.9048276  0.9365517
  0.3  1          0.6               0.75       150      0.9835315  0.9275862  0.9462069
  0.3  1          0.6               1.00        50      0.9571570  0.8668966  0.9213793
  0.3  1          0.6               1.00       100      0.9733995  0.8931034  0.9386207
  0.3  1          0.6               1.00       150      0.9828074  0.9151724  0.9482759
  0.3  1          0.8               0.50        50      0.9570024  0.8524138  0.9296552
  0.3  1          0.8               0.50       100      0.9761914  0.9068966  0.9365517
  0.3  1          0.8               0.50       150      0.9826040  0.9351724  0.9448276
  0.3  1          0.8               0.75        50      0.9557622  0.8600000  0.9193103
  0.3  1          0.8               0.75       100      0.9760345  0.9020690  0.9413793
  0.3  1          0.8               0.75       150      0.9836159  0.9206897  0.9517241
  0.3  1          0.8               1.00        50      0.9570559  0.8551724  0.9234483
  0.3  1          0.8               1.00       100      0.9740725  0.8958621  0.9379310
  0.3  1          0.8               1.00       150      0.9828193  0.9213793  0.9489655
  0.3  2          0.6               0.50        50      0.9788264  0.9227586  0.9379310
  0.3  2          0.6               0.50       100      0.9904923  0.9613793  0.9482759
  0.3  2          0.6               0.50       150      0.9932580  0.9793103  0.9579310
  0.3  2          0.6               0.75        50      0.9794411  0.9234483  0.9386207
  0.3  2          0.6               0.75       100      0.9912925  0.9606897  0.9517241
  0.3  2          0.6               0.75       150      0.9939156  0.9793103  0.9593103
  0.3  2          0.6               1.00        50      0.9775077  0.9124138  0.9400000
  0.3  2          0.6               1.00       100      0.9904637  0.9524138  0.9503448
  0.3  2          0.6               1.00       150      0.9942342  0.9758621  0.9531034
  0.3  2          0.8               0.50        50      0.9784043  0.9255172  0.9331034
  0.3  2          0.8               0.50       100      0.9899215  0.9655172  0.9475862
  0.3  2          0.8               0.50       150      0.9933270  0.9793103  0.9537931
  0.3  2          0.8               0.75        50      0.9807848  0.9144828  0.9434483
  0.3  2          0.8               0.75       100      0.9920476  0.9593103  0.9551724
  0.3  2          0.8               0.75       150      0.9948228  0.9841379  0.9586207
  0.3  2          0.8               1.00        50      0.9792699  0.9165517  0.9434483
  0.3  2          0.8               1.00       100      0.9914780  0.9544828  0.9558621
  0.3  2          0.8               1.00       150      0.9948347  0.9786207  0.9641379
  0.3  3          0.6               0.50        50      0.9892961  0.9448276  0.9475862
  0.3  3          0.6               0.50       100      0.9946683  0.9841379  0.9593103
  0.3  3          0.6               0.50       150      0.9958740  0.9924138  0.9648276
  0.3  3          0.6               0.75        50      0.9889834  0.9517241  0.9496552
  0.3  3          0.6               0.75       100      0.9949298  0.9848276  0.9558621
  0.3  3          0.6               0.75       150      0.9961998  0.9917241  0.9641379
  0.3  3          0.6               1.00        50      0.9900571  0.9420690  0.9510345
  0.3  3          0.6               1.00       100      0.9956373  0.9862069  0.9600000
  0.3  3          0.6               1.00       150      0.9969631  0.9896552  0.9641379
  0.3  3          0.8               0.50        50      0.9892818  0.9475862  0.9468966
  0.3  3          0.8               0.50       100      0.9954221  0.9875862  0.9634483
  0.3  3          0.8               0.50       150      0.9968704  0.9931034  0.9641379
  0.3  3          0.8               0.75        50      0.9910166  0.9627586  0.9462069
  0.3  3          0.8               0.75       100      0.9958882  0.9875862  0.9641379
  0.3  3          0.8               0.75       150      0.9972319  0.9917241  0.9675862
  0.3  3          0.8               1.00        50      0.9910583  0.9489655  0.9510345
  0.3  3          0.8               1.00       100      0.9963210  0.9862069  0.9586207
  0.3  3          0.8               1.00       150      0.9977051  0.9937931  0.9648276
  0.4  1          0.6               0.50        50      0.9648977  0.8779310  0.9289655
  0.4  1          0.6               0.50       100      0.9814590  0.9262069  0.9441379
  0.4  1          0.6               0.50       150      0.9878430  0.9496552  0.9496552
  0.4  1          0.6               0.75        50      0.9646266  0.8765517  0.9282759
  0.4  1          0.6               0.75       100      0.9797717  0.9193103  0.9413793
  0.4  1          0.6               0.75       150      0.9875172  0.9365517  0.9524138
  0.4  1          0.6               1.00        50      0.9638407  0.8793103  0.9234483
  0.4  1          0.6               1.00       100      0.9791320  0.9124138  0.9420690
  0.4  1          0.6               1.00       150      0.9867337  0.9282759  0.9517241
  0.4  1          0.8               0.50        50      0.9631106  0.8793103  0.9200000
  0.4  1          0.8               0.50       100      0.9808062  0.9241379  0.9434483
  0.4  1          0.8               0.50       150      0.9874863  0.9344828  0.9455172
  0.4  1          0.8               0.75        50      0.9640428  0.8675862  0.9255172
  0.4  1          0.8               0.75       100      0.9813543  0.9103448  0.9496552
  0.4  1          0.8               0.75       150      0.9872390  0.9386207  0.9482759
  0.4  1          0.8               1.00        50      0.9634055  0.8779310  0.9282759
  0.4  1          0.8               1.00       100      0.9787872  0.9082759  0.9413793
  0.4  1          0.8               1.00       150      0.9867705  0.9365517  0.9510345
  0.4  2          0.6               0.50        50      0.9833032  0.9248276  0.9434483
  0.4  2          0.6               0.50       100      0.9921332  0.9703448  0.9551724
  0.4  2          0.6               0.50       150      0.9941379  0.9862069  0.9613793
  0.4  2          0.6               0.75        50      0.9836659  0.9317241  0.9448276
  0.4  2          0.6               0.75       100      0.9929822  0.9710345  0.9572414
  0.4  2          0.6               0.75       150      0.9956742  0.9862069  0.9606897
  0.4  2          0.6               1.00        50      0.9842782  0.9331034  0.9434483
  0.4  2          0.6               1.00       100      0.9932759  0.9724138  0.9551724
  0.4  2          0.6               1.00       150      0.9955113  0.9875862  0.9620690
  0.4  2          0.8               0.50        50      0.9848466  0.9268966  0.9475862
  0.4  2          0.8               0.50       100      0.9931605  0.9682759  0.9613793
  0.4  2          0.8               0.50       150      0.9943353  0.9868966  0.9606897
  0.4  2          0.8               0.75        50      0.9855220  0.9406897  0.9441379
  0.4  2          0.8               0.75       100      0.9936956  0.9751724  0.9572414
  0.4  2          0.8               0.75       150      0.9953603  0.9868966  0.9593103
  0.4  2          0.8               1.00        50      0.9850713  0.9324138  0.9462069
  0.4  2          0.8               1.00       100      0.9940095  0.9751724  0.9524138
  0.4  2          0.8               1.00       150      0.9963139  0.9882759  0.9641379
  0.4  3          0.6               0.50        50      0.9906801  0.9696552  0.9420690
  0.4  3          0.6               0.50       100      0.9952985  0.9903448  0.9551724
  0.4  3          0.6               0.50       150      0.9957574  0.9951724  0.9579310
  0.4  3          0.6               0.75        50      0.9936504  0.9710345  0.9551724
  0.4  3          0.6               0.75       100      0.9973817  0.9896552  0.9600000
  0.4  3          0.6               0.75       150      0.9977741  0.9924138  0.9627586
  0.4  3          0.6               1.00        50      0.9925208  0.9682759  0.9531034
  0.4  3          0.6               1.00       100      0.9963615  0.9910345  0.9648276
  0.4  3          0.6               1.00       150      0.9976671  0.9951724  0.9703448
  0.4  3          0.8               0.50        50      0.9915149  0.9717241  0.9537931
  0.4  3          0.8               0.50       100      0.9955553  0.9903448  0.9600000
  0.4  3          0.8               0.50       150      0.9959620  0.9944828  0.9627586
  0.4  3          0.8               0.75        50      0.9930844  0.9655172  0.9572414
  0.4  3          0.8               0.75       100      0.9969394  0.9910345  0.9641379
  0.4  3          0.8               0.75       150      0.9972533  0.9958621  0.9655172
  0.4  3          0.8               1.00        50      0.9937895  0.9675862  0.9551724
  0.4  3          0.8               1.00       100      0.9975981  0.9924138  0.9627586
  0.4  3          0.8               1.00       150      0.9985446  0.9951724  0.9682759

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 150, max_depth = 3, eta = 0.4, gamma = 0, colsample_bytree = 0.8, min_child_weight = 1 and subsample = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     49.8      1.6
  positive      0.2     48.4
                            
 Accuracy (average) : 0.9817

[1] "TRAIN accuracy: 0.981724137931035"
[1] "TRAIN +precision: 0.995038979447201"
[1] "TRAIN -precision: 0.96910678307589"
[1] "TRAIN specifity: 0.995172413793103"
[1] "TRAIN sensitivity: 0.968275862068966"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       55       16
            positive       21      481
[1] "TEST accuracy: 0.93542757417103"
[1] "TEST +precision: 0.958167330677291"
[1] "TEST -precision: 0.774647887323944"
[1] "TEST specifity: 0.723684210526316"
[1] "TEST sensitivity: 0.967806841046278"

[1] "DATASET NAME: ElSur_Bi_IR_1"
[1] "TRAIN INSTANCES: 2326"
[1] "TEST INSTANCES: 396"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 5.15299391746521"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

2326 samples
 617 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1860, 1861, 1861, 1861, 1861 
Resampling results:

  ROC        Sens  Spec     
  0.9957008  1     0.9922599

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     50.0      0.4
  positive      0.0     49.6
                            
 Accuracy (average) : 0.9961

[1] "TRAIN accuracy: 0.996130696474635"
[1] "TRAIN +precision: 1"
[1] "TRAIN -precision: 0.992320819112628"
[1] "TRAIN specifity: 1"
[1] "TRAIN sensitivity: 0.992261392949269"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        1        1
            positive        5      389
[1] "TEST accuracy: 0.984848484848485"
[1] "TEST +precision: 0.987309644670051"
[1] "TEST -precision: 0.5"
[1] "TEST specifity: 0.166666666666667"
[1] "TEST sensitivity: 0.997435897435897"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 2.75363263289134"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

2326 samples
 617 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1861, 1861, 1860, 1861, 1861 
Resampling results across tuning parameters:

  C      M  ROC        Sens  Spec     
  0.010  1  0.9860213  1     0.9647403
  0.010  2  0.9860213  1     0.9647403
  0.010  3  0.9858870  1     0.9630198
  0.255  1  0.9871554  1     0.9673228
  0.255  2  0.9871554  1     0.9673228
  0.255  3  0.9864456  1     0.9647440
  0.500  1  0.9891218  1     0.9716220
  0.500  2  0.9891218  1     0.9716220
  0.500  3  0.9882234  1     0.9699016

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.5 and M = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     50.0      1.4
  positive      0.0     48.6
                            
 Accuracy (average) : 0.9858

[1] "TRAIN accuracy: 0.985812553740327"
[1] "TRAIN +precision: 1"
[1] "TRAIN -precision: 0.972408026755853"
[1] "TRAIN specifity: 1"
[1] "TRAIN sensitivity: 0.971625107480653"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        2        2
            positive        4      388
[1] "TEST accuracy: 0.984848484848485"
[1] "TEST +precision: 0.989795918367347"
[1] "TEST -precision: 0.5"
[1] "TEST specifity: 0.333333333333333"
[1] "TEST sensitivity: 0.994871794871795"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 7.20855436722438"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

2326 samples
 617 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1861, 1861, 1861, 1860, 1861 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens       Spec     
  0.3  1          0.6               0.50        50      0.9910997  0.9355594  0.9544509
  0.3  1          0.6               0.50       100      0.9960793  1.0000000  0.9707711
  0.3  1          0.6               0.50       150      0.9964744  1.0000000  0.9759324
  0.3  1          0.6               0.75        50      0.9917431  0.9381123  0.9647699
  0.3  1          0.6               0.75       100      0.9948993  1.0000000  0.9707785
  0.3  1          0.6               0.75       150      0.9961969  1.0000000  0.9716405
  0.3  1          0.6               1.00        50      0.9920971  0.9398513  0.9621726
  0.3  1          0.6               1.00       100      0.9956800  1.0000000  0.9699127
  0.3  1          0.6               1.00       150      0.9955373  1.0000000  0.9664718
  0.3  1          0.8               0.50        50      0.9902619  0.9466960  0.9467145
  0.3  1          0.8               0.50       100      0.9954432  1.0000000  0.9699201
  0.3  1          0.8               0.50       150      0.9960286  1.0000000  0.9750777
  0.3  1          0.8               0.75        50      0.9911072  0.9553648  0.9432625
  0.3  1          0.8               0.75       100      0.9947455  1.0000000  0.9724952
  0.3  1          0.8               0.75       150      0.9962387  1.0000000  0.9733536
  0.3  1          0.8               1.00        50      0.9916373  0.9407096  0.9596049
  0.3  1          0.8               1.00       100      0.9951356  1.0000000  0.9707748
  0.3  1          0.8               1.00       150      0.9949290  1.0000000  0.9664718
  0.3  2          0.6               0.50        50      0.9959414  1.0000000  0.9733573
  0.3  2          0.6               0.50       100      0.9967552  1.0000000  0.9785112
  0.3  2          0.6               0.50       150      0.9971659  1.0000000  0.9776491
  0.3  2          0.6               0.75        50      0.9972521  1.0000000  0.9776528
  0.3  2          0.6               0.75       100      0.9968445  1.0000000  0.9802316
  0.3  2          0.6               0.75       150      0.9972621  1.0000000  0.9810789
  0.3  2          0.6               1.00        50      0.9952168  1.0000000  0.9733610
  0.3  2          0.6               1.00       100      0.9957604  1.0000000  0.9724841
  0.3  2          0.6               1.00       150      0.9960527  1.0000000  0.9759139
  0.3  2          0.8               0.50        50      0.9940091  1.0000000  0.9767981
  0.3  2          0.8               0.50       100      0.9959301  1.0000000  0.9759324
  0.3  2          0.8               0.50       150      0.9959343  1.0000000  0.9793695
  0.3  2          0.8               0.75        50      0.9951504  1.0000000  0.9733499
  0.3  2          0.8               0.75       100      0.9970993  1.0000000  0.9707711
  0.3  2          0.8               0.75       150      0.9970031  1.0000000  0.9742045
  0.3  2          0.8               1.00        50      0.9946279  1.0000000  0.9724952
  0.3  2          0.8               1.00       100      0.9947292  1.0000000  0.9690469
  0.3  2          0.8               1.00       150      0.9947497  1.0000000  0.9750592
  0.3  3          0.6               0.50        50      0.9970671  1.0000000  0.9759361
  0.3  3          0.6               0.50       100      0.9972288  1.0000000  0.9793658
  0.3  3          0.6               0.50       150      0.9968588  1.0000000  0.9819410
  0.3  3          0.6               0.75        50      0.9972406  1.0000000  0.9776454
  0.3  3          0.6               0.75       100      0.9972625  1.0000000  0.9785001
  0.3  3          0.6               0.75       150      0.9973548  1.0000000  0.9793695
  0.3  3          0.6               1.00        50      0.9981390  1.0000000  0.9724915
  0.3  3          0.6               1.00       100      0.9975916  1.0000000  0.9802242
  0.3  3          0.6               1.00       150      0.9976214  1.0000000  0.9810789
  0.3  3          0.8               0.50        50      0.9971736  1.0000000  0.9819483
  0.3  3          0.8               0.50       100      0.9974693  1.0000000  0.9810900
  0.3  3          0.8               0.50       150      0.9972991  1.0000000  0.9793732
  0.3  3          0.8               0.75        50      0.9965036  1.0000000  0.9742156
  0.3  3          0.8               0.75       100      0.9972106  1.0000000  0.9767870
  0.3  3          0.8               0.75       150      0.9969516  1.0000000  0.9793621
  0.3  3          0.8               1.00        50      0.9957702  1.0000000  0.9724841
  0.3  3          0.8               1.00       100      0.9954465  1.0000000  0.9759139
  0.3  3          0.8               1.00       150      0.9949543  1.0000000  0.9759213
  0.4  1          0.6               0.50        50      0.9936686  1.0000000  0.9518462
  0.4  1          0.6               0.50       100      0.9959607  1.0000000  0.9724952
  0.4  1          0.6               0.50       150      0.9964077  1.0000000  0.9759324
  0.4  1          0.6               0.75        50      0.9943796  0.9922747  0.9621837
  0.4  1          0.6               0.75       100      0.9955213  1.0000000  0.9724952
  0.4  1          0.6               0.75       150      0.9968074  1.0000000  0.9750666
  0.4  1          0.6               1.00        50      0.9939962  1.0000000  0.9690654
  0.4  1          0.6               1.00       100      0.9946290  1.0000000  0.9690506
  0.4  1          0.6               1.00       150      0.9952961  1.0000000  0.9690469
  0.4  1          0.8               0.50        50      0.9940598  0.9913793  0.9673524
  0.4  1          0.8               0.50       100      0.9956774  1.0000000  0.9759287
  0.4  1          0.8               0.50       150      0.9967297  1.0000000  0.9759324
  0.4  1          0.8               0.75        50      0.9930843  1.0000000  0.9733610
  0.4  1          0.8               0.75       100      0.9955299  1.0000000  0.9707748
  0.4  1          0.8               0.75       150      0.9956790  1.0000000  0.9733499
  0.4  1          0.8               1.00        50      0.9936299  1.0000000  0.9656319
  0.4  1          0.8               1.00       100      0.9945867  1.0000000  0.9664718
  0.4  1          0.8               1.00       150      0.9947559  1.0000000  0.9707637
  0.4  2          0.6               0.50        50      0.9971030  1.0000000  0.9768018
  0.4  2          0.6               0.50       100      0.9967296  1.0000000  0.9785075
  0.4  2          0.6               0.50       150      0.9962964  1.0000000  0.9767870
  0.4  2          0.6               0.75        50      0.9964010  1.0000000  0.9759361
  0.4  2          0.6               0.75       100      0.9962856  1.0000000  0.9767907
  0.4  2          0.6               0.75       150      0.9963374  1.0000000  0.9802316
  0.4  2          0.6               1.00        50      0.9950396  1.0000000  0.9776565
  0.4  2          0.6               1.00       100      0.9959602  1.0000000  0.9767796
  0.4  2          0.6               1.00       150      0.9956196  1.0000000  0.9759213
  0.4  2          0.8               0.50        50      0.9962066  1.0000000  0.9767907
  0.4  2          0.8               0.50       100      0.9965705  1.0000000  0.9793695
  0.4  2          0.8               0.50       150      0.9965336  1.0000000  0.9810863
  0.4  2          0.8               0.75        50      0.9947970  1.0000000  0.9725026
  0.4  2          0.8               0.75       100      0.9961230  1.0000000  0.9750666
  0.4  2          0.8               0.75       150      0.9968780  1.0000000  0.9750629
  0.4  2          0.8               1.00        50      0.9943789  1.0000000  0.9742119
  0.4  2          0.8               1.00       100      0.9943698  1.0000000  0.9733388
  0.4  2          0.8               1.00       150      0.9945064  1.0000000  0.9742008
  0.4  3          0.6               0.50        50      0.9979984  1.0000000  0.9810937
  0.4  3          0.6               0.50       100      0.9981094  1.0000000  0.9836651
  0.4  3          0.6               0.50       150      0.9979836  1.0000000  0.9828067
  0.4  3          0.6               0.75        50      0.9972547  1.0000000  0.9802242
  0.4  3          0.6               0.75       100      0.9974656  1.0000000  0.9741971
  0.4  3          0.6               0.75       150      0.9971437  1.0000000  0.9793658
  0.4  3          0.6               1.00        50      0.9970031  1.0000000  0.9742082
  0.4  3          0.6               1.00       100      0.9971104  1.0000000  0.9759250
  0.4  3          0.6               1.00       150      0.9968255  1.0000000  0.9776454
  0.4  3          0.8               0.50        50      0.9965746  1.0000000  0.9793732
  0.4  3          0.8               0.50       100      0.9974143  1.0000000  0.9785149
  0.4  3          0.8               0.50       150      0.9969254  1.0000000  0.9828141
  0.4  3          0.8               0.75        50      0.9971289  1.0000000  0.9767907
  0.4  3          0.8               0.75       100      0.9972547  1.0000000  0.9793658
  0.4  3          0.8               0.75       150      0.9972695  1.0000000  0.9793658
  0.4  3          0.8               1.00        50      0.9954171  1.0000000  0.9716257
  0.4  3          0.8               1.00       100      0.9949244  1.0000000  0.9776417
  0.4  3          0.8               1.00       150      0.9956273  1.0000000  0.9785075

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 50, max_depth = 3, eta = 0.3, gamma = 0, colsample_bytree = 0.6, min_child_weight = 1 and subsample = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     50.0      1.4
  positive      0.0     48.6
                            
 Accuracy (average) : 0.9862

[1] "TRAIN accuracy: 0.986242476354256"
[1] "TRAIN +precision: 1"
[1] "TRAIN -precision: 0.973221757322176"
[1] "TRAIN specifity: 1"
[1] "TRAIN sensitivity: 0.972484952708512"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        0        5
            positive        6      385
[1] "TEST accuracy: 0.972222222222222"
[1] "TEST +precision: 0.9846547314578"
[1] "TEST -precision: 0"
[1] "TEST specifity: 0"
[1] "TEST sensitivity: 0.987179487179487"

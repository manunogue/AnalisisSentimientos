[1] "DATASET NAME: HRM_Uni_IR_0"
[1] "TRAIN INSTANCES: 336"
[1] "TEST INSTANCES: 112"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 2.72204780578613"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

336 samples
747 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 269, 269, 268, 269, 269 
Resampling results:

  ROC        Sens       Spec     
  0.8989999  0.5777778  0.9724138

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative      7.7      2.4
  positive      5.7     84.2
                            
 Accuracy (average) : 0.9196

[1] "TRAIN accuracy: 0.919642857142857"
[1] "TRAIN +precision: 0.937086092715232"
[1] "TRAIN -precision: 0.764705882352941"
[1] "TRAIN specifity: 0.577777777777778"
[1] "TRAIN sensitivity: 0.972508591065292"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       10        6
            positive        8       88
[1] "TEST accuracy: 0.875"
[1] "TEST +precision: 0.916666666666667"
[1] "TEST -precision: 0.625"
[1] "TEST specifity: 0.555555555555556"
[1] "TEST sensitivity: 0.936170212765957"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 1.0187956849734"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

336 samples
747 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 269, 269, 269, 268, 269 
Resampling results across tuning parameters:

  C      M  ROC        Sens        Spec     
  0.010  1  0.5439022  0.08888889  0.9793688
  0.010  2  0.5474836  0.11111111  0.9862069
  0.010  3  0.5348399  0.11111111  0.9689655
  0.255  1  0.5715923  0.28888889  0.9690240
  0.255  2  0.5827002  0.15555556  0.9759205
  0.255  3  0.5921943  0.17777778  0.9690240
  0.500  1  0.6032794  0.28888889  0.9483343
  0.500  2  0.6678356  0.24444444  0.9552309
  0.500  3  0.6498571  0.24444444  0.9690240

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.5 and M = 2.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative      3.3      3.9
  positive     10.1     82.7
                            
 Accuracy (average) : 0.8601

[1] "TRAIN accuracy: 0.860119047619048"
[1] "TRAIN +precision: 0.891025641025641"
[1] "TRAIN -precision: 0.458333333333333"
[1] "TRAIN specifity: 0.244444444444444"
[1] "TRAIN sensitivity: 0.9553264604811"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        7        6
            positive       11       88
[1] "TEST accuracy: 0.848214285714286"
[1] "TEST +precision: 0.888888888888889"
[1] "TEST -precision: 0.538461538461538"
[1] "TEST specifity: 0.388888888888889"
[1] "TEST sensitivity: 0.936170212765957"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 1.88162323236465"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

336 samples
747 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 268, 269, 269, 269, 269 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens       Spec     
  0.3  1          0.6               0.50        50      0.8944607  0.4444444  0.9828171
  0.3  1          0.6               0.50       100      0.8730762  0.4444444  0.9724722
  0.3  1          0.6               0.50       150      0.8712514  0.4666667  0.9759205
  0.3  1          0.6               0.75        50      0.8842685  0.3777778  0.9862069
  0.3  1          0.6               0.75       100      0.8807098  0.4444444  0.9862653
  0.3  1          0.6               0.75       150      0.8682901  0.4666667  0.9828171
  0.3  1          0.6               1.00        50      0.8939931  0.3333333  0.9931034
  0.3  1          0.6               1.00       100      0.8983635  0.4222222  0.9896552
  0.3  1          0.6               1.00       150      0.8934281  0.4666667  0.9862653
  0.3  1          0.8               0.50        50      0.8977141  0.4666667  0.9897136
  0.3  1          0.8               0.50       100      0.8923826  0.4222222  0.9793688
  0.3  1          0.8               0.50       150      0.8858757  0.4666667  0.9759205
  0.3  1          0.8               0.75        50      0.8851744  0.3777778  0.9896552
  0.3  1          0.8               0.75       100      0.8817326  0.4444444  0.9828171
  0.3  1          0.8               0.75       150      0.8828106  0.5111111  0.9759205
  0.3  1          0.8               1.00        50      0.8880771  0.3333333  0.9931034
  0.3  1          0.8               1.00       100      0.8925125  0.4444444  0.9931034
  0.3  1          0.8               1.00       150      0.8909864  0.4666667  0.9828171
  0.3  2          0.6               0.50        50      0.8933567  0.4666667  0.9827586
  0.3  2          0.6               0.50       100      0.8708617  0.5111111  0.9690824
  0.3  2          0.6               0.50       150      0.8682382  0.5111111  0.9759790
  0.3  2          0.6               0.75        50      0.9050945  0.4888889  0.9897136
  0.3  2          0.6               0.75       100      0.8976752  0.4888889  0.9828171
  0.3  2          0.6               0.75       150      0.8911358  0.5111111  0.9724722
  0.3  2          0.6               1.00        50      0.8820800  0.4000000  0.9897136
  0.3  2          0.6               1.00       100      0.8865186  0.4666667  0.9793688
  0.3  2          0.6               1.00       150      0.8876615  0.4666667  0.9759205
  0.3  2          0.8               0.50        50      0.8644360  0.4222222  0.9827586
  0.3  2          0.8               0.50       100      0.8627249  0.4000000  0.9656341
  0.3  2          0.8               0.50       150      0.8624456  0.4000000  0.9759205
  0.3  2          0.8               0.75        50      0.8796578  0.3777778  0.9897136
  0.3  2          0.8               0.75       100      0.8866874  0.4444444  0.9897136
  0.3  2          0.8               0.75       150      0.8862199  0.4444444  0.9759205
  0.3  2          0.8               1.00        50      0.9054257  0.4444444  1.0000000
  0.3  2          0.8               1.00       100      0.8915254  0.4444444  0.9724722
  0.3  2          0.8               1.00       150      0.8922917  0.4444444  0.9690240
  0.3  3          0.6               0.50        50      0.8789467  0.4222222  0.9965517
  0.3  3          0.6               0.50       100      0.8838301  0.4222222  0.9862653
  0.3  3          0.6               0.50       150      0.8742516  0.4888889  0.9862653
  0.3  3          0.6               0.75        50      0.8926619  0.4666667  0.9862653
  0.3  3          0.6               0.75       100      0.8842457  0.4444444  0.9828171
  0.3  3          0.6               0.75       150      0.8819404  0.4666667  0.9828171
  0.3  3          0.6               1.00        50      0.8905156  0.4222222  0.9862653
  0.3  3          0.6               1.00       100      0.8860575  0.4888889  0.9724722
  0.3  3          0.6               1.00       150      0.8860445  0.5111111  0.9724722
  0.3  3          0.8               0.50        50      0.8819079  0.4666667  0.9897136
  0.3  3          0.8               0.50       100      0.8730697  0.5333333  0.9793688
  0.3  3          0.8               0.50       150      0.8670044  0.4666667  0.9759790
  0.3  3          0.8               0.75        50      0.8949802  0.4222222  0.9897136
  0.3  3          0.8               0.75       100      0.8904085  0.4444444  0.9897136
  0.3  3          0.8               0.75       150      0.8900123  0.4666667  0.9828171
  0.3  3          0.8               1.00        50      0.8952984  0.4444444  0.9931619
  0.3  3          0.8               1.00       100      0.8876356  0.4444444  0.9828171
  0.3  3          0.8               1.00       150      0.8876226  0.4666667  0.9759205
  0.4  1          0.6               0.50        50      0.8867816  0.4222222  0.9828171
  0.4  1          0.6               0.50       100      0.8701669  0.4888889  0.9759790
  0.4  1          0.6               0.50       150      0.8697708  0.4444444  0.9656341
  0.4  1          0.6               0.75        50      0.8936262  0.4000000  0.9931034
  0.4  1          0.6               0.75       100      0.8930450  0.4666667  0.9828171
  0.4  1          0.6               0.75       150      0.8884148  0.4444444  0.9793688
  0.4  1          0.6               1.00        50      0.8977661  0.3777778  0.9965517
  0.4  1          0.6               1.00       100      0.8983700  0.4444444  0.9896552
  0.4  1          0.6               1.00       150      0.8934476  0.4444444  0.9794272
  0.4  1          0.8               0.50        50      0.8722157  0.4888889  0.9793103
  0.4  1          0.8               0.50       100      0.8891876  0.5111111  0.9793103
  0.4  1          0.8               0.50       150      0.8739074  0.4666667  0.9828171
  0.4  1          0.8               0.75        50      0.9030554  0.4222222  0.9965517
  0.4  1          0.8               0.75       100      0.8885545  0.5333333  0.9759205
  0.4  1          0.8               0.75       150      0.8696669  0.4888889  0.9725307
  0.4  1          0.8               1.00        50      0.8858757  0.3777778  0.9965517
  0.4  1          0.8               1.00       100      0.8925645  0.4444444  0.9862653
  0.4  1          0.8               1.00       150      0.8872135  0.4666667  0.9794272
  0.4  2          0.6               0.50        50      0.8685759  0.4000000  0.9793103
  0.4  2          0.6               0.50       100      0.8704916  0.4666667  0.9724722
  0.4  2          0.6               0.50       150      0.8681603  0.4222222  0.9793103
  0.4  2          0.6               0.75        50      0.8896747  0.4222222  0.9862069
  0.4  2          0.6               0.75       100      0.8827002  0.5111111  0.9690240
  0.4  2          0.6               0.75       150      0.8765959  0.4888889  0.9690240
  0.4  2          0.6               1.00        50      0.8977628  0.4444444  0.9896552
  0.4  2          0.6               1.00       100      0.8949477  0.4666667  0.9828171
  0.4  2          0.6               1.00       150      0.8987402  0.4888889  0.9759205
  0.4  2          0.8               0.50        50      0.8502695  0.4666667  0.9655757
  0.4  2          0.8               0.50       100      0.8502305  0.4888889  0.9690240
  0.4  2          0.8               0.50       150      0.8590363  0.4888889  0.9621274
  0.4  2          0.8               0.75        50      0.8884278  0.4444444  0.9793688
  0.4  2          0.8               0.75       100      0.8780895  0.4444444  0.9793688
  0.4  2          0.8               0.75       150      0.8769466  0.4666667  0.9828171
  0.4  2          0.8               1.00        50      0.8947562  0.4444444  0.9897136
  0.4  2          0.8               1.00       100      0.8854081  0.4444444  0.9759205
  0.4  2          0.8               1.00       150      0.8880382  0.4888889  0.9690240
  0.4  3          0.6               0.50        50      0.8797325  0.4444444  0.9828755
  0.4  3          0.6               0.50       100      0.8700565  0.4666667  0.9828755
  0.4  3          0.6               0.50       150      0.8708812  0.4666667  0.9759790
  0.4  3          0.6               0.75        50      0.8711994  0.4888889  0.9862069
  0.4  3          0.6               0.75       100      0.8754594  0.4888889  0.9793688
  0.4  3          0.6               0.75       150      0.8807975  0.4666667  0.9793688
  0.4  3          0.6               1.00        50      0.8929411  0.4222222  0.9828171
  0.4  3          0.6               1.00       100      0.8852393  0.4666667  0.9759205
  0.4  3          0.6               1.00       150      0.8814598  0.4444444  0.9724722
  0.4  3          0.8               0.50        50      0.8815767  0.4444444  0.9724722
  0.4  3          0.8               0.50       100      0.8708163  0.4666667  0.9656341
  0.4  3          0.8               0.50       150      0.8723164  0.4888889  0.9725307
  0.4  3          0.8               0.75        50      0.8851094  0.4888889  0.9828171
  0.4  3          0.8               0.75       100      0.8865965  0.4888889  0.9793688
  0.4  3          0.8               0.75       150      0.8876940  0.4888889  0.9759205
  0.4  3          0.8               1.00        50      0.8785246  0.4888889  0.9759205
  0.4  3          0.8               1.00       100      0.8807650  0.4888889  0.9759205
  0.4  3          0.8               1.00       150      0.8891486  0.4888889  0.9690240

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 50, max_depth = 2, eta = 0.3, gamma = 0, colsample_bytree = 0.8, min_child_weight = 1 and subsample = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative      6.0      0.0
  positive      7.4     86.6
                            
 Accuracy (average) : 0.9256

[1] "TRAIN accuracy: 0.925595238095238"
[1] "TRAIN +precision: 0.920886075949367"
[1] "TRAIN -precision: 1"
[1] "TRAIN specifity: 0.444444444444444"
[1] "TRAIN sensitivity: 1"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       10        1
            positive        8       93
[1] "TEST accuracy: 0.919642857142857"
[1] "TEST +precision: 0.920792079207921"
[1] "TEST -precision: 0.909090909090909"
[1] "TEST specifity: 0.555555555555556"
[1] "TEST sensitivity: 0.98936170212766"

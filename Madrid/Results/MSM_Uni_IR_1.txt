[1] "DATASET NAME: MSM_Uni_IR_1"
[1] "TRAIN INSTANCES: 3472"
[1] "TEST INSTANCES: 600"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 20.1676948070526"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

3472 samples
 656 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 2778, 2778, 2777, 2778, 2777 
Resampling results:

  ROC  Sens  Spec     
  1    1     0.9953924

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     50.0      0.2
  positive      0.0     49.8
                            
 Accuracy (average) : 0.9977

[1] "TRAIN accuracy: 0.997695852534562"
[1] "TRAIN +precision: 1"
[1] "TRAIN -precision: 0.995412844036697"
[1] "TRAIN specifity: 1"
[1] "TRAIN sensitivity: 0.995391705069124"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        7        4
            positive       13      576
[1] "TEST accuracy: 0.971666666666667"
[1] "TEST +precision: 0.977928692699491"
[1] "TEST -precision: 0.636363636363636"
[1] "TEST specifity: 0.35"
[1] "TEST sensitivity: 0.993103448275862"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 5.50485238234202"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

3472 samples
 656 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 2778, 2778, 2777, 2777, 2778 
Resampling results across tuning parameters:

  C      M  ROC        Sens  Spec     
  0.010  1  0.9866009  1     0.9683146
  0.010  2  0.9865469  1     0.9683146
  0.010  3  0.9865469  1     0.9683146
  0.255  1  0.9885167  1     0.9706217
  0.255  2  0.9884710  1     0.9706217
  0.255  3  0.9884710  1     0.9706217
  0.500  1  0.9885761  1     0.9763772
  0.500  2  0.9885304  1     0.9763772
  0.500  3  0.9892974  1     0.9740750

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.5 and M = 3.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     50.0      1.3
  positive      0.0     48.7
                           
 Accuracy (average) : 0.987

[1] "TRAIN accuracy: 0.987039170506912"
[1] "TRAIN +precision: 1"
[1] "TRAIN -precision: 0.974733295901179"
[1] "TRAIN specifity: 1"
[1] "TRAIN sensitivity: 0.974078341013825"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        9       14
            positive       11      566
[1] "TEST accuracy: 0.958333333333333"
[1] "TEST +precision: 0.980935875216638"
[1] "TEST -precision: 0.391304347826087"
[1] "TEST specifity: 0.45"
[1] "TEST sensitivity: 0.975862068965517"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 10.2097428997358"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

3472 samples
 656 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 2776, 2778, 2778, 2778, 2778 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens       Spec     
  0.3  1          0.6               0.50        50      0.9802171  0.9424062  0.9124549
  0.3  1          0.6               0.50       100      0.9921991  0.9827139  0.9447034
  0.3  1          0.6               0.50       150      0.9950189  0.9827139  0.9544934
  0.3  1          0.6               0.75        50      0.9780947  0.9228196  0.9222333
  0.3  1          0.6               0.75       100      0.9915178  0.9827139  0.9504654
  0.3  1          0.6               0.75       150      0.9953913  0.9827139  0.9602587
  0.3  1          0.6               1.00        50      0.9816027  0.9533390  0.9274140
  0.3  1          0.6               1.00       100      0.9919365  0.9769668  0.9544967
  0.3  1          0.6               1.00       150      0.9951035  0.9827139  0.9654444
  0.3  1          0.8               0.50        50      0.9770594  0.9406390  0.9124631
  0.3  1          0.8               0.50       100      0.9916577  0.9827139  0.9464341
  0.3  1          0.8               0.50       150      0.9952278  0.9844380  0.9614081
  0.3  1          0.8               0.75        50      0.9779037  0.9372122  0.9205042
  0.3  1          0.8               0.75       100      0.9908731  0.9827139  0.9498907
  0.3  1          0.8               0.75       150      0.9951975  0.9827139  0.9608318
  0.3  1          0.8               1.00        50      0.9800634  0.9308606  0.9233794
  0.3  1          0.8               1.00       100      0.9918762  0.9827139  0.9544983
  0.3  1          0.8               1.00       150      0.9950206  0.9827139  0.9654411
  0.3  2          0.6               0.50        50      0.9937820  0.9844380  0.9504687
  0.3  2          0.6               0.50       100      0.9981541  1.0000000  0.9746613
  0.3  2          0.6               0.50       150      0.9985290  1.0000000  0.9781145
  0.3  2          0.6               0.75        50      0.9948064  0.9827139  0.9579483
  0.3  2          0.6               0.75       100      0.9985860  1.0000000  0.9729272
  0.3  2          0.6               0.75       150      0.9995632  1.0000000  0.9792673
  0.3  2          0.6               1.00        50      0.9952882  0.9827139  0.9665954
  0.3  2          0.6               1.00       100      0.9991184  1.0000000  0.9798403
  0.3  2          0.6               1.00       150      0.9996645  1.0000000  0.9827205
  0.3  2          0.8               0.50        50      0.9955412  0.9861721  0.9591043
  0.3  2          0.8               0.50       100      0.9985563  1.0000000  0.9723475
  0.3  2          0.8               0.50       150      0.9995467  1.0000000  0.9798420
  0.3  2          0.8               0.75        50      0.9948853  0.9827139  0.9562274
  0.3  2          0.8               0.75       100      0.9985758  0.9942363  0.9752344
  0.3  2          0.8               0.75       150      0.9997559  1.0000000  0.9821458
  0.3  2          0.8               1.00        50      0.9954421  0.9827139  0.9660125
  0.3  2          0.8               1.00       100      0.9986735  1.0000000  0.9740800
  0.3  2          0.8               1.00       150      0.9993223  1.0000000  0.9827205
  0.3  3          0.6               0.50        50      0.9983029  1.0000000  0.9689009
  0.3  3          0.6               0.50       100      0.9995885  1.0000000  0.9827222
  0.3  3          0.6               0.50       150      0.9997259  1.0000000  0.9838733
  0.3  3          0.6               0.75        50      0.9986387  0.9907781  0.9735086
  0.3  3          0.6               0.75       100      0.9997425  1.0000000  0.9798453
  0.3  3          0.6               0.75       150      0.9999801  1.0000000  0.9832969
  0.3  3          0.6               1.00        50      0.9986824  1.0000000  0.9717761
  0.3  3          0.6               1.00       100      0.9996645  1.0000000  0.9838716
  0.3  3          0.6               1.00       150      0.9998771  1.0000000  0.9861771
  0.3  3          0.8               0.50        50      0.9981508  1.0000000  0.9711998
  0.3  3          0.8               0.50       100      0.9997824  1.0000000  0.9809964
  0.3  3          0.8               0.50       150      0.9996977  1.0000000  0.9827222
  0.3  3          0.8               0.75        50      0.9985419  1.0000000  0.9717778
  0.3  3          0.8               0.75       100      0.9996979  1.0000000  0.9838716
  0.3  3          0.8               0.75       150      0.9998491  1.0000000  0.9844496
  0.3  3          0.8               1.00        50      0.9990717  0.9965418  0.9740816
  0.3  3          0.8               1.00       100      0.9997691  1.0000000  0.9856007
  0.3  3          0.8               1.00       150      0.9999369  1.0000000  0.9855991
  0.4  1          0.6               0.50        50      0.9847504  0.9533472  0.9251284
  0.4  1          0.6               0.50       100      0.9941173  0.9827139  0.9498907
  0.4  1          0.6               0.50       150      0.9961777  0.9844380  0.9625592
  0.4  1          0.6               0.75        50      0.9859038  0.9602355  0.9383633
  0.4  1          0.6               0.75       100      0.9942388  0.9827139  0.9556444
  0.4  1          0.6               0.75       150      0.9965443  0.9873199  0.9683179
  0.4  1          0.6               1.00        50      0.9856525  0.9613982  0.9400908
  0.4  1          0.6               1.00       100      0.9943985  0.9827139  0.9579549
  0.4  1          0.6               1.00       150      0.9967999  0.9827139  0.9677399
  0.4  1          0.8               0.50        50      0.9843361  0.9613982  0.9256865
  0.4  1          0.8               0.50       100      0.9942485  0.9827139  0.9527725
  0.4  1          0.8               0.50       150      0.9962526  0.9965418  0.9642883
  0.4  1          0.8               0.75        50      0.9836270  0.9608334  0.9343304
  0.4  1          0.8               0.75       100      0.9942842  0.9827139  0.9539220
  0.4  1          0.8               0.75       150      0.9963822  0.9873199  0.9706267
  0.4  1          0.8               1.00        50      0.9867186  0.9677382  0.9389397
  0.4  1          0.8               1.00       100      0.9944848  0.9827139  0.9579549
  0.4  1          0.8               1.00       150      0.9969873  0.9827139  0.9717745
  0.4  2          0.6               0.50        50      0.9967712  0.9907781  0.9625675
  0.4  2          0.6               0.50       100      0.9993563  1.0000000  0.9786893
  0.4  2          0.6               0.50       150      0.9998505  1.0000000  0.9775382
  0.4  2          0.6               0.75        50      0.9971402  1.0000000  0.9671635
  0.4  2          0.6               0.75       100      0.9988755  1.0000000  0.9815645
  0.4  2          0.6               0.75       150      0.9994984  1.0000000  0.9838716
  0.4  2          0.6               1.00        50      0.9976819  0.9878963  0.9625625
  0.4  2          0.6               1.00       100      0.9990968  1.0000000  0.9798420
  0.4  2          0.6               1.00       150      0.9997940  1.0000000  0.9832986
  0.4  2          0.8               0.50        50      0.9968701  1.0000000  0.9608318
  0.4  2          0.8               0.50       100      0.9987653  1.0000000  0.9752344
  0.4  2          0.8               0.50       150      0.9996164  1.0000000  0.9832969
  0.4  2          0.8               0.75        50      0.9970554  0.9873199  0.9711981
  0.4  2          0.8               0.75       100      0.9991778  1.0000000  0.9821475
  0.4  2          0.8               0.75       150      0.9997558  1.0000000  0.9867518
  0.4  2          0.8               1.00        50      0.9972133  0.9925122  0.9671702
  0.4  2          0.8               1.00       100      0.9992975  1.0000000  0.9809931
  0.4  2          0.8               1.00       150      0.9996927  1.0000000  0.9809931
  0.4  3          0.6               0.50        50      0.9987801  1.0000000  0.9740833
  0.4  3          0.6               0.50       100      0.9998488  1.0000000  0.9827205
  0.4  3          0.6               0.50       150      0.9999302  1.0000000  0.9827189
  0.4  3          0.6               0.75        50      0.9989305  1.0000000  0.9821508
  0.4  3          0.6               0.75       100      0.9998655  1.0000000  0.9861804
  0.4  3          0.6               0.75       150      0.9999801  1.0000000  0.9844496
  0.4  3          0.6               1.00        50      0.9994536  1.0000000  0.9809947
  0.4  3          0.6               1.00       100      0.9999435  1.0000000  0.9855974
  0.4  3          0.6               1.00       150      0.9999701  1.0000000  0.9844480
  0.4  3          0.8               0.50        50      0.9990718  1.0000000  0.9752344
  0.4  3          0.8               0.50       100      0.9998638  1.0000000  0.9809964
  0.4  3          0.8               0.50       150      0.9999900  1.0000000  0.9827205
  0.4  3          0.8               0.75        50      0.9994502  1.0000000  0.9827205
  0.4  3          0.8               0.75       100      0.9999817  1.0000000  0.9861787
  0.4  3          0.8               0.75       150      0.9998040  1.0000000  0.9867518
  0.4  3          0.8               1.00        50      0.9989695  1.0000000  0.9821475
  0.4  3          0.8               1.00       100      0.9995010  1.0000000  0.9884826
  0.4  3          0.8               1.00       150      0.9993913  1.0000000  0.9884809

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 150, max_depth = 3, eta = 0.4, gamma = 0, colsample_bytree = 0.8, min_child_weight = 1 and subsample = 0.5.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     50.0      0.9
  positive      0.0     49.1
                            
 Accuracy (average) : 0.9914

[1] "TRAIN accuracy: 0.991359447004608"
[1] "TRAIN +precision: 1"
[1] "TRAIN -precision: 0.983012457531144"
[1] "TRAIN specifity: 1"
[1] "TRAIN sensitivity: 0.982718894009217"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       12       12
            positive        8      568
[1] "TEST accuracy: 0.966666666666667"
[1] "TEST +precision: 0.986111111111111"
[1] "TEST -precision: 0.5"
[1] "TEST specifity: 0.6"
[1] "TEST sensitivity: 0.979310344827586"

[1] "DATASET NAME: HRM_Bi_IR_0"
[1] "TRAIN INSTANCES: 336"
[1] "TEST INSTANCES: 112"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 1.89002799987793"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

336 samples
964 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 269, 268, 268, 269, 270 
Resampling results:

  ROC        Sens       Spec     
  0.8265208  0.1666667  0.9826376

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative      2.4      1.5
  positive     11.9     84.2
                            
 Accuracy (average) : 0.8661

[1] "TRAIN accuracy: 0.866071428571429"
[1] "TRAIN +precision: 0.876160990712074"
[1] "TRAIN -precision: 0.615384615384615"
[1] "TRAIN specifity: 0.166666666666667"
[1] "TRAIN sensitivity: 0.982638888888889"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        4        1
            positive       11       96
[1] "TEST accuracy: 0.892857142857143"
[1] "TEST +precision: 0.897196261682243"
[1] "TEST -precision: 0.8"
[1] "TEST specifity: 0.266666666666667"
[1] "TEST sensitivity: 0.989690721649485"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 1.27506888310115"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

336 samples
964 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 268, 269, 270, 269, 268 
Resampling results across tuning parameters:

  C      M  ROC        Sens  Spec     
  0.010  1  0.4982759  0     0.9965517
  0.010  2  0.4982759  0     0.9965517
  0.010  3  0.5000000  0     1.0000000
  0.255  1  0.5567877  0     0.9896552
  0.255  2  0.5369026  0     0.9931034
  0.255  3  0.5000000  0     1.0000000
  0.500  1  0.5498649  0     0.9965517
  0.500  2  0.4669177  0     0.9965517
  0.500  3  0.4982759  0     0.9965517

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.255 and M = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative      0.0      0.9
  positive     14.3     84.8
                            
 Accuracy (average) : 0.8482

[1] "TRAIN accuracy: 0.848214285714286"
[1] "TRAIN +precision: 0.855855855855856"
[1] "TRAIN -precision: 0"
[1] "TRAIN specifity: 0"
[1] "TRAIN sensitivity: 0.989583333333333"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        0        0
            positive       15       97
[1] "TEST accuracy: 0.866071428571429"
[1] "TEST +precision: 0.866071428571429"
[1] "TEST -precision: NaN"
[1] "TEST specifity: 0"
[1] "TEST sensitivity: 1"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 2.03115034898122"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

336 samples
964 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 269, 268, 270, 269, 268 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens  Spec     
  0.3  1          0.6               0.50        50      0.5625993  0     1.0000000
  0.3  1          0.6               0.50       100      0.6113457  0     1.0000000
  0.3  1          0.6               0.50       150      0.6091487  0     1.0000000
  0.3  1          0.6               0.75        50      0.6130093  0     1.0000000
  0.3  1          0.6               0.75       100      0.6212237  0     1.0000000
  0.3  1          0.6               0.75       150      0.6034560  0     1.0000000
  0.3  1          0.6               1.00        50      0.6439696  0     1.0000000
  0.3  1          0.6               1.00       100      0.6264785  0     1.0000000
  0.3  1          0.6               1.00       150      0.6216411  0     1.0000000
  0.3  1          0.8               0.50        50      0.5896152  0     1.0000000
  0.3  1          0.8               0.50       100      0.6006349  0     1.0000000
  0.3  1          0.8               0.50       150      0.6115292  0     1.0000000
  0.3  1          0.8               0.75        50      0.5884614  0     1.0000000
  0.3  1          0.8               0.75       100      0.5917393  0     1.0000000
  0.3  1          0.8               0.75       150      0.5960540  0     1.0000000
  0.3  1          0.8               1.00        50      0.6447842  0     1.0000000
  0.3  1          0.8               1.00       100      0.6237965  0     1.0000000
  0.3  1          0.8               1.00       150      0.6207085  0     1.0000000
  0.3  2          0.6               0.50        50      0.5952205  0     1.0000000
  0.3  2          0.6               0.50       100      0.5938002  0     1.0000000
  0.3  2          0.6               0.50       150      0.5960264  0     1.0000000
  0.3  2          0.6               0.75        50      0.6132537  0     1.0000000
  0.3  2          0.6               0.75       100      0.6103126  0     1.0000000
  0.3  2          0.6               0.75       150      0.6028137  0     1.0000000
  0.3  2          0.6               1.00        50      0.6295073  0     1.0000000
  0.3  2          0.6               1.00       100      0.6301801  0     1.0000000
  0.3  2          0.6               1.00       150      0.6246414  0     0.9965517
  0.3  2          0.8               0.50        50      0.5972108  0     1.0000000
  0.3  2          0.8               0.50       100      0.5911333  0     1.0000000
  0.3  2          0.8               0.50       150      0.5825445  0     1.0000000
  0.3  2          0.8               0.75        50      0.6217628  0     1.0000000
  0.3  2          0.8               0.75       100      0.6105300  0     0.9965517
  0.3  2          0.8               0.75       150      0.6173261  0     0.9965517
  0.3  2          0.8               1.00        50      0.6419574  0     1.0000000
  0.3  2          0.8               1.00       100      0.6343097  0     0.9965517
  0.3  2          0.8               1.00       150      0.6269406  0     0.9965517
  0.3  3          0.6               0.50        50      0.5642740  0     1.0000000
  0.3  3          0.6               0.50       100      0.5936220  0     1.0000000
  0.3  3          0.6               0.50       150      0.5816203  0     0.9965517
  0.3  3          0.6               0.75        50      0.6030419  0     1.0000000
  0.3  3          0.6               0.75       100      0.5884429  0     1.0000000
  0.3  3          0.6               0.75       150      0.6160802  0     0.9965517
  0.3  3          0.6               1.00        50      0.6195110  0     1.0000000
  0.3  3          0.6               1.00       100      0.6168377  0     0.9965517
  0.3  3          0.6               1.00       150      0.6160782  0     0.9965517
  0.3  3          0.8               0.50        50      0.6130537  0     1.0000000
  0.3  3          0.8               0.50       100      0.5849745  0     1.0000000
  0.3  3          0.8               0.50       150      0.6249785  0     1.0000000
  0.3  3          0.8               0.75        50      0.6195164  0     1.0000000
  0.3  3          0.8               0.75       100      0.6232665  0     0.9965517
  0.3  3          0.8               0.75       150      0.6272397  0     0.9965517
  0.3  3          0.8               1.00        50      0.6148703  0     1.0000000
  0.3  3          0.8               1.00       100      0.6181418  0     0.9965517
  0.3  3          0.8               1.00       150      0.6182903  0     0.9965517
  0.4  1          0.6               0.50        50      0.5894992  0     1.0000000
  0.4  1          0.6               0.50       100      0.5824639  0     1.0000000
  0.4  1          0.6               0.50       150      0.5882103  0     1.0000000
  0.4  1          0.6               0.75        50      0.6230527  0     1.0000000
  0.4  1          0.6               0.75       100      0.6177008  0     1.0000000
  0.4  1          0.6               0.75       150      0.6357972  0     1.0000000
  0.4  1          0.6               1.00        50      0.6162435  0     1.0000000
  0.4  1          0.6               1.00       100      0.6173859  0     1.0000000
  0.4  1          0.6               1.00       150      0.6169107  0     1.0000000
  0.4  1          0.8               0.50        50      0.6269426  0     1.0000000
  0.4  1          0.8               0.50       100      0.6113534  0     1.0000000
  0.4  1          0.8               0.50       150      0.5976780  0     1.0000000
  0.4  1          0.8               0.75        50      0.6317893  0     1.0000000
  0.4  1          0.8               0.75       100      0.5989655  0     1.0000000
  0.4  1          0.8               0.75       150      0.6164307  0     1.0000000
  0.4  1          0.8               1.00        50      0.6404399  0     1.0000000
  0.4  1          0.8               1.00       100      0.6301304  0     1.0000000
  0.4  1          0.8               1.00       150      0.6268391  0     1.0000000
  0.4  2          0.6               0.50        50      0.6118240  0     1.0000000
  0.4  2          0.6               0.50       100      0.5988586  0     1.0000000
  0.4  2          0.6               0.50       150      0.6046206  0     1.0000000
  0.4  2          0.6               0.75        50      0.5935098  0     1.0000000
  0.4  2          0.6               0.75       100      0.5830581  0     1.0000000
  0.4  2          0.6               0.75       150      0.6012566  0     0.9965517
  0.4  2          0.6               1.00        50      0.6177458  0     1.0000000
  0.4  2          0.6               1.00       100      0.6229371  0     0.9965517
  0.4  2          0.6               1.00       150      0.6191379  0     0.9965517
  0.4  2          0.8               0.50        50      0.6067057  0     1.0000000
  0.4  2          0.8               0.50       100      0.5948471  0     1.0000000
  0.4  2          0.8               0.50       150      0.6103630  0     1.0000000
  0.4  2          0.8               0.75        50      0.6062745  0     1.0000000
  0.4  2          0.8               0.75       100      0.6011733  0     1.0000000
  0.4  2          0.8               0.75       150      0.6180483  0     0.9965517
  0.4  2          0.8               1.00        50      0.6377976  0     1.0000000
  0.4  2          0.8               1.00       100      0.6339400  0     0.9965517
  0.4  2          0.8               1.00       150      0.6315336  0     0.9965517
  0.4  3          0.6               0.50        50      0.5645039  0     1.0000000
  0.4  3          0.6               0.50       100      0.6180930  0     1.0000000
  0.4  3          0.6               0.50       150      0.5766579  0     1.0000000
  0.4  3          0.6               0.75        50      0.6017591  0     1.0000000
  0.4  3          0.6               0.75       100      0.6252376  0     0.9965517
  0.4  3          0.6               0.75       150      0.6110123  0     0.9965517
  0.4  3          0.6               1.00        50      0.6400299  0     1.0000000
  0.4  3          0.6               1.00       100      0.6287178  0     0.9965517
  0.4  3          0.6               1.00       150      0.6260190  0     0.9965517
  0.4  3          0.8               0.50        50      0.5710987  0     1.0000000
  0.4  3          0.8               0.50       100      0.5923738  0     1.0000000
  0.4  3          0.8               0.50       150      0.6073923  0     1.0000000
  0.4  3          0.8               0.75        50      0.6225113  0     1.0000000
  0.4  3          0.8               0.75       100      0.5974807  0     0.9965517
  0.4  3          0.8               0.75       150      0.6071627  0     0.9965517
  0.4  3          0.8               1.00        50      0.6281613  0     0.9965517
  0.4  3          0.8               1.00       100      0.6164559  0     0.9965517
  0.4  3          0.8               1.00       150      0.6158853  0     0.9965517

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 50, max_depth = 1, eta = 0.3, gamma = 0, colsample_bytree = 0.8, min_child_weight = 1 and subsample = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative      0.0      0.0
  positive     14.3     85.7
                            
 Accuracy (average) : 0.8571

[1] "TRAIN accuracy: 0.857142857142857"
[1] "TRAIN +precision: 0.857142857142857"
[1] "TRAIN -precision: NaN"
[1] "TRAIN specifity: 0"
[1] "TRAIN sensitivity: 1"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        0        0
            positive       15       97
[1] "TEST accuracy: 0.866071428571429"
[1] "TEST +precision: 0.866071428571429"
[1] "TEST -precision: NaN"
[1] "TEST specifity: 0"
[1] "TEST sensitivity: 1"

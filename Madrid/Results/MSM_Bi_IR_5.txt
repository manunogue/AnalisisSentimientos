[1] "DATASET NAME: MSM_Bi_IR_5"
[1] "TRAIN INSTANCES: 2132"
[1] "TEST INSTANCES: 600"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 11.2918360233307"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

2132 samples
 583 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1706, 1705, 1706, 1705, 1706 
Resampling results:

  ROC        Sens       Spec     
  0.9923449  0.9648734  0.9780847

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     18.0      1.8
  positive      0.7     79.5
                            
 Accuracy (average) : 0.9756

[1] "TRAIN accuracy: 0.975609756097561"
[1] "TRAIN +precision: 0.991812865497076"
[1] "TRAIN -precision: 0.909952606635071"
[1] "TRAIN specifity: 0.964824120603015"
[1] "TRAIN sensitivity: 0.978085351787774"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        2       14
            positive       16      568
[1] "TEST accuracy: 0.95"
[1] "TEST +precision: 0.972602739726027"
[1] "TEST -precision: 0.125"
[1] "TEST specifity: 0.111111111111111"
[1] "TEST sensitivity: 0.975945017182131"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 2.25571141640345"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

2132 samples
 583 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1706, 1706, 1705, 1705, 1706 
Resampling results across tuning parameters:

  C      M  ROC        Sens       Spec     
  0.010  1  0.8041657  0.5303165  0.9832786
  0.010  2  0.7938531  0.5176582  0.9827023
  0.010  3  0.7910296  0.5125949  0.9850077
  0.255  1  0.8842566  0.6306329  0.9821176
  0.255  2  0.8844829  0.6306329  0.9821176
  0.255  3  0.8796985  0.6230696  0.9850044
  0.500  1  0.9077531  0.6431329  0.9901951
  0.500  2  0.9091995  0.6431329  0.9913478
  0.500  3  0.8958202  0.6230696  0.9896170

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.5 and M = 2.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     12.0      0.7
  positive      6.7     80.6
                            
 Accuracy (average) : 0.9264

[1] "TRAIN accuracy: 0.926360225140713"
[1] "TRAIN +precision: 0.923696937130575"
[1] "TRAIN -precision: 0.944649446494465"
[1] "TRAIN specifity: 0.64321608040201"
[1] "TRAIN sensitivity: 0.991349480968858"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        2       10
            positive       16      572
[1] "TEST accuracy: 0.956666666666667"
[1] "TEST +precision: 0.972789115646258"
[1] "TEST -precision: 0.166666666666667"
[1] "TEST specifity: 0.111111111111111"
[1] "TEST sensitivity: 0.982817869415808"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 6.46914673248927"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

2132 samples
 583 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1706, 1705, 1705, 1705, 1707 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens       Spec     
  0.3  1          0.6               0.50        50      0.8879303  0.3793354  0.9896170
  0.3  1          0.6               0.50       100      0.9247248  0.5302848  0.9867369
  0.3  1          0.6               0.50       150      0.9395450  0.6306013  0.9827023
  0.3  1          0.6               0.75        50      0.9028387  0.4022152  0.9890423
  0.3  1          0.6               0.75       100      0.9405105  0.5302215  0.9855841
  0.3  1          0.6               0.75       150      0.9557591  0.6735443  0.9832736
  0.3  1          0.6               1.00        50      0.9086525  0.3795570  0.9896187
  0.3  1          0.6               1.00       100      0.9397446  0.5654114  0.9884660
  0.3  1          0.6               1.00       150      0.9594375  0.6559494  0.9884676
  0.3  1          0.8               0.50        50      0.8943007  0.3945570  0.9901967
  0.3  1          0.8               0.50       100      0.9297873  0.5729747  0.9861588
  0.3  1          0.8               0.50       150      0.9434692  0.6609177  0.9844314
  0.3  1          0.8               0.75        50      0.9063222  0.4071519  0.9884676
  0.3  1          0.8               0.75       100      0.9410092  0.5454114  0.9861588
  0.3  1          0.8               0.75       150      0.9537438  0.6610127  0.9838533
  0.3  1          0.8               1.00        50      0.9074656  0.3719620  0.9901967
  0.3  1          0.8               1.00       100      0.9403561  0.5554114  0.9901967
  0.3  1          0.8               1.00       150      0.9612940  0.6408544  0.9884676
  0.3  2          0.6               0.50        50      0.9183704  0.5505063  0.9878879
  0.3  2          0.6               0.50       100      0.9459645  0.7110127  0.9827006
  0.3  2          0.6               0.50       150      0.9550456  0.7913608  0.9815479
  0.3  2          0.6               0.75        50      0.9371091  0.5528165  0.9901967
  0.3  2          0.6               0.75       100      0.9611199  0.7210759  0.9850027
  0.3  2          0.6               0.75       150      0.9701130  0.8164557  0.9803968
  0.3  2          0.6               1.00        50      0.9471513  0.5982595  0.9901967
  0.3  2          0.6               1.00       100      0.9711636  0.7261709  0.9884676
  0.3  2          0.6               1.00       150      0.9782841  0.8140190  0.9815512
  0.3  2          0.8               0.50        50      0.9271789  0.5276899  0.9861605
  0.3  2          0.8               0.50       100      0.9515153  0.7084494  0.9827023
  0.3  2          0.8               0.50       150      0.9608627  0.7662975  0.9803951
  0.3  2          0.8               0.75        50      0.9423670  0.5577532  0.9896187
  0.3  2          0.8               0.75       100      0.9652878  0.7462658  0.9855841
  0.3  2          0.8               0.75       150      0.9717070  0.8265190  0.9809715
  0.3  2          0.8               1.00        50      0.9490498  0.5703797  0.9901951
  0.3  2          0.8               1.00       100      0.9727725  0.7311709  0.9901967
  0.3  2          0.8               1.00       150      0.9776555  0.8090823  0.9861621
  0.3  3          0.6               0.50        50      0.9454718  0.6131962  0.9832786
  0.3  3          0.6               0.50       100      0.9586737  0.7687658  0.9792407
  0.3  3          0.6               0.50       150      0.9652741  0.8163608  0.9775116
  0.3  3          0.6               0.75        50      0.9589339  0.6685443  0.9861621
  0.3  3          0.6               0.75       100      0.9722188  0.8341772  0.9798188
  0.3  3          0.6               0.75       150      0.9765699  0.8717405  0.9792407
  0.3  3          0.6               1.00        50      0.9663175  0.6934177  0.9896187
  0.3  3          0.6               1.00       100      0.9788869  0.8341139  0.9844330
  0.3  3          0.6               1.00       150      0.9831122  0.8792722  0.9798188
  0.3  3          0.8               0.50        50      0.9470583  0.6182278  0.9873116
  0.3  3          0.8               0.50       100      0.9627651  0.7862975  0.9844314
  0.3  3          0.8               0.50       150      0.9668545  0.8340506  0.9815462
  0.3  3          0.8               0.75        50      0.9609705  0.6610127  0.9861621
  0.3  3          0.8               0.75       100      0.9742550  0.8416139  0.9803951
  0.3  3          0.8               0.75       150      0.9788086  0.8793038  0.9775133
  0.3  3          0.8               1.00        50      0.9659259  0.7060759  0.9896204
  0.3  3          0.8               1.00       100      0.9789115  0.8215823  0.9855858
  0.3  3          0.8               1.00       150      0.9832630  0.8768354  0.9832803
  0.4  1          0.6               0.50        50      0.8958347  0.4349051  0.9861605
  0.4  1          0.6               0.50       100      0.9254491  0.5906646  0.9798154
  0.4  1          0.6               0.50       150      0.9428121  0.7009494  0.9775150
  0.4  1          0.6               0.75        50      0.9183437  0.4748101  0.9890423
  0.4  1          0.6               0.75       100      0.9523841  0.6307595  0.9873132
  0.4  1          0.6               0.75       150      0.9606490  0.7337025  0.9798188
  0.4  1          0.6               1.00        50      0.9256485  0.4901266  0.9884660
  0.4  1          0.6               1.00       100      0.9551087  0.6508228  0.9861605
  0.4  1          0.6               1.00       150      0.9664881  0.7110759  0.9867385
  0.4  1          0.8               0.50        50      0.8972798  0.4723734  0.9890423
  0.4  1          0.8               0.50       100      0.9354368  0.6033544  0.9832786
  0.4  1          0.8               0.50       150      0.9467968  0.6859177  0.9798188
  0.4  1          0.8               0.75        50      0.9159193  0.4926582  0.9884660
  0.4  1          0.8               0.75       100      0.9513798  0.6458861  0.9867369
  0.4  1          0.8               0.75       150      0.9614149  0.7438291  0.9792391
  0.4  1          0.8               1.00        50      0.9233448  0.5125949  0.9878896
  0.4  1          0.8               1.00       100      0.9544346  0.6382911  0.9850077
  0.4  1          0.8               1.00       150      0.9662039  0.7085759  0.9855841
  0.4  2          0.6               0.50        50      0.9349122  0.6356329  0.9855808
  0.4  2          0.6               0.50       100      0.9525751  0.7610759  0.9803951
  0.4  2          0.6               0.50       150      0.9611650  0.8188924  0.9803935
  0.4  2          0.6               0.75        50      0.9460572  0.6809810  0.9855841
  0.4  2          0.6               0.75       100      0.9672349  0.8064241  0.9798171
  0.4  2          0.6               0.75       150      0.9749087  0.8567405  0.9792407
  0.4  2          0.6               1.00        50      0.9597562  0.6758228  0.9890423
  0.4  2          0.6               1.00       100      0.9750063  0.7939241  0.9838550
  0.4  2          0.6               1.00       150      0.9805001  0.8492089  0.9815512
  0.4  2          0.8               0.50        50      0.9354179  0.6507278  0.9821209
  0.4  2          0.8               0.50       100      0.9566568  0.7687975  0.9815479
  0.4  2          0.8               0.50       150      0.9630720  0.8264873  0.9775116
  0.4  2          0.8               0.75        50      0.9489769  0.6380696  0.9890407
  0.4  2          0.8               0.75       100      0.9659685  0.7787342  0.9832753
  0.4  2          0.8               0.75       150      0.9723309  0.8490823  0.9809698
  0.4  2          0.8               1.00        50      0.9634649  0.6609177  0.9861588
  0.4  2          0.8               1.00       100      0.9759351  0.7964557  0.9832770
  0.4  2          0.8               1.00       150      0.9809252  0.8542405  0.9780880
  0.4  3          0.6               0.50        50      0.9457600  0.7008544  0.9838550
  0.4  3          0.6               0.50       100      0.9627417  0.7989241  0.9786677
  0.4  3          0.6               0.50       150      0.9644484  0.8290190  0.9769369
  0.4  3          0.6               0.75        50      0.9627502  0.7462658  0.9878896
  0.4  3          0.6               0.75       100      0.9745677  0.8616772  0.9827006
  0.4  3          0.6               0.75       150      0.9785499  0.8994304  0.9821242
  0.4  3          0.6               1.00        50      0.9699063  0.7687342  0.9861621
  0.4  3          0.6               1.00       100      0.9815460  0.8642405  0.9838567
  0.4  3          0.6               1.00       150      0.9850921  0.8969620  0.9821242
  0.4  3          0.8               0.50        50      0.9504678  0.7134494  0.9832770
  0.4  3          0.8               0.50       100      0.9651335  0.8114557  0.9780897
  0.4  3          0.8               0.50       150      0.9684479  0.8390190  0.9792407
  0.4  3          0.8               0.75        50      0.9605330  0.7637658  0.9855841
  0.4  3          0.8               0.75       100      0.9766483  0.8617405  0.9832786
  0.4  3          0.8               0.75       150      0.9787868  0.9044304  0.9780880
  0.4  3          0.8               1.00        50      0.9727691  0.7563291  0.9896204
  0.4  3          0.8               1.00       100      0.9828821  0.8717405  0.9855841
  0.4  3          0.8               1.00       150      0.9851325  0.9020253  0.9832786

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 150, max_depth = 3, eta = 0.4, gamma = 0, colsample_bytree = 0.8, min_child_weight = 1 and subsample = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     16.8      1.4
  positive      1.8     80.0
                            
 Accuracy (average) : 0.9681

[1] "TRAIN accuracy: 0.968105065666041"
[1] "TRAIN +precision: 0.977637614678899"
[1] "TRAIN -precision: 0.925257731958763"
[1] "TRAIN specifity: 0.902010050251256"
[1] "TRAIN sensitivity: 0.983275663206459"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        4        6
            positive       14      576
[1] "TEST accuracy: 0.966666666666667"
[1] "TEST +precision: 0.976271186440678"
[1] "TEST -precision: 0.4"
[1] "TEST specifity: 0.222222222222222"
[1] "TEST sensitivity: 0.989690721649485"

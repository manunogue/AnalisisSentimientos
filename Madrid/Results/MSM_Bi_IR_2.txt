[1] "DATASET NAME: MSM_Bi_IR_2"
[1] "TRAIN INSTANCES: 2633"
[1] "TEST INSTANCES: 600"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 12.2385749816895"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

2633 samples
 583 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 2107, 2106, 2106, 2106, 2107 
Resampling results:

  ROC        Sens       Spec     
  0.9831211  0.9688516  0.9590661

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     33.1      2.7
  positive      1.1     63.2
                            
 Accuracy (average) : 0.9624

[1] "TRAIN accuracy: 0.962400303835929"
[1] "TRAIN +precision: 0.983441750443525"
[1] "TRAIN -precision: 0.924628450106157"
[1] "TRAIN specifity: 0.968854282536151"
[1] "TRAIN sensitivity: 0.959054209919262"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        8       42
            positive       10      540
[1] "TEST accuracy: 0.913333333333333"
[1] "TEST +precision: 0.981818181818182"
[1] "TEST -precision: 0.16"
[1] "TEST specifity: 0.444444444444444"
[1] "TEST sensitivity: 0.927835051546392"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 3.14151105086009"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

2633 samples
 583 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 2107, 2107, 2106, 2106, 2106 
Resampling results across tuning parameters:

  C      M  ROC        Sens       Spec     
  0.010  1  0.9075268  0.7797207  0.9711749
  0.010  2  0.9073250  0.7797207  0.9711749
  0.010  3  0.9048692  0.7640782  0.9775150
  0.255  1  0.9208569  0.7908566  0.9740567
  0.255  2  0.9206551  0.7908566  0.9740567
  0.255  3  0.9227262  0.7908566  0.9803968
  0.500  1  0.9319019  0.7908566  0.9809732
  0.500  2  0.9317674  0.7908566  0.9809732
  0.500  3  0.9332753  0.7908566  0.9873132

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.5 and M = 3.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     27.0      0.8
  positive      7.1     65.0
                            
 Accuracy (average) : 0.9202

[1] "TRAIN accuracy: 0.920243068742879"
[1] "TRAIN +precision: 0.901052631578947"
[1] "TRAIN -precision: 0.969986357435198"
[1] "TRAIN specifity: 0.790878754171301"
[1] "TRAIN sensitivity: 0.987312572087659"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        2        4
            positive       16      578
[1] "TEST accuracy: 0.966666666666667"
[1] "TEST +precision: 0.973063973063973"
[1] "TEST -precision: 0.333333333333333"
[1] "TEST specifity: 0.111111111111111"
[1] "TEST sensitivity: 0.993127147766323"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 7.59348518053691"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

2633 samples
 583 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 2106, 2107, 2107, 2106, 2106 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens       Spec     
  0.3  1          0.6               0.50        50      0.9196612  0.5750962  0.9838533
  0.3  1          0.6               0.50       100      0.9605107  0.6707573  0.9769253
  0.3  1          0.6               0.50       150      0.9695051  0.7986034  0.9711599
  0.3  1          0.6               0.75        50      0.9203273  0.5917381  0.9855824
  0.3  1          0.6               0.75       100      0.9612214  0.7074736  0.9780830
  0.3  1          0.6               0.75       150      0.9727811  0.7853135  0.9746214
  0.3  1          0.6               1.00        50      0.9175213  0.5817691  0.9844297
  0.3  1          0.6               1.00       100      0.9608162  0.6951707  0.9792374
  0.3  1          0.6               1.00       150      0.9740631  0.7630478  0.9763522
  0.3  1          0.8               0.50        50      0.9205088  0.5939975  0.9821242
  0.3  1          0.8               0.50       100      0.9628187  0.7107883  0.9769286
  0.3  1          0.8               0.50       150      0.9680084  0.7875109  0.9688511
  0.3  1          0.8               0.75        50      0.9192495  0.5728430  0.9855824
  0.3  1          0.8               0.75       100      0.9637762  0.6840968  0.9809665
  0.3  1          0.8               0.75       150      0.9721905  0.7875481  0.9746231
  0.3  1          0.8               1.00        50      0.9193606  0.5761887  0.9850061
  0.3  1          0.8               1.00       100      0.9621708  0.7085040  0.9809698
  0.3  1          0.8               1.00       150      0.9737762  0.7663811  0.9752011
  0.3  2          0.6               0.50        50      0.9633044  0.7051645  0.9798154
  0.3  2          0.6               0.50       100      0.9767815  0.8575667  0.9734720
  0.3  2          0.6               0.50       150      0.9807246  0.9466108  0.9671303
  0.3  2          0.6               0.75        50      0.9681533  0.7129795  0.9792357
  0.3  2          0.6               0.75       100      0.9808680  0.8742831  0.9728923
  0.3  2          0.6               0.75       150      0.9839285  0.9521850  0.9700155
  0.3  2          0.6               1.00        50      0.9674993  0.7196648  0.9792341
  0.3  2          0.6               1.00       100      0.9818079  0.8709497  0.9757792
  0.3  2          0.6               1.00       150      0.9855873  0.9510739  0.9728957
  0.3  2          0.8               0.50        50      0.9655858  0.7141279  0.9815429
  0.3  2          0.8               0.50       100      0.9780888  0.8742768  0.9746248
  0.3  2          0.8               0.50       150      0.9820483  0.9444072  0.9717379
  0.3  2          0.8               0.75        50      0.9689203  0.7307759  0.9821226
  0.3  2          0.8               0.75       100      0.9801647  0.8798386  0.9769353
  0.3  2          0.8               0.75       150      0.9840733  0.9555183  0.9705902
  0.3  2          0.8               1.00        50      0.9695064  0.7318870  0.9838533
  0.3  2          0.8               1.00       100      0.9836679  0.8654066  0.9786644
  0.3  2          0.8               1.00       150      0.9864806  0.9444072  0.9763572
  0.3  3          0.6               0.50        50      0.9729360  0.7830726  0.9746231
  0.3  3          0.6               0.50       100      0.9812177  0.9410366  0.9700138
  0.3  3          0.6               0.50       150      0.9849896  0.9566046  0.9700105
  0.3  3          0.6               0.75        50      0.9797084  0.8253321  0.9815462
  0.3  3          0.6               0.75       100      0.9858147  0.9521850  0.9775133
  0.3  3          0.6               0.75       150      0.9883982  0.9610739  0.9740551
  0.3  3          0.6               1.00        50      0.9807367  0.8264556  0.9832753
  0.3  3          0.6               1.00       100      0.9871892  0.9454997  0.9757742
  0.3  3          0.6               1.00       150      0.9892214  0.9566294  0.9751995
  0.3  3          0.8               0.50        50      0.9760463  0.8253321  0.9775083
  0.3  3          0.8               0.50       100      0.9837421  0.9555059  0.9740467
  0.3  3          0.8               0.50       150      0.9863940  0.9610739  0.9717413
  0.3  3          0.8               0.75        50      0.9780356  0.8420298  0.9780847
  0.3  3          0.8               0.75       100      0.9863143  0.9555183  0.9757842
  0.3  3          0.8               0.75       150      0.9883298  0.9610739  0.9728973
  0.3  3          0.8               1.00        50      0.9813167  0.8397579  0.9809665
  0.3  3          0.8               1.00       100      0.9874688  0.9477219  0.9798171
  0.3  3          0.8               1.00       150      0.9893370  0.9610739  0.9769336
  0.4  1          0.6               0.50        50      0.9277907  0.6407325  0.9809732
  0.4  1          0.6               0.50       100      0.9688706  0.7575171  0.9734670
  0.4  1          0.6               0.50       150      0.9746119  0.8431223  0.9665473
  0.4  1          0.6               0.75        50      0.9366875  0.6440844  0.9792357
  0.4  1          0.6               0.75       100      0.9671091  0.7619119  0.9717329
  0.4  1          0.6               0.75       150      0.9759851  0.8475543  0.9723143
  0.4  1          0.6               1.00        50      0.9362401  0.6084482  0.9850027
  0.4  1          0.6               1.00       100      0.9710059  0.7585661  0.9757758
  0.4  1          0.6               1.00       150      0.9773800  0.8297827  0.9717413
  0.4  1          0.8               0.50        50      0.9261464  0.6373433  0.9792357
  0.4  1          0.8               0.50       100      0.9653592  0.7797207  0.9700055
  0.4  1          0.8               0.50       150      0.9719721  0.8686592  0.9677017
  0.4  1          0.8               0.75        50      0.9395130  0.6273371  0.9821192
  0.4  1          0.8               0.75       100      0.9687031  0.7741837  0.9746214
  0.4  1          0.8               0.75       150      0.9755277  0.8564991  0.9677000
  0.4  1          0.8               1.00        50      0.9327394  0.6407014  0.9798104
  0.4  1          0.8               1.00       100      0.9713190  0.7541217  0.9751978
  0.4  1          0.8               1.00       150      0.9774761  0.8275605  0.9705869
  0.4  2          0.6               0.50        50      0.9710705  0.7752700  0.9792374
  0.4  2          0.6               0.50       100      0.9788331  0.9143700  0.9717379
  0.4  2          0.6               0.50       150      0.9831898  0.9543700  0.9682797
  0.4  2          0.6               0.75        50      0.9728731  0.8097765  0.9769269
  0.4  2          0.6               0.75       100      0.9826257  0.9265860  0.9688528
  0.4  2          0.6               0.75       150      0.9856107  0.9610739  0.9694325
  0.4  2          0.6               1.00        50      0.9767253  0.8019739  0.9803901
  0.4  2          0.6               1.00       100      0.9846482  0.9265984  0.9752045
  0.4  2          0.6               1.00       150      0.9872205  0.9555183  0.9734737
  0.4  2          0.8               0.50        50      0.9692455  0.7841589  0.9769353
  0.4  2          0.8               0.50       100      0.9799888  0.9232526  0.9705919
  0.4  2          0.8               0.50       150      0.9837549  0.9577405  0.9723176
  0.4  2          0.8               0.75        50      0.9741489  0.8309497  0.9798171
  0.4  2          0.8               0.75       100      0.9838417  0.9332775  0.9728923
  0.4  2          0.8               0.75       150      0.9857214  0.9610739  0.9694325
  0.4  2          0.8               1.00        50      0.9765999  0.8142148  0.9832753
  0.4  2          0.8               1.00       100      0.9864334  0.9343886  0.9786610
  0.4  2          0.8               1.00       150      0.9885543  0.9566294  0.9757758
  0.4  3          0.6               0.50        50      0.9783323  0.8898696  0.9763489
  0.4  3          0.6               0.50       100      0.9841812  0.9477157  0.9723126
  0.4  3          0.6               0.50       150      0.9859226  0.9566046  0.9682797
  0.4  3          0.6               0.75        50      0.9815922  0.8853693  0.9775066
  0.4  3          0.6               0.75       100      0.9871401  0.9610739  0.9723160
  0.4  3          0.6               0.75       150      0.9892386  0.9610739  0.9728940
  0.4  3          0.6               1.00        50      0.9833322  0.8865177  0.9798171
  0.4  3          0.6               1.00       100      0.9892084  0.9610739  0.9763605
  0.4  3          0.6               1.00       150      0.9912972  0.9610739  0.9757808
  0.4  3          0.8               0.50        50      0.9775862  0.8609373  0.9700088
  0.4  3          0.8               0.50       100      0.9838111  0.9610739  0.9711632
  0.4  3          0.8               0.50       150      0.9866962  0.9610739  0.9682781
  0.4  3          0.8               0.75        50      0.9828115  0.9065425  0.9769303
  0.4  3          0.8               0.75       100      0.9879514  0.9610739  0.9763572
  0.4  3          0.8               0.75       150      0.9889069  0.9610739  0.9717429
  0.4  3          0.8               1.00        50      0.9831579  0.8798510  0.9786594
  0.4  3          0.8               1.00       100      0.9893178  0.9610739  0.9763522
  0.4  3          0.8               1.00       150      0.9910201  0.9610739  0.9775100

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 150, max_depth = 3, eta = 0.4, gamma = 0, colsample_bytree = 0.6, min_child_weight = 1 and subsample = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     32.8      1.6
  positive      1.3     64.3
                            
 Accuracy (average) : 0.9708

[1] "TRAIN accuracy: 0.970755791872389"
[1] "TRAIN +precision: 0.979733642154024"
[1] "TRAIN -precision: 0.95364238410596"
[1] "TRAIN specifity: 0.961067853170189"
[1] "TRAIN sensitivity: 0.975778546712803"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        4        7
            positive       14      575
[1] "TEST accuracy: 0.965"
[1] "TEST +precision: 0.976230899830221"
[1] "TEST -precision: 0.363636363636364"
[1] "TEST specifity: 0.222222222222222"
[1] "TEST sensitivity: 0.987972508591065"

[1] "DATASET NAME: TCT_Bi_IR_5"
[1] "TRAIN INSTANCES: 790"
[1] "TEST INSTANCES: 225"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 3.06211304664612"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

790 samples
940 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 632, 632, 632, 632, 632 
Resampling results:

  ROC        Sens     Spec     
  0.9897569  0.91875  0.9952381

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     18.6      0.4
  positive      1.6     79.4
                            
 Accuracy (average) : 0.9797

[1] "TRAIN accuracy: 0.979746835443038"
[1] "TRAIN +precision: 0.9796875"
[1] "TRAIN -precision: 0.98"
[1] "TRAIN specifity: 0.91875"
[1] "TRAIN sensitivity: 0.995238095238095"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        3        0
            positive       14      208
[1] "TEST accuracy: 0.937777777777778"
[1] "TEST +precision: 0.936936936936937"
[1] "TEST -precision: 1"
[1] "TEST specifity: 0.176470588235294"
[1] "TEST sensitivity: 1"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 1.75417278210322"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

790 samples
940 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 632, 632, 632, 632, 632 
Resampling results across tuning parameters:

  C      M  ROC        Sens     Spec     
  0.010  1  0.6476935  0.26875  0.9873016
  0.010  2  0.6503472  0.26875  0.9904762
  0.010  3  0.6195188  0.23125  0.9920635
  0.255  1  0.7837798  0.41250  0.9984127
  0.255  2  0.7820933  0.41250  0.9984127
  0.255  3  0.7632440  0.38750  0.9984127
  0.500  1  0.8090774  0.42500  0.9984127
  0.500  2  0.7984623  0.41875  0.9984127
  0.500  3  0.7632937  0.38750  1.0000000

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.5 and M = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative      8.6      0.1
  positive     11.6     79.6
                            
 Accuracy (average) : 0.8823

[1] "TRAIN accuracy: 0.882278481012658"
[1] "TRAIN +precision: 0.872399445214979"
[1] "TRAIN -precision: 0.985507246376812"
[1] "TRAIN specifity: 0.425"
[1] "TRAIN sensitivity: 0.998412698412698"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        1        4
            positive       16      204
[1] "TEST accuracy: 0.911111111111111"
[1] "TEST +precision: 0.927272727272727"
[1] "TEST -precision: 0.2"
[1] "TEST specifity: 0.0588235294117647"
[1] "TEST sensitivity: 0.980769230769231"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 3.52842178344727"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

790 samples
940 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 632, 632, 632, 632, 632 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens     Spec     
  0.3  1          0.6               0.50        50      0.8593750  0.33750  0.9873016
  0.3  1          0.6               0.50       100      0.9068452  0.50625  0.9857143
  0.3  1          0.6               0.50       150      0.9200149  0.53750  0.9825397
  0.3  1          0.6               0.75        50      0.8679812  0.38125  0.9888889
  0.3  1          0.6               0.75       100      0.9137401  0.54375  0.9825397
  0.3  1          0.6               0.75       150      0.9311012  0.60000  0.9841270
  0.3  1          0.6               1.00        50      0.8690476  0.36250  0.9920635
  0.3  1          0.6               1.00       100      0.9084573  0.53750  0.9857143
  0.3  1          0.6               1.00       150      0.9288194  0.57500  0.9793651
  0.3  1          0.8               0.50        50      0.8547123  0.32500  0.9841270
  0.3  1          0.8               0.50       100      0.8972718  0.50000  0.9793651
  0.3  1          0.8               0.50       150      0.9095734  0.54375  0.9793651
  0.3  1          0.8               0.75        50      0.8565228  0.38750  0.9888889
  0.3  1          0.8               0.75       100      0.9171131  0.55000  0.9888889
  0.3  1          0.8               0.75       150      0.9339782  0.60625  0.9857143
  0.3  1          0.8               1.00        50      0.8713046  0.36250  0.9904762
  0.3  1          0.8               1.00       100      0.9087302  0.53750  0.9825397
  0.3  1          0.8               1.00       150      0.9275794  0.57500  0.9825397
  0.3  2          0.6               0.50        50      0.9003472  0.49375  0.9841270
  0.3  2          0.6               0.50       100      0.9175595  0.58125  0.9793651
  0.3  2          0.6               0.50       150      0.9192708  0.58125  0.9777778
  0.3  2          0.6               0.75        50      0.9152778  0.53750  0.9793651
  0.3  2          0.6               0.75       100      0.9363839  0.62500  0.9777778
  0.3  2          0.6               0.75       150      0.9357391  0.62500  0.9730159
  0.3  2          0.6               1.00        50      0.9200397  0.55000  0.9873016
  0.3  2          0.6               1.00       100      0.9525050  0.63750  0.9825397
  0.3  2          0.6               1.00       150      0.9535714  0.64375  0.9809524
  0.3  2          0.8               0.50        50      0.9030506  0.47500  0.9825397
  0.3  2          0.8               0.50       100      0.9149306  0.60000  0.9746032
  0.3  2          0.8               0.50       150      0.9197917  0.61875  0.9746032
  0.3  2          0.8               0.75        50      0.9137153  0.51250  0.9857143
  0.3  2          0.8               0.75       100      0.9350694  0.61875  0.9793651
  0.3  2          0.8               0.75       150      0.9327877  0.66250  0.9761905
  0.3  2          0.8               1.00        50      0.9207093  0.54375  0.9857143
  0.3  2          0.8               1.00       100      0.9533482  0.63750  0.9825397
  0.3  2          0.8               1.00       150      0.9537946  0.64375  0.9793651
  0.3  3          0.6               0.50        50      0.9276538  0.53750  0.9746032
  0.3  3          0.6               0.50       100      0.9331597  0.58750  0.9793651
  0.3  3          0.6               0.50       150      0.9329117  0.60000  0.9793651
  0.3  3          0.6               0.75        50      0.9307540  0.58125  0.9841270
  0.3  3          0.6               0.75       100      0.9382440  0.63125  0.9793651
  0.3  3          0.6               0.75       150      0.9369296  0.68125  0.9777778
  0.3  3          0.6               1.00        50      0.9379960  0.60000  0.9809524
  0.3  3          0.6               1.00       100      0.9492063  0.63125  0.9746032
  0.3  3          0.6               1.00       150      0.9481647  0.64375  0.9746032
  0.3  3          0.8               0.50        50      0.9053323  0.51250  0.9809524
  0.3  3          0.8               0.50       100      0.9153274  0.56250  0.9777778
  0.3  3          0.8               0.50       150      0.9150546  0.60000  0.9730159
  0.3  3          0.8               0.75        50      0.9359871  0.58125  0.9857143
  0.3  3          0.8               0.75       100      0.9402530  0.63750  0.9793651
  0.3  3          0.8               0.75       150      0.9405506  0.63750  0.9746032
  0.3  3          0.8               1.00        50      0.9412450  0.61250  0.9825397
  0.3  3          0.8               1.00       100      0.9510665  0.64375  0.9730159
  0.3  3          0.8               1.00       150      0.9483879  0.64375  0.9730159
  0.4  1          0.6               0.50        50      0.8635417  0.39375  0.9841270
  0.4  1          0.6               0.50       100      0.9019097  0.55000  0.9809524
  0.4  1          0.6               0.50       150      0.9179563  0.56250  0.9777778
  0.4  1          0.6               0.75        50      0.8861855  0.41250  0.9873016
  0.4  1          0.6               0.75       100      0.9244048  0.60000  0.9841270
  0.4  1          0.6               0.75       150      0.9399058  0.62500  0.9809524
  0.4  1          0.6               1.00        50      0.8808036  0.45000  0.9873016
  0.4  1          0.6               1.00       100      0.9243056  0.58750  0.9825397
  0.4  1          0.6               1.00       150      0.9443452  0.61250  0.9809524
  0.4  1          0.8               0.50        50      0.8740327  0.41250  0.9825397
  0.4  1          0.8               0.50       100      0.9153274  0.53125  0.9825397
  0.4  1          0.8               0.50       150      0.9250248  0.59375  0.9761905
  0.4  1          0.8               0.75        50      0.8816716  0.46250  0.9857143
  0.4  1          0.8               0.75       100      0.9238591  0.60625  0.9793651
  0.4  1          0.8               0.75       150      0.9352183  0.63125  0.9777778
  0.4  1          0.8               1.00        50      0.8907242  0.46250  0.9873016
  0.4  1          0.8               1.00       100      0.9214534  0.58125  0.9793651
  0.4  1          0.8               1.00       150      0.9398562  0.60625  0.9793651
  0.4  2          0.6               0.50        50      0.9049603  0.51250  0.9777778
  0.4  2          0.6               0.50       100      0.9086806  0.58125  0.9761905
  0.4  2          0.6               0.50       150      0.9117312  0.57500  0.9777778
  0.4  2          0.6               0.75        50      0.9213790  0.55000  0.9825397
  0.4  2          0.6               0.75       100      0.9324653  0.63750  0.9714286
  0.4  2          0.6               0.75       150      0.9347222  0.66250  0.9730159
  0.4  2          0.6               1.00        50      0.9314732  0.59375  0.9857143
  0.4  2          0.6               1.00       100      0.9505704  0.62500  0.9825397
  0.4  2          0.6               1.00       150      0.9499256  0.63750  0.9761905
  0.4  2          0.8               0.50        50      0.9239087  0.50000  0.9873016
  0.4  2          0.8               0.50       100      0.9258185  0.58125  0.9793651
  0.4  2          0.8               0.50       150      0.9209821  0.61250  0.9777778
  0.4  2          0.8               0.75        50      0.9165923  0.58125  0.9793651
  0.4  2          0.8               0.75       100      0.9309276  0.63125  0.9730159
  0.4  2          0.8               0.75       150      0.9297619  0.64375  0.9746032
  0.4  2          0.8               1.00        50      0.9287698  0.57500  0.9841270
  0.4  2          0.8               1.00       100      0.9479167  0.64375  0.9730159
  0.4  2          0.8               1.00       150      0.9466518  0.65000  0.9714286
  0.4  3          0.6               0.50        50      0.9123512  0.53750  0.9809524
  0.4  3          0.6               0.50       100      0.9146081  0.58125  0.9761905
  0.4  3          0.6               0.50       150      0.9177579  0.60000  0.9793651
  0.4  3          0.6               0.75        50      0.9283482  0.60000  0.9761905
  0.4  3          0.6               0.75       100      0.9325893  0.63750  0.9761905
  0.4  3          0.6               0.75       150      0.9355903  0.65625  0.9746032
  0.4  3          0.6               1.00        50      0.9481151  0.63125  0.9857143
  0.4  3          0.6               1.00       100      0.9504216  0.65625  0.9777778
  0.4  3          0.6               1.00       150      0.9530754  0.68750  0.9777778
  0.4  3          0.8               0.50        50      0.9182292  0.56875  0.9777778
  0.4  3          0.8               0.50       100      0.9218254  0.58125  0.9793651
  0.4  3          0.8               0.50       150      0.9200893  0.60000  0.9761905
  0.4  3          0.8               0.75        50      0.9364583  0.63125  0.9809524
  0.4  3          0.8               0.75       100      0.9392609  0.67500  0.9761905
  0.4  3          0.8               0.75       150      0.9407242  0.66875  0.9761905
  0.4  3          0.8               1.00        50      0.9468750  0.62500  0.9793651
  0.4  3          0.8               1.00       100      0.9486359  0.63750  0.9698413
  0.4  3          0.8               1.00       150      0.9472718  0.68125  0.9730159

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 150, max_depth = 2, eta = 0.3, gamma = 0, colsample_bytree = 0.8, min_child_weight = 1 and subsample = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     13.0      1.6
  positive      7.2     78.1
                            
 Accuracy (average) : 0.9114

[1] "TRAIN accuracy: 0.911392405063291"
[1] "TRAIN +precision: 0.915430267062315"
[1] "TRAIN -precision: 0.887931034482759"
[1] "TRAIN specifity: 0.64375"
[1] "TRAIN sensitivity: 0.979365079365079"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        2        1
            positive       15      207
[1] "TEST accuracy: 0.928888888888889"
[1] "TEST +precision: 0.932432432432432"
[1] "TEST -precision: 0.666666666666667"
[1] "TEST specifity: 0.117647058823529"
[1] "TEST sensitivity: 0.995192307692308"

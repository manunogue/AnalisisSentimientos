[1] "DATASET NAME: ElSur_Uni_IR_2"
[1] "TRAIN INSTANCES: 1755"
[1] "TEST INSTANCES: 396"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 4.34414887428284"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

1755 samples
 684 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1404, 1404, 1404, 1404, 1404 
Resampling results:

  ROC  Sens  Spec     
  1    1     0.9982759

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     33.8      0.1
  positive      0.0     66.1
                            
 Accuracy (average) : 0.9989

[1] "TRAIN accuracy: 0.998860398860399"
[1] "TRAIN +precision: 1"
[1] "TRAIN -precision: 0.996638655462185"
[1] "TRAIN specifity: 1"
[1] "TRAIN sensitivity: 0.998278829604131"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        1        0
            positive        4      391
[1] "TEST accuracy: 0.98989898989899"
[1] "TEST +precision: 0.989873417721519"
[1] "TEST -precision: 1"
[1] "TEST specifity: 0.2"
[1] "TEST sensitivity: 1"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 1.96185030142466"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

1755 samples
 684 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1404, 1403, 1405, 1404, 1404 
Resampling results across tuning parameters:

  C      M  ROC        Sens  Spec     
  0.010  1  0.9951656  1     0.9853633
  0.010  2  0.9951656  1     0.9853633
  0.010  3  0.9951656  1     0.9853633
  0.255  1  0.9956313  1     0.9879495
  0.255  2  0.9956313  1     0.9879495
  0.255  3  0.9956313  1     0.9879495
  0.500  1  0.9948942  1     0.9896700
  0.500  2  0.9948942  1     0.9896700
  0.500  3  0.9948944  1     0.9896700

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.255 and M = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     33.8      0.8
  positive      0.0     65.4
                           
 Accuracy (average) : 0.992

[1] "TRAIN accuracy: 0.992022792022792"
[1] "TRAIN +precision: 1"
[1] "TRAIN -precision: 0.976935749588138"
[1] "TRAIN specifity: 1"
[1] "TRAIN sensitivity: 0.987951807228916"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        3        3
            positive        2      388
[1] "TEST accuracy: 0.987373737373737"
[1] "TEST +precision: 0.994871794871795"
[1] "TEST -precision: 0.5"
[1] "TEST specifity: 0.6"
[1] "TEST sensitivity: 0.9923273657289"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 5.98936524788539"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

1755 samples
 684 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1403, 1404, 1404, 1404, 1405 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens       Spec     
  0.3  1          0.6               0.50        50      0.9971332  1.0000000  0.9784927
  0.3  1          0.6               0.50       100      0.9984964  1.0000000  0.9819262
  0.3  1          0.6               0.50       150      0.9984749  1.0000000  0.9836540
  0.3  1          0.6               0.75        50      0.9975735  1.0000000  0.9776306
  0.3  1          0.6               0.75       100      0.9991836  1.0000000  0.9862365
  0.3  1          0.6               0.75       150      0.9989165  1.0000000  0.9862328
  0.3  1          0.6               1.00        50      0.9982435  1.0000000  0.9793510
  0.3  1          0.6               1.00       100      0.9993135  1.0000000  0.9870912
  0.3  1          0.6               1.00       150      0.9992853  1.0000000  0.9879532
  0.3  1          0.8               0.50        50      0.9959730  0.9831933  0.9793436
  0.3  1          0.8               0.50       100      0.9984960  1.0000000  0.9836503
  0.3  1          0.8               0.50       150      0.9986198  1.0000000  0.9862328
  0.3  1          0.8               0.75        50      0.9982257  1.0000000  0.9802094
  0.3  1          0.8               0.75       100      0.9991623  1.0000000  0.9845124
  0.3  1          0.8               0.75       150      0.9990977  1.0000000  0.9879532
  0.3  1          0.8               1.00        50      0.9982295  1.0000000  0.9793510
  0.3  1          0.8               1.00       100      0.9993358  1.0000000  0.9845087
  0.3  1          0.8               1.00       150      0.9992853  1.0000000  0.9879532
  0.3  2          0.6               0.50        50      0.9995008  1.0000000  0.9896737
  0.3  2          0.6               0.50       100      0.9990833  1.0000000  0.9922599
  0.3  2          0.6               0.50       150      0.9984895  1.0000000  0.9905283
  0.3  2          0.6               0.75        50      0.9990980  1.0000000  0.9896774
  0.3  2          0.6               0.75       100      0.9990328  1.0000000  0.9931182
  0.3  2          0.6               0.75       150      0.9985257  1.0000000  0.9931182
  0.3  2          0.6               1.00        50      0.9990890  1.0000000  0.9862365
  0.3  2          0.6               1.00       100      0.9989736  1.0000000  0.9931108
  0.3  2          0.6               1.00       150      0.9985184  1.0000000  0.9913941
  0.3  2          0.8               0.50        50      0.9991180  1.0000000  0.9870912
  0.3  2          0.8               0.50       100      0.9986053  1.0000000  0.9905320
  0.3  2          0.8               0.50       150      0.9983303  1.0000000  0.9879495
  0.3  2          0.8               0.75        50      0.9990461  1.0000000  0.9888116
  0.3  2          0.8               0.75       100      0.9986848  1.0000000  0.9922562
  0.3  2          0.8               0.75       150      0.9986054  1.0000000  0.9922562
  0.3  2          0.8               1.00        50      0.9995514  1.0000000  0.9853744
  0.3  2          0.8               1.00       100      0.9991904  1.0000000  0.9931108
  0.3  2          0.8               1.00       150      0.9986123  1.0000000  0.9922525
  0.3  3          0.6               0.50        50      0.9997832  1.0000000  0.9939729
  0.3  3          0.6               0.50       100      0.9994296  1.0000000  0.9931145
  0.3  3          0.6               0.50       150      0.9991475  1.0000000  0.9922525
  0.3  3          0.6               0.75        50      0.9994518  1.0000000  0.9931145
  0.3  3          0.6               0.75       100      0.9989821  1.0000000  0.9939766
  0.3  3          0.6               0.75       150      0.9987286  1.0000000  0.9913941
  0.3  3          0.6               1.00        50      0.9992931  1.0000000  0.9922525
  0.3  3          0.6               1.00       100      0.9991705  1.0000000  0.9939766
  0.3  3          0.6               1.00       150      0.9991343  1.0000000  0.9922562
  0.3  3          0.8               0.50        50      0.9992542  1.0000000  0.9948387
  0.3  3          0.8               0.50       100      0.9989577  1.0000000  0.9939803
  0.3  3          0.8               0.50       150      0.9987197  1.0000000  0.9948387
  0.3  3          0.8               0.75        50      0.9994012  1.0000000  0.9931145
  0.3  3          0.8               0.75       100      0.9987794  1.0000000  0.9913904
  0.3  3          0.8               0.75       150      0.9987142  1.0000000  0.9931108
  0.3  3          0.8               1.00        50      0.9992329  1.0000000  0.9905357
  0.3  3          0.8               1.00       100      0.9986703  1.0000000  0.9922525
  0.3  3          0.8               1.00       150      0.9985836  1.0000000  0.9913867
  0.4  1          0.6               0.50        50      0.9984451  1.0000000  0.9827882
  0.4  1          0.6               0.50       100      0.9984745  1.0000000  0.9862291
  0.4  1          0.6               0.50       150      0.9983518  1.0000000  0.9862217
  0.4  1          0.6               0.75        50      0.9991675  1.0000000  0.9845087
  0.4  1          0.6               0.75       100      0.9988224  1.0000000  0.9870949
  0.4  1          0.6               0.75       150      0.9991122  1.0000000  0.9870949
  0.4  1          0.6               1.00        50      0.9991467  1.0000000  0.9827919
  0.4  1          0.6               1.00       100      0.9994732  1.0000000  0.9862291
  0.4  1          0.6               1.00       150      0.9990900  1.0000000  0.9879532
  0.4  1          0.8               0.50        50      0.9980188  1.0000000  0.9836503
  0.4  1          0.8               0.50       100      0.9986414  1.0000000  0.9845087
  0.4  1          0.8               0.50       150      0.9985182  1.0000000  0.9870912
  0.4  1          0.8               0.75        50      0.9988065  1.0000000  0.9827919
  0.4  1          0.8               0.75       100      0.9991189  1.0000000  0.9879532
  0.4  1          0.8               0.75       150      0.9991053  1.0000000  0.9870912
  0.4  1          0.8               1.00        50      0.9990517  1.0000000  0.9845161
  0.4  1          0.8               1.00       100      0.9993939  1.0000000  0.9862291
  0.4  1          0.8               1.00       150      0.9992352  1.0000000  0.9888116
  0.4  2          0.6               0.50        50      0.9992259  1.0000000  0.9888227
  0.4  2          0.6               0.50       100      0.9985329  1.0000000  0.9905357
  0.4  2          0.6               0.50       150      0.9983808  1.0000000  0.9888153
  0.4  2          0.6               0.75        50      0.9997692  1.0000000  0.9931182
  0.4  2          0.6               0.75       100      0.9992787  1.0000000  0.9939766
  0.4  2          0.6               0.75       150      0.9991705  1.0000000  0.9913941
  0.4  2          0.6               1.00        50      0.9995222  1.0000000  0.9931145
  0.4  2          0.6               1.00       100      0.9986920  1.0000000  0.9922488
  0.4  2          0.6               1.00       150      0.9984968  1.0000000  0.9922525
  0.4  2          0.8               0.50        50      0.9995311  1.0000000  0.9888116
  0.4  2          0.8               0.50       100      0.9991921  1.0000000  0.9905320
  0.4  2          0.8               0.50       150      0.9990619  1.0000000  0.9896700
  0.4  2          0.8               0.75        50      0.9994877  1.0000000  0.9913941
  0.4  2          0.8               0.75       100      0.9991485  1.0000000  0.9913941
  0.4  2          0.8               0.75       150      0.9987864  1.0000000  0.9913941
  0.4  2          0.8               1.00        50      0.9990537  1.0000000  0.9913978
  0.4  2          0.8               1.00       100      0.9985620  1.0000000  0.9913941
  0.4  2          0.8               1.00       150      0.9984679  1.0000000  0.9922562
  0.4  3          0.6               0.50        50      0.9991052  1.0000000  0.9896737
  0.4  3          0.6               0.50       100      0.9985402  1.0000000  0.9913941
  0.4  3          0.6               0.50       150      0.9986996  1.0000000  0.9922562
  0.4  3          0.6               0.75        50      0.9992715  1.0000000  0.9948387
  0.4  3          0.6               0.75       100      0.9991633  1.0000000  0.9922525
  0.4  3          0.6               0.75       150      0.9991270  1.0000000  0.9931145
  0.4  3          0.6               1.00        50      0.9996610  1.0000000  0.9939766
  0.4  3          0.6               1.00       100      0.9992354  1.0000000  0.9939766
  0.4  3          0.6               1.00       150      0.9991921  1.0000000  0.9948350
  0.4  3          0.8               0.50        50      0.9992852  1.0000000  0.9922599
  0.4  3          0.8               0.50       100      0.9985256  1.0000000  0.9914015
  0.4  3          0.8               0.50       150      0.9985113  1.0000000  0.9922636
  0.4  3          0.8               0.75        50      0.9994300  1.0000000  0.9922525
  0.4  3          0.8               0.75       100      0.9988952  1.0000000  0.9913941
  0.4  3          0.8               0.75       150      0.9989314  1.0000000  0.9913941
  0.4  3          0.8               1.00        50      0.9992354  1.0000000  0.9939729
  0.4  3          0.8               1.00       100      0.9991705  1.0000000  0.9948350
  0.4  3          0.8               1.00       150      0.9991343  1.0000000  0.9948350

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 50, max_depth = 3, eta = 0.3, gamma = 0, colsample_bytree = 0.6, min_child_weight = 1 and subsample = 0.5.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     33.8      0.4
  positive      0.0     65.8
                           
 Accuracy (average) : 0.996

[1] "TRAIN accuracy: 0.996011396011396"
[1] "TRAIN +precision: 1"
[1] "TRAIN -precision: 0.988333333333333"
[1] "TRAIN specifity: 1"
[1] "TRAIN sensitivity: 0.993975903614458"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        4        3
            positive        1      388
[1] "TEST accuracy: 0.98989898989899"
[1] "TEST +precision: 0.997429305912596"
[1] "TEST -precision: 0.571428571428571"
[1] "TEST specifity: 0.8"
[1] "TEST sensitivity: 0.9923273657289"

[1] "DATASET NAME: Botin_Uni_IR_10"
[1] "TRAIN INSTANCES: 1834"
[1] "TEST INSTANCES: 573"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 13.8096580505371"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

1834 samples
 638 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1467, 1468, 1467, 1467, 1467 
Resampling results:

  ROC        Sens      Spec     
  0.9602343  0.822864  0.9710345

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     17.2      2.3
  positive      3.7     76.8
                          
 Accuracy (average) : 0.94

[1] "TRAIN accuracy: 0.940021810250818"
[1] "TRAIN +precision: 0.953929539295393"
[1] "TRAIN -precision: 0.88268156424581"
[1] "TRAIN specifity: 0.822916666666667"
[1] "TRAIN sensitivity: 0.971034482758621"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       46       21
            positive       30      476
[1] "TEST accuracy: 0.910994764397906"
[1] "TEST +precision: 0.940711462450593"
[1] "TEST -precision: 0.686567164179104"
[1] "TEST specifity: 0.605263157894737"
[1] "TEST sensitivity: 0.957746478873239"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 2.2839210152626"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

1834 samples
 638 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1467, 1467, 1468, 1467, 1467 
Resampling results across tuning parameters:

  C      M  ROC        Sens       Spec     
  0.010  1  0.8077954  0.6197198  0.9682759
  0.010  2  0.7946581  0.6041012  0.9662069
  0.010  3  0.7888952  0.5937457  0.9675862
  0.255  1  0.8330149  0.7318182  0.9441379
  0.255  2  0.8503317  0.7161312  0.9468966
  0.255  3  0.8470294  0.6979494  0.9420690
  0.500  1  0.8571541  0.7552290  0.9337931
  0.500  2  0.8622076  0.7317840  0.9351724
  0.500  3  0.8535027  0.7136022  0.9379310

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.5 and M = 2.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     15.3      5.1
  positive      5.6     73.9
                            
 Accuracy (average) : 0.8926

[1] "TRAIN accuracy: 0.892584514721919"
[1] "TRAIN +precision: 0.929403701165182"
[1] "TRAIN -precision: 0.749333333333333"
[1] "TRAIN specifity: 0.731770833333333"
[1] "TRAIN sensitivity: 0.935172413793103"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       38       28
            positive       38      469
[1] "TEST accuracy: 0.884816753926702"
[1] "TEST +precision: 0.925049309664694"
[1] "TEST -precision: 0.575757575757576"
[1] "TEST specifity: 0.5"
[1] "TEST sensitivity: 0.943661971830986"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 5.75194965203603"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

1834 samples
 638 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1467, 1467, 1467, 1468, 1467 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens       Spec     
  0.3  1          0.6               0.50        50      0.9492393  0.5471292  0.9910345
  0.3  1          0.6               0.50       100      0.9658535  0.6981887  0.9868966
  0.3  1          0.6               0.50       150      0.9695326  0.7371497  0.9868966
  0.3  1          0.6               0.75        50      0.9562884  0.5679768  0.9931034
  0.3  1          0.6               0.75       100      0.9654003  0.6643541  0.9896552
  0.3  1          0.6               0.75       150      0.9706621  0.7293917  0.9875862
  0.3  1          0.6               1.00        50      0.9554469  0.5315448  0.9931034
  0.3  1          0.6               1.00       100      0.9664409  0.6487013  0.9917241
  0.3  1          0.6               1.00       150      0.9711130  0.7112440  0.9889655
  0.3  1          0.8               0.50        50      0.9506321  0.5393712  0.9924138
  0.3  1          0.8               0.50       100      0.9649408  0.6850991  0.9868966
  0.3  1          0.8               0.50       150      0.9680656  0.7449077  0.9855172
  0.3  1          0.8               0.75        50      0.9527996  0.5497266  0.9924138
  0.3  1          0.8               0.75       100      0.9646184  0.6748120  0.9882759
  0.3  1          0.8               0.75       150      0.9697471  0.7450444  0.9875862
  0.3  1          0.8               1.00        50      0.9533210  0.5315448  0.9924138
  0.3  1          0.8               1.00       100      0.9662410  0.6434040  0.9910345
  0.3  1          0.8               1.00       150      0.9709638  0.7138756  0.9882759
  0.3  2          0.6               0.50        50      0.9627750  0.7086466  0.9882759
  0.3  2          0.6               0.50       100      0.9664748  0.7866370  0.9882759
  0.3  2          0.6               0.50       150      0.9692750  0.8204375  0.9841379
  0.3  2          0.6               0.75        50      0.9638718  0.6981545  0.9896552
  0.3  2          0.6               0.75       100      0.9713345  0.7788107  0.9855172
  0.3  2          0.6               0.75       150      0.9727804  0.8074163  0.9806897
  0.3  2          0.6               1.00        50      0.9656122  0.6747437  0.9889655
  0.3  2          0.6               1.00       100      0.9712040  0.7762816  0.9868966
  0.3  2          0.6               1.00       150      0.9723404  0.8048872  0.9848276
  0.3  2          0.8               0.50        50      0.9649419  0.6928230  0.9910345
  0.3  2          0.8               0.50       100      0.9710216  0.7891661  0.9889655
  0.3  2          0.8               0.50       150      0.9709520  0.7996582  0.9834483
  0.3  2          0.8               0.75        50      0.9676404  0.7008202  0.9868966
  0.3  2          0.8               0.75       100      0.9727795  0.7762474  0.9848276
  0.3  2          0.8               0.75       150      0.9739576  0.8282638  0.9827586
  0.3  2          0.8               1.00        50      0.9668716  0.6852016  0.9889655
  0.3  2          0.8               1.00       100      0.9714868  0.7633288  0.9882759
  0.3  2          0.8               1.00       150      0.9736341  0.8127478  0.9848276
  0.3  3          0.6               0.50        50      0.9702287  0.7633288  0.9862069
  0.3  3          0.6               0.50       100      0.9742440  0.8204716  0.9848276
  0.3  3          0.6               0.50       150      0.9707763  0.8438483  0.9813793
  0.3  3          0.6               0.75        50      0.9694601  0.7659604  0.9834483
  0.3  3          0.6               0.75       100      0.9725741  0.8256664  0.9820690
  0.3  3          0.6               0.75       150      0.9712486  0.8360560  0.9806897
  0.3  3          0.6               1.00        50      0.9721746  0.7763158  0.9882759
  0.3  3          0.6               1.00       100      0.9737625  0.8179426  0.9862069
  0.3  3          0.6               1.00       150      0.9765693  0.8334928  0.9855172
  0.3  3          0.8               0.50        50      0.9716064  0.7528025  0.9875862
  0.3  3          0.8               0.50       100      0.9729574  0.8099795  0.9848276
  0.3  3          0.8               0.50       150      0.9716968  0.8255981  0.9779310
  0.3  3          0.8               0.75        50      0.9677748  0.7580656  0.9889655
  0.3  3          0.8               0.75       100      0.9717481  0.8075188  0.9875862
  0.3  3          0.8               0.75       150      0.9736473  0.8464457  0.9800000
  0.3  3          0.8               1.00        50      0.9687800  0.7267601  0.9855172
  0.3  3          0.8               1.00       100      0.9730713  0.8126452  0.9841379
  0.3  3          0.8               1.00       150      0.9731812  0.8360560  0.9834483
  0.4  1          0.6               0.50        50      0.9537349  0.6123035  0.9862069
  0.4  1          0.6               0.50       100      0.9636857  0.7164730  0.9910345
  0.4  1          0.6               0.50       150      0.9673956  0.7657211  0.9848276
  0.4  1          0.6               0.75        50      0.9592816  0.5731716  0.9896552
  0.4  1          0.6               0.75       100      0.9658194  0.7214969  0.9868966
  0.4  1          0.6               0.75       150      0.9702000  0.7736500  0.9848276
  0.4  1          0.6               1.00        50      0.9582844  0.5835954  0.9910345
  0.4  1          0.6               1.00       100      0.9684104  0.7163705  0.9882759
  0.4  1          0.6               1.00       150      0.9723175  0.7580656  0.9875862
  0.4  1          0.8               0.50        50      0.9564063  0.6070403  0.9875862
  0.4  1          0.8               0.50       100      0.9662531  0.7371497  0.9875862
  0.4  1          0.8               0.50       150      0.9669959  0.7553315  0.9800000
  0.4  1          0.8               0.75        50      0.9577026  0.5913876  0.9910345
  0.4  1          0.8               0.75       100      0.9682558  0.7267259  0.9910345
  0.4  1          0.8               0.75       150      0.9721638  0.7763841  0.9848276
  0.4  1          0.8               1.00        50      0.9589079  0.6043746  0.9931034
  0.4  1          0.8               1.00       100      0.9679222  0.7007861  0.9875862
  0.4  1          0.8               1.00       150      0.9724700  0.7450103  0.9875862
  0.4  2          0.6               0.50        50      0.9681065  0.7527341  0.9848276
  0.4  2          0.6               0.50       100      0.9715052  0.7814081  0.9786207
  0.4  2          0.6               0.50       150      0.9707300  0.8256323  0.9758621
  0.4  2          0.6               0.75        50      0.9692387  0.7423787  0.9903448
  0.4  2          0.6               0.75       100      0.9737099  0.8256323  0.9848276
  0.4  2          0.6               0.75       150      0.9739254  0.8438141  0.9800000
  0.4  2          0.6               1.00        50      0.9705025  0.7293917  0.9862069
  0.4  2          0.6               1.00       100      0.9741089  0.8023240  0.9868966
  0.4  2          0.6               1.00       150      0.9746575  0.8205058  0.9862069
  0.4  2          0.8               0.50        50      0.9660315  0.7528366  0.9875862
  0.4  2          0.8               0.50       100      0.9697915  0.8047163  0.9813793
  0.4  2          0.8               0.50       150      0.9679626  0.8359877  0.9751724
  0.4  2          0.8               0.75        50      0.9691211  0.7477444  0.9903448
  0.4  2          0.8               0.75       100      0.9719164  0.8177717  0.9855172
  0.4  2          0.8               0.75       150      0.9738811  0.8308271  0.9820690
  0.4  2          0.8               1.00        50      0.9685439  0.7293917  0.9834483
  0.4  2          0.8               1.00       100      0.9753564  0.8127136  0.9862069
  0.4  2          0.8               1.00       150      0.9761939  0.8282638  0.9862069
  0.4  3          0.6               0.50        50      0.9670117  0.7656869  0.9841379
  0.4  3          0.6               0.50       100      0.9688002  0.8308612  0.9731034
  0.4  3          0.6               0.50       150      0.9666868  0.8281955  0.9744828
  0.4  3          0.6               0.75        50      0.9712662  0.8022898  0.9868966
  0.4  3          0.6               0.75       100      0.9716227  0.8308612  0.9772414
  0.4  3          0.6               0.75       150      0.9699880  0.8413192  0.9772414
  0.4  3          0.6               1.00        50      0.9710166  0.8048530  0.9862069
  0.4  3          0.6               1.00       100      0.9759645  0.8360902  0.9827586
  0.4  3          0.6               1.00       150      0.9753986  0.8412850  0.9820690
  0.4  3          0.8               0.50        50      0.9684734  0.7552632  0.9841379
  0.4  3          0.8               0.50       100      0.9714553  0.8230007  0.9779310
  0.4  3          0.8               0.50       150      0.9659212  0.8308271  0.9731034
  0.4  3          0.8               0.75        50      0.9720692  0.7867054  0.9841379
  0.4  3          0.8               0.75       100      0.9723354  0.8282980  0.9806897
  0.4  3          0.8               0.75       150      0.9697556  0.8516405  0.9800000
  0.4  3          0.8               1.00        50      0.9719356  0.7867738  0.9882759
  0.4  3          0.8               1.00       100      0.9753566  0.8360902  0.9827586
  0.4  3          0.8               1.00       150      0.9746922  0.8386876  0.9834483

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 150, max_depth = 3, eta = 0.3, gamma = 0, colsample_bytree = 0.6, min_child_weight = 1 and subsample = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     17.4      1.1
  positive      3.5     77.9
                            
 Accuracy (average) : 0.9537

[1] "TRAIN accuracy: 0.953653217011996"
[1] "TRAIN +precision: 0.957133288680509"
[1] "TRAIN -precision: 0.93841642228739"
[1] "TRAIN specifity: 0.833333333333333"
[1] "TRAIN sensitivity: 0.98551724137931"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       54        8
            positive       22      489
[1] "TEST accuracy: 0.947643979057592"
[1] "TEST +precision: 0.956947162426614"
[1] "TEST -precision: 0.870967741935484"
[1] "TEST specifity: 0.710526315789474"
[1] "TEST sensitivity: 0.983903420523139"

[1] "DATASET NAME: MSM_Bi_IR_1"
[1] "TRAIN INSTANCES: 3468"
[1] "TEST INSTANCES: 600"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 22.7903549671173"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

3468 samples
 583 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 2775, 2774, 2774, 2775, 2774 
Resampling results:

  ROC        Sens      Spec     
  0.9816595  0.973487  0.9492246

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     48.7      2.5
  positive      1.3     47.5
                            
 Accuracy (average) : 0.9614

[1] "TRAIN accuracy: 0.961361014994233"
[1] "TRAIN +precision: 0.972813238770686"
[1] "TRAIN -precision: 0.95045045045045"
[1] "TRAIN specifity: 0.973471741637832"
[1] "TRAIN sensitivity: 0.949250288350634"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        8       40
            positive       10      542
[1] "TEST accuracy: 0.916666666666667"
[1] "TEST +precision: 0.981884057971015"
[1] "TEST -precision: 0.166666666666667"
[1] "TEST specifity: 0.444444444444444"
[1] "TEST sensitivity: 0.93127147766323"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 5.16652340094248"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

3468 samples
 583 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 2775, 2774, 2774, 2775, 2774 
Resampling results across tuning parameters:

  C      M  ROC        Sens       Spec     
  0.010  1  0.9576589  0.9307841  0.9723193
  0.010  2  0.9574627  0.9307841  0.9723193
  0.010  3  0.9575457  0.9307841  0.9717429
  0.255  1  0.9578008  0.9307841  0.9723193
  0.255  2  0.9576045  0.9307841  0.9723193
  0.255  3  0.9576527  0.9307841  0.9717429
  0.500  1  0.9653738  0.9307841  0.9803951
  0.500  2  0.9652524  0.9307841  0.9803951
  0.500  3  0.9646975  0.9307841  0.9780880

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.5 and M = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     46.5      1.0
  positive      3.5     49.0
                            
 Accuracy (average) : 0.9556

[1] "TRAIN accuracy: 0.955594002306805"
[1] "TRAIN +precision: 0.934065934065934"
[1] "TRAIN -precision: 0.979368932038835"
[1] "TRAIN specifity: 0.930795847750865"
[1] "TRAIN sensitivity: 0.980392156862745"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        2        8
            positive       16      574
[1] "TEST accuracy: 0.96"
[1] "TEST +precision: 0.972881355932203"
[1] "TEST -precision: 0.2"
[1] "TEST specifity: 0.111111111111111"
[1] "TEST sensitivity: 0.986254295532646"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 10.0819825649261"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

3468 samples
 583 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 2774, 2774, 2774, 2774, 2776 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens       Spec     
  0.3  1          0.6               0.50        50      0.9273409  0.7583632  0.9538622
  0.3  1          0.6               0.50       100      0.9640025  0.8667872  0.9521314
  0.3  1          0.6               0.50       150      0.9729208  0.9267678  0.9498243
  0.3  1          0.6               0.75        50      0.9261867  0.7548966  0.9636671
  0.3  1          0.6               0.75       100      0.9683854  0.8494744  0.9561776
  0.3  1          0.6               0.75       150      0.9747517  0.9158118  0.9561693
  0.3  1          0.6               1.00        50      0.9245228  0.7531575  0.9613633
  0.3  1          0.6               1.00       100      0.9669439  0.8460362  0.9573270
  0.3  1          0.6               1.00       150      0.9755012  0.9158118  0.9579034
  0.3  1          0.8               0.50        50      0.9232851  0.7698889  0.9555929
  0.3  1          0.8               0.50       100      0.9672457  0.8644750  0.9527078
  0.3  1          0.8               0.50       150      0.9729140  0.9481035  0.9452150
  0.3  1          0.8               0.75        50      0.9294611  0.7687461  0.9579001
  0.3  1          0.8               0.75       100      0.9683476  0.8269961  0.9596309
  0.3  1          0.8               0.75       150      0.9739805  0.9290683  0.9550216
  0.3  1          0.8               1.00        50      0.9236475  0.7479702  0.9630924
  0.3  1          0.8               1.00       100      0.9657271  0.8517999  0.9573254
  0.3  1          0.8               1.00       150      0.9751522  0.9163882  0.9590562
  0.3  2          0.6               0.50        50      0.9685347  0.8811964  0.9602089
  0.3  2          0.6               0.50       100      0.9797550  0.9515617  0.9550182
  0.3  2          0.6               0.50       150      0.9828266  0.9573254  0.9555896
  0.3  2          0.6               0.75        50      0.9724744  0.8812180  0.9630874
  0.3  2          0.6               0.75       100      0.9828503  0.9440689  0.9596292
  0.3  2          0.6               0.75       150      0.9855553  0.9573254  0.9607869
  0.3  2          0.6               1.00        50      0.9723259  0.8788942  0.9653979
  0.3  2          0.6               1.00       100      0.9838683  0.9504090  0.9630907
  0.3  2          0.6               1.00       150      0.9870824  0.9538672  0.9619380
  0.3  2          0.8               0.50        50      0.9718327  0.8656394  0.9596309
  0.3  2          0.8               0.50       100      0.9804924  0.9515617  0.9573254
  0.3  2          0.8               0.50       150      0.9836277  0.9573254  0.9544419
  0.3  2          0.8               0.75        50      0.9725823  0.8783212  0.9602089
  0.3  2          0.8               0.75       100      0.9819978  0.9509853  0.9561693
  0.3  2          0.8               0.75       150      0.9855751  0.9573254  0.9607853
  0.3  2          0.8               1.00        50      0.9718439  0.8933268  0.9584798
  0.3  2          0.8               1.00       100      0.9839955  0.9457930  0.9619363
  0.3  2          0.8               1.00       150      0.9868159  0.9573254  0.9630907
  0.3  3          0.6               0.50        50      0.9800084  0.9429162  0.9590562
  0.3  3          0.6               0.50       100      0.9852688  0.9573254  0.9602039
  0.3  3          0.6               0.50       150      0.9875539  0.9573254  0.9613583
  0.3  3          0.6               0.75        50      0.9805265  0.9429112  0.9636654
  0.3  3          0.6               0.75       100      0.9862068  0.9538672  0.9694291
  0.3  3          0.6               0.75       150      0.9891989  0.9573254  0.9711599
  0.3  3          0.6               1.00        50      0.9814891  0.9394529  0.9653979
  0.3  3          0.6               1.00       100      0.9879209  0.9573254  0.9705819
  0.3  3          0.6               1.00       150      0.9909665  0.9573254  0.9717363
  0.3  3          0.8               0.50        50      0.9781825  0.9429162  0.9567507
  0.3  3          0.8               0.50       100      0.9849751  0.9573254  0.9590545
  0.3  3          0.8               0.50       150      0.9881559  0.9573254  0.9555963
  0.3  3          0.8               0.75        50      0.9814972  0.9411871  0.9648165
  0.3  3          0.8               0.75       100      0.9864807  0.9573254  0.9665473
  0.3  3          0.8               0.75       150      0.9892308  0.9573254  0.9677017
  0.3  3          0.8               1.00        50      0.9826199  0.9394480  0.9682764
  0.3  3          0.8               1.00       100      0.9882528  0.9573254  0.9700088
  0.3  3          0.8               1.00       150      0.9912098  0.9573254  0.9700088
  0.4  1          0.6               0.50        50      0.9406108  0.8356466  0.9452133
  0.4  1          0.6               0.50       100      0.9697158  0.8979211  0.9434842
  0.4  1          0.6               0.50       150      0.9760576  0.9463694  0.9446403
  0.4  1          0.6               0.75        50      0.9442791  0.8408139  0.9509887
  0.4  1          0.6               0.75       100      0.9728058  0.9135014  0.9527128
  0.4  1          0.6               0.75       150      0.9773890  0.9463694  0.9486765
  0.4  1          0.6               1.00        50      0.9432543  0.8085289  0.9555979
  0.4  1          0.6               1.00       100      0.9727032  0.9065849  0.9550199
  0.4  1          0.6               1.00       150      0.9788501  0.9463694  0.9515567
  0.4  1          0.8               0.50        50      0.9425092  0.8183755  0.9486732
  0.4  1          0.8               0.50       100      0.9712588  0.9088821  0.9486798
  0.4  1          0.8               0.50       150      0.9762659  0.9480985  0.9423331
  0.4  1          0.8               0.75        50      0.9481115  0.8414053  0.9515617
  0.4  1          0.8               0.75       100      0.9724671  0.9042795  0.9527144
  0.4  1          0.8               0.75       150      0.9768679  0.9463694  0.9475238
  0.4  1          0.8               1.00        50      0.9442459  0.8131399  0.9561743
  0.4  1          0.8               1.00       100      0.9737966  0.9065849  0.9579018
  0.4  1          0.8               1.00       150      0.9787089  0.9463694  0.9515567
  0.4  2          0.6               0.50        50      0.9729170  0.9077377  0.9475188
  0.4  2          0.6               0.50       100      0.9807573  0.9573254  0.9486715
  0.4  2          0.6               0.50       150      0.9834712  0.9573254  0.9532875
  0.4  2          0.6               0.75        50      0.9778764  0.9250337  0.9544352
  0.4  2          0.6               0.75       100      0.9849704  0.9573254  0.9579018
  0.4  2          0.6               0.75       150      0.9876319  0.9573254  0.9579018
  0.4  2          0.6               1.00        50      0.9784747  0.9227333  0.9630891
  0.4  2          0.6               1.00       100      0.9854800  0.9498326  0.9648198
  0.4  2          0.6               1.00       150      0.9884698  0.9573254  0.9671253
  0.4  2          0.8               0.50        50      0.9719829  0.9060136  0.9509787
  0.4  2          0.8               0.50       100      0.9809915  0.9573254  0.9480902
  0.4  2          0.8               0.50       150      0.9846196  0.9573254  0.9555946
  0.4  2          0.8               0.75        50      0.9758827  0.9250487  0.9602106
  0.4  2          0.8               0.75       100      0.9840811  0.9573254  0.9642435
  0.4  2          0.8               0.75       150      0.9875531  0.9573254  0.9596309
  0.4  2          0.8               1.00        50      0.9800593  0.9267595  0.9625144
  0.4  2          0.8               1.00       100      0.9865782  0.9538672  0.9659759
  0.4  2          0.8               1.00       150      0.9892502  0.9573254  0.9671253
  0.4  3          0.6               0.50        50      0.9818393  0.9538672  0.9607769
  0.4  3          0.6               0.50       100      0.9860998  0.9573254  0.9630891
  0.4  3          0.6               0.50       150      0.9882766  0.9573254  0.9625060
  0.4  3          0.6               0.75        50      0.9837259  0.9521381  0.9590562
  0.4  3          0.6               0.75       100      0.9890598  0.9573254  0.9671253
  0.4  3          0.6               0.75       150      0.9909908  0.9573254  0.9688544
  0.4  3          0.6               1.00        50      0.9835382  0.9521381  0.9694291
  0.4  3          0.6               1.00       100      0.9897103  0.9573254  0.9705869
  0.4  3          0.6               1.00       150      0.9923451  0.9573254  0.9734704
  0.4  3          0.8               0.50        50      0.9808574  0.9538672  0.9596259
  0.4  3          0.8               0.50       100      0.9861177  0.9573254  0.9584748
  0.4  3          0.8               0.50       150      0.9886499  0.9573254  0.9584731
  0.4  3          0.8               0.75        50      0.9839106  0.9515617  0.9613583
  0.4  3          0.8               0.75       100      0.9884262  0.9573254  0.9607819
  0.4  3          0.8               0.75       150      0.9908548  0.9573254  0.9648182
  0.4  3          0.8               1.00        50      0.9850127  0.9515617  0.9688528
  0.4  3          0.8               1.00       100      0.9900993  0.9573254  0.9705852
  0.4  3          0.8               1.00       150      0.9925106  0.9573254  0.9723126

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 150, max_depth = 3, eta = 0.4, gamma = 0, colsample_bytree = 0.8, min_child_weight = 1 and subsample = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     47.9      1.4
  positive      2.1     48.6
                            
 Accuracy (average) : 0.9648

[1] "TRAIN accuracy: 0.96482122260669"
[1] "TRAIN +precision: 0.957954545454546"
[1] "TRAIN -precision: 0.971896955503513"
[1] "TRAIN specifity: 0.957324106113033"
[1] "TRAIN sensitivity: 0.972318339100346"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        4        9
            positive       14      573
[1] "TEST accuracy: 0.961666666666667"
[1] "TEST +precision: 0.976149914821124"
[1] "TEST -precision: 0.307692307692308"
[1] "TEST specifity: 0.222222222222222"
[1] "TEST sensitivity: 0.984536082474227"

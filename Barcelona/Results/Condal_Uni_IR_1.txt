[1] "DATASET NAME: Condal_Uni_IR_1"
[1] "TRAIN INSTANCES: 4542"
[1] "TEST INSTANCES: 795"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 23.7934520244598"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

4542 samples
 652 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 3634, 3634, 3634, 3632, 3634 
Resampling results:

  ROC       Sens  Spec     
  0.998593  1     0.9947146

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     50.0      0.3
  positive      0.0     49.7
                            
 Accuracy (average) : 0.9974

[1] "TRAIN accuracy: 0.997357992073976"
[1] "TRAIN +precision: 1"
[1] "TRAIN -precision: 0.994743758212878"
[1] "TRAIN specifity: 1"
[1] "TRAIN sensitivity: 0.994715984147952"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       20        4
            positive       19      752
[1] "TEST accuracy: 0.971069182389937"
[1] "TEST +precision: 0.975356679636835"
[1] "TEST -precision: 0.833333333333333"
[1] "TEST specifity: 0.512820512820513"
[1] "TEST sensitivity: 0.994708994708995"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 9.62905079921087"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

4542 samples
 652 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 3634, 3634, 3633, 3633, 3634 
Resampling results across tuning parameters:

  C      M  ROC        Sens  Spec     
  0.010  1  0.9879251  1     0.9674193
  0.010  2  0.9879251  1     0.9674193
  0.010  3  0.9888486  1     0.9669787
  0.255  1  0.9867977  1     0.9709425
  0.255  2  0.9867977  1     0.9709425
  0.255  3  0.9878702  1     0.9705020
  0.500  1  0.9878486  1     0.9749044
  0.500  2  0.9878486  1     0.9749044
  0.500  3  0.9878702  1     0.9705020

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.01 and M = 3.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     50.0      1.7
  positive      0.0     48.3
                            
 Accuracy (average) : 0.9835

[1] "TRAIN accuracy: 0.983487450462351"
[1] "TRAIN +precision: 1"
[1] "TRAIN -precision: 0.968030690537084"
[1] "TRAIN specifity: 1"
[1] "TRAIN sensitivity: 0.966974900924703"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       16       20
            positive       23      736
[1] "TEST accuracy: 0.945911949685535"
[1] "TEST +precision: 0.96969696969697"
[1] "TEST -precision: 0.444444444444444"
[1] "TEST specifity: 0.41025641025641"
[1] "TEST sensitivity: 0.973544973544973"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 14.7588849027952"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

4542 samples
 652 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 3633, 3634, 3633, 3634, 3634 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens       Spec     
  0.3  1          0.6               0.50        50      0.9739483  0.8881532  0.9031282
  0.3  1          0.6               0.50       100      0.9926039  0.9766626  0.9423150
  0.3  1          0.6               0.50       150      0.9959508  0.9955976  0.9568485
  0.3  1          0.6               0.75        50      0.9749472  0.9220632  0.9088503
  0.3  1          0.6               0.75       100      0.9925215  0.9845883  0.9493595
  0.3  1          0.6               0.75       150      0.9962030  0.9925139  0.9608113
  0.3  1          0.6               1.00        50      0.9755295  0.9207349  0.9044440
  0.3  1          0.6               1.00       100      0.9930117  0.9815046  0.9484804
  0.3  1          0.6               1.00       150      0.9962854  0.9925139  0.9590502
  0.3  1          0.8               0.50        50      0.9737479  0.9150177  0.9062061
  0.3  1          0.8               0.50       100      0.9918372  0.9819490  0.9462787
  0.3  1          0.8               0.50       150      0.9957860  0.9938355  0.9568476
  0.3  1          0.8               0.75        50      0.9746652  0.9172145  0.9075316
  0.3  1          0.8               0.75       100      0.9926003  0.9845883  0.9480409
  0.3  1          0.8               0.75       150      0.9958345  0.9964758  0.9616905
  0.3  1          0.8               1.00        50      0.9750146  0.9141337  0.9070872
  0.3  1          0.8               1.00       100      0.9927389  0.9837072  0.9484814
  0.3  1          0.8               1.00       150      0.9964134  0.9933950  0.9608123
  0.3  2          0.6               0.50        50      0.9950241  0.9903161  0.9445215
  0.3  2          0.6               0.50       100      0.9981776  1.0000000  0.9678588
  0.3  2          0.6               0.50       150      0.9987718  1.0000000  0.9753440
  0.3  2          0.6               0.75        50      0.9947031  0.9933950  0.9445186
  0.3  2          0.6               0.75       100      0.9984443  1.0000000  0.9713811
  0.3  2          0.6               0.75       150      0.9989181  1.0000000  0.9801878
  0.3  2          0.6               1.00        50      0.9953459  0.9938365  0.9489229
  0.3  2          0.6               1.00       100      0.9979665  1.0000000  0.9722632
  0.3  2          0.6               1.00       150      0.9985865  1.0000000  0.9797492
  0.3  2          0.8               0.50        50      0.9957800  0.9955986  0.9436375
  0.3  2          0.8               0.50       100      0.9985328  1.0000000  0.9709396
  0.3  2          0.8               0.50       150      0.9989298  1.0000000  0.9815084
  0.3  2          0.8               0.75        50      0.9948386  0.9947175  0.9458431
  0.3  2          0.8               0.75       100      0.9981834  1.0000000  0.9691804
  0.3  2          0.8               0.75       150      0.9986088  1.0000000  0.9801888
  0.3  2          0.8               1.00        50      0.9953113  0.9973568  0.9476052
  0.3  2          0.8               1.00       100      0.9983720  1.0000000  0.9709406
  0.3  2          0.8               1.00       150      0.9987595  1.0000000  0.9788672
  0.3  3          0.6               0.50        50      0.9974423  1.0000000  0.9643336
  0.3  3          0.6               0.50       100      0.9985103  1.0000000  0.9801849
  0.3  3          0.6               0.50       150      0.9989286  1.0000000  0.9828281
  0.3  3          0.6               0.75        50      0.9979074  1.0000000  0.9625706
  0.3  3          0.6               0.75       100      0.9989590  1.0000000  0.9815114
  0.3  3          0.6               0.75       150      0.9991633  1.0000000  0.9863533
  0.3  3          0.6               1.00        50      0.9978475  1.0000000  0.9656581
  0.3  3          0.6               1.00       100      0.9989338  1.0000000  0.9823885
  0.3  3          0.6               1.00       150      0.9992486  1.0000000  0.9854713
  0.3  3          0.8               0.50        50      0.9978214  1.0000000  0.9634545
  0.3  3          0.8               0.50       100      0.9988493  1.0000000  0.9784257
  0.3  3          0.8               0.50       150      0.9990069  1.0000000  0.9823895
  0.3  3          0.8               0.75        50      0.9979743  1.0000000  0.9643346
  0.3  3          0.8               0.75       100      0.9990540  1.0000000  0.9819470
  0.3  3          0.8               0.75       150      0.9992467  1.0000000  0.9876729
  0.3  3          0.8               1.00        50      0.9984409  1.0000000  0.9630130
  0.3  3          0.8               1.00       100      0.9992041  1.0000000  0.9815094
  0.3  3          0.8               1.00       150      0.9994811  1.0000000  0.9872353
  0.4  1          0.6               0.50        50      0.9828013  0.9374682  0.9282248
  0.4  1          0.6               0.50       100      0.9943919  0.9911923  0.9524432
  0.4  1          0.6               0.50       150      0.9967830  0.9977974  0.9630140
  0.4  1          0.6               0.75        50      0.9841334  0.9445176  0.9291107
  0.4  1          0.6               0.75       100      0.9954521  0.9929554  0.9568476
  0.4  1          0.6               0.75       150      0.9975221  0.9982379  0.9678579
  0.4  1          0.6               1.00        50      0.9836908  0.9449475  0.9313075
  0.4  1          0.6               1.00       100      0.9955548  0.9933950  0.9594888
  0.4  1          0.6               1.00       150      0.9972053  1.0000000  0.9665382
  0.4  1          0.8               0.50        50      0.9810258  0.9502483  0.9203030
  0.4  1          0.8               0.50       100      0.9941809  0.9885530  0.9515641
  0.4  1          0.8               0.50       150      0.9968075  0.9977974  0.9634545
  0.4  1          0.8               0.75        50      0.9829355  0.9471598  0.9216149
  0.4  1          0.8               0.75       100      0.9951633  0.9947166  0.9542054
  0.4  1          0.8               0.75       150      0.9969876  1.0000000  0.9669778
  0.4  1          0.8               1.00        50      0.9844703  0.9537590  0.9220564
  0.4  1          0.8               1.00       100      0.9954458  0.9889897  0.9559665
  0.4  1          0.8               1.00       150      0.9971714  1.0000000  0.9660958
  0.4  2          0.6               0.50        50      0.9961215  0.9969202  0.9555269
  0.4  2          0.6               0.50       100      0.9986638  1.0000000  0.9722583
  0.4  2          0.6               0.50       150      0.9988799  1.0000000  0.9815084
  0.4  2          0.6               0.75        50      0.9970817  1.0000000  0.9599293
  0.4  2          0.6               0.75       100      0.9986412  1.0000000  0.9788672
  0.4  2          0.6               0.75       150      0.9989347  1.0000000  0.9823914
  0.4  2          0.6               1.00        50      0.9973040  1.0000000  0.9559675
  0.4  2          0.6               1.00       100      0.9987235  1.0000000  0.9788682
  0.4  2          0.6               1.00       150      0.9989510  1.0000000  0.9832706
  0.4  2          0.8               0.50        50      0.9964128  0.9977974  0.9559646
  0.4  2          0.8               0.50       100      0.9984490  1.0000000  0.9757835
  0.4  2          0.8               0.50       150      0.9989645  1.0000000  0.9819509
  0.4  2          0.8               0.75        50      0.9967164  1.0000000  0.9555260
  0.4  2          0.8               0.75       100      0.9987140  1.0000000  0.9771061
  0.4  2          0.8               0.75       150      0.9988349  1.0000000  0.9845931
  0.4  2          0.8               1.00        50      0.9965161  1.0000000  0.9590512
  0.4  2          0.8               1.00       100      0.9985629  1.0000000  0.9766665
  0.4  2          0.8               1.00       150      0.9990627  1.0000000  0.9810689
  0.4  3          0.6               0.50        50      0.9982414  1.0000000  0.9704943
  0.4  3          0.6               0.50       100      0.9990750  1.0000000  0.9815084
  0.4  3          0.6               0.50       150      0.9989489  1.0000000  0.9837092
  0.4  3          0.6               0.75        50      0.9984700  1.0000000  0.9696190
  0.4  3          0.6               0.75       100      0.9993852  1.0000000  0.9850336
  0.4  3          0.6               0.75       150      0.9994558  1.0000000  0.9859137
  0.4  3          0.6               1.00        50      0.9985091  1.0000000  0.9735809
  0.4  3          0.6               1.00       100      0.9992495  1.0000000  0.9845912
  0.4  3          0.6               1.00       150      0.9993694  1.0000000  0.9867929
  0.4  3          0.8               0.50        50      0.9988203  1.0000000  0.9705001
  0.4  3          0.8               0.50       100      0.9990267  1.0000000  0.9837092
  0.4  3          0.8               0.50       150      0.9988684  1.0000000  0.9854703
  0.4  3          0.8               0.75        50      0.9983723  1.0000000  0.9704991
  0.4  3          0.8               0.75       100      0.9988871  1.0000000  0.9828300
  0.4  3          0.8               0.75       150      0.9988072  1.0000000  0.9889926
  0.4  3          0.8               1.00        50      0.9988473  1.0000000  0.9705010
  0.4  3          0.8               1.00       100      0.9994461  1.0000000  0.9823895
  0.4  3          0.8               1.00       150      0.9995164  1.0000000  0.9867938

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 150, max_depth = 3, eta = 0.4, gamma = 0, colsample_bytree = 0.8, min_child_weight = 1 and subsample = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     50.0      0.7
  positive      0.0     49.3
                            
 Accuracy (average) : 0.9934

[1] "TRAIN accuracy: 0.993394980184941"
[1] "TRAIN +precision: 1"
[1] "TRAIN -precision: 0.986962190352021"
[1] "TRAIN specifity: 1"
[1] "TRAIN sensitivity: 0.986789960369881"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       22        9
            positive       17      747
[1] "TEST accuracy: 0.967295597484277"
[1] "TEST +precision: 0.977748691099476"
[1] "TEST -precision: 0.709677419354839"
[1] "TEST specifity: 0.564102564102564"
[1] "TEST sensitivity: 0.988095238095238"

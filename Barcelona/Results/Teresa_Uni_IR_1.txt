[1] "DATASET NAME: Teresa_Uni_IR_1"
[1] "TRAIN INSTANCES: 2172"
[1] "TEST INSTANCES: 379"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 5.97945189476013"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

2172 samples
 698 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1738, 1736, 1738, 1738, 1738 
Resampling results:

  ROC        Sens  Spec     
  0.9992143  1     0.9963176

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     50.0      0.2
  positive      0.0     49.8
                            
 Accuracy (average) : 0.9982

[1] "TRAIN accuracy: 0.998158379373849"
[1] "TRAIN +precision: 1"
[1] "TRAIN -precision: 0.996330275229358"
[1] "TRAIN specifity: 1"
[1] "TRAIN sensitivity: 0.996316758747698"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        9        0
            positive        9      361
[1] "TEST accuracy: 0.976253298153034"
[1] "TEST +precision: 0.975675675675676"
[1] "TEST -precision: 1"
[1] "TEST specifity: 0.5"
[1] "TEST sensitivity: 1"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 2.63410929838816"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

2172 samples
 698 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1737, 1738, 1738, 1737, 1738 
Resampling results across tuning parameters:

  C      M  ROC        Sens  Spec     
  0.010  1  0.9844141  1     0.9640722
  0.010  2  0.9844141  1     0.9640722
  0.010  3  0.9844749  1     0.9631506
  0.255  1  0.9847531  1     0.9640722
  0.255  2  0.9847531  1     0.9640722
  0.255  3  0.9851945  1     0.9640722
  0.500  1  0.9865137  1     0.9686763
  0.500  2  0.9865137  1     0.9686763
  0.500  3  0.9851277  1     0.9659155

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.5 and M = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     50.0      1.6
  positive      0.0     48.4
                            
 Accuracy (average) : 0.9843

[1] "TRAIN accuracy: 0.984346224677716"
[1] "TRAIN +precision: 1"
[1] "TRAIN -precision: 0.969642857142857"
[1] "TRAIN specifity: 1"
[1] "TRAIN sensitivity: 0.968692449355433"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        9        7
            positive        9      354
[1] "TEST accuracy: 0.95778364116095"
[1] "TEST +precision: 0.975206611570248"
[1] "TEST -precision: 0.5625"
[1] "TEST specifity: 0.5"
[1] "TEST sensitivity: 0.980609418282548"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 7.25695266326268"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

2172 samples
 698 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1738, 1737, 1737, 1738, 1738 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens       Spec     
  0.3  1          0.6               0.50        50      0.9881527  0.9622500  0.9392255
  0.3  1          0.6               0.50       100      0.9959752  0.9935484  0.9603983
  0.3  1          0.6               0.50       150      0.9974974  1.0000000  0.9659240
  0.3  1          0.6               0.75        50      0.9896300  0.9585423  0.9465861
  0.3  1          0.6               0.75       100      0.9968592  0.9953917  0.9622416
  0.3  1          0.6               0.75       150      0.9971167  1.0000000  0.9723629
  0.3  1          0.6               1.00        50      0.9894881  0.9585634  0.9465776
  0.3  1          0.6               1.00       100      0.9962781  0.9908003  0.9677546
  0.3  1          0.6               1.00       150      0.9970070  1.0000000  0.9742062
  0.3  1          0.8               0.50        50      0.9879388  0.9576417  0.9410603
  0.3  1          0.8               0.50       100      0.9959507  1.0000000  0.9594639
  0.3  1          0.8               0.50       150      0.9975036  1.0000000  0.9686890
  0.3  1          0.8               0.75        50      0.9894305  0.9650150  0.9447470
  0.3  1          0.8               0.75       100      0.9965447  1.0000000  0.9613157
  0.3  1          0.8               0.75       150      0.9973974  1.0000000  0.9705281
  0.3  1          0.8               1.00        50      0.9899551  0.9585634  0.9530250
  0.3  1          0.8               1.00       100      0.9959307  0.9953917  0.9659155
  0.3  1          0.8               1.00       150      0.9968124  1.0000000  0.9732846
  0.3  2          0.6               0.50        50      0.9985052  1.0000000  0.9696106
  0.3  2          0.6               0.50       100      0.9988666  1.0000000  0.9769754
  0.3  2          0.6               0.50       150      0.9990366  1.0000000  0.9815795
  0.3  2          0.6               0.75        50      0.9979286  1.0000000  0.9732846
  0.3  2          0.6               0.75       100      0.9991038  1.0000000  0.9815753
  0.3  2          0.6               0.75       150      0.9991252  1.0000000  0.9815795
  0.3  2          0.6               1.00        50      0.9984210  1.0000000  0.9714582
  0.3  2          0.6               1.00       100      0.9995074  1.0000000  0.9871095
  0.3  2          0.6               1.00       150      0.9993332  1.0000000  0.9889528
  0.3  2          0.8               0.50        50      0.9984251  1.0000000  0.9696064
  0.3  2          0.8               0.50       100      0.9993289  1.0000000  0.9806536
  0.3  2          0.8               0.50       150      0.9994563  1.0000000  0.9797362
  0.3  2          0.8               0.75        50      0.9985275  1.0000000  0.9696022
  0.3  2          0.8               0.75       100      0.9994566  1.0000000  0.9788188
  0.3  2          0.8               0.75       150      0.9993291  1.0000000  0.9834228
  0.3  2          0.8               1.00        50      0.9980912  1.0000000  0.9668456
  0.3  2          0.8               1.00       100      0.9992018  1.0000000  0.9806578
  0.3  2          0.8               1.00       150      0.9995838  1.0000000  0.9852661
  0.3  3          0.6               0.50        50      0.9992652  1.0000000  0.9806578
  0.3  3          0.6               0.50       100      0.9995414  1.0000000  0.9834144
  0.3  3          0.6               0.50       150      0.9996094  1.0000000  0.9843403
  0.3  3          0.6               0.75        50      0.9999278  1.0000000  0.9834228
  0.3  3          0.6               0.75       100      1.0000000  1.0000000  0.9889443
  0.3  3          0.6               0.75       150      0.9999830  1.0000000  0.9861793
  0.3  3          0.6               1.00        50      0.9994818  1.0000000  0.9825012
  0.3  3          0.6               1.00       100      0.9996475  1.0000000  0.9907919
  0.3  3          0.6               1.00       150      0.9998981  1.0000000  0.9880311
  0.3  3          0.8               0.50        50      0.9995414  1.0000000  0.9778971
  0.3  3          0.8               0.50       100      0.9994737  1.0000000  0.9806621
  0.3  3          0.8               0.50       150      0.9996264  1.0000000  0.9806578
  0.3  3          0.8               0.75        50      0.9996602  1.0000000  0.9834186
  0.3  3          0.8               0.75       100      1.0000000  1.0000000  0.9871052
  0.3  3          0.8               0.75       150      1.0000000  1.0000000  0.9871010
  0.3  3          0.8               1.00        50      0.9997494  1.0000000  0.9815795
  0.3  3          0.8               1.00       100      0.9998515  1.0000000  0.9871010
  0.3  3          0.8               1.00       150      0.9999320  1.0000000  0.9871010
  0.4  1          0.6               0.50        50      0.9929548  0.9825054  0.9548725
  0.4  1          0.6               0.50       100      0.9972269  1.0000000  0.9668499
  0.4  1          0.6               0.50       150      0.9978796  1.0000000  0.9723714
  0.4  1          0.6               0.75        50      0.9936274  0.9732888  0.9539466
  0.4  1          0.6               0.75       100      0.9971767  1.0000000  0.9677631
  0.4  1          0.6               0.75       150      0.9978125  1.0000000  0.9742147
  0.4  1          0.6               1.00        50      0.9922419  0.9686847  0.9557857
  0.4  1          0.6               1.00       100      0.9969860  1.0000000  0.9723671
  0.4  1          0.6               1.00       150      0.9975706  1.0000000  0.9778929
  0.4  1          0.8               0.50        50      0.9921678  0.9880184  0.9511901
  0.4  1          0.8               0.50       100      0.9966334  1.0000000  0.9640891
  0.4  1          0.8               0.50       150      0.9973329  1.0000000  0.9705323
  0.4  1          0.8               0.75        50      0.9928913  0.9797404  0.9603940
  0.4  1          0.8               0.75       100      0.9972661  1.0000000  0.9677588
  0.4  1          0.8               0.75       150      0.9975913  1.0000000  0.9714497
  0.4  1          0.8               1.00        50      0.9930591  0.9687016  0.9520991
  0.4  1          0.8               1.00       100      0.9971125  1.0000000  0.9714413
  0.4  1          0.8               1.00       150      0.9977820  1.0000000  0.9760495
  0.4  2          0.6               0.50        50      0.9990628  1.0000000  0.9751279
  0.4  2          0.6               0.50       100      0.9994495  1.0000000  0.9824969
  0.4  2          0.6               0.50       150      0.9991811  1.0000000  0.9825054
  0.4  2          0.6               0.75        50      0.9978919  1.0000000  0.9732888
  0.4  2          0.6               0.75       100      0.9988920  1.0000000  0.9871052
  0.4  2          0.6               0.75       150      0.9989509  1.0000000  0.9852661
  0.4  2          0.6               1.00        50      0.9985860  1.0000000  0.9705281
  0.4  2          0.6               1.00       100      0.9987003  1.0000000  0.9843445
  0.4  2          0.6               1.00       150      0.9989976  1.0000000  0.9852661
  0.4  2          0.8               0.50        50      0.9994606  1.0000000  0.9778929
  0.4  2          0.8               0.50       100      0.9996432  1.0000000  0.9825054
  0.4  2          0.8               0.50       150      0.9993217  1.0000000  0.9815837
  0.4  2          0.8               0.75        50      0.9990700  1.0000000  0.9742189
  0.4  2          0.8               0.75       100      0.9994351  1.0000000  0.9806621
  0.4  2          0.8               0.75       150      0.9994224  1.0000000  0.9824969
  0.4  2          0.8               1.00        50      0.9986579  1.0000000  0.9778971
  0.4  2          0.8               1.00       100      0.9995159  1.0000000  0.9861920
  0.4  2          0.8               1.00       150      0.9998089  1.0000000  0.9852704
  0.4  3          0.6               0.50        50      0.9998386  1.0000000  0.9889485
  0.4  3          0.6               0.50       100      0.9997452  1.0000000  0.9871010
  0.4  3          0.6               0.50       150      0.9999193  1.0000000  0.9889443
  0.4  3          0.6               0.75        50      0.9994788  1.0000000  0.9834270
  0.4  3          0.6               0.75       100      0.9998514  1.0000000  0.9898702
  0.4  3          0.6               0.75       150      0.9992581  1.0000000  0.9889485
  0.4  3          0.6               1.00        50      0.9990284  1.0000000  0.9861836
  0.4  3          0.6               1.00       100      0.9990759  1.0000000  0.9861836
  0.4  3          0.6               1.00       150      0.9986099  1.0000000  0.9871052
  0.4  3          0.8               0.50        50      0.9998896  1.0000000  0.9806578
  0.4  3          0.8               0.50       100      0.9997537  1.0000000  0.9834144
  0.4  3          0.8               0.50       150      0.9999193  1.0000000  0.9834144
  0.4  3          0.8               0.75        50      0.9998131  1.0000000  0.9880269
  0.4  3          0.8               0.75       100      0.9999533  1.0000000  0.9889443
  0.4  3          0.8               0.75       150      0.9999320  1.0000000  0.9880227
  0.4  3          0.8               1.00        50      0.9990107  1.0000000  0.9806621
  0.4  3          0.8               1.00       100      0.9998812  1.0000000  0.9861836
  0.4  3          0.8               1.00       150      0.9995243  1.0000000  0.9843403

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 100, max_depth = 3, eta = 0.3, gamma = 0, colsample_bytree = 0.6, min_child_weight = 1 and subsample = 0.75.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     50.0      0.6
  positive      0.0     49.4
                            
 Accuracy (average) : 0.9945

[1] "TRAIN accuracy: 0.994475138121547"
[1] "TRAIN +precision: 1"
[1] "TRAIN -precision: 0.989071038251366"
[1] "TRAIN specifity: 1"
[1] "TRAIN sensitivity: 0.988950276243094"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       12        6
            positive        6      355
[1] "TEST accuracy: 0.968337730870712"
[1] "TEST +precision: 0.983379501385042"
[1] "TEST -precision: 0.666666666666667"
[1] "TEST specifity: 0.666666666666667"
[1] "TEST sensitivity: 0.983379501385042"

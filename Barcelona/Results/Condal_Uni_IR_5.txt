[1] "DATASET NAME: Condal_Uni_IR_5"
[1] "TRAIN INSTANCES: 2816"
[1] "TEST INSTANCES: 795"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 8.79846501350403"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

2816 samples
 652 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 2253, 2253, 2253, 2253, 2252 
Resampling results:

  ROC        Sens       Spec     
  0.9982761  0.9926606  0.9911952

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     19.2      0.7
  positive      0.1     79.9
                            
 Accuracy (average) : 0.9915

[1] "TRAIN accuracy: 0.991477272727273"
[1] "TRAIN +precision: 0.998226164079823"
[1] "TRAIN -precision: 0.964349376114082"
[1] "TRAIN specifity: 0.992660550458716"
[1] "TRAIN sensitivity: 0.991193306913254"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       22        4
            positive       17      752
[1] "TEST accuracy: 0.973584905660377"
[1] "TEST +precision: 0.977893368010403"
[1] "TEST -precision: 0.846153846153846"
[1] "TEST specifity: 0.564102564102564"
[1] "TEST sensitivity: 0.994708994708995"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 4.4645436167717"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

2816 samples
 652 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 2253, 2252, 2253, 2253, 2253 
Resampling results across tuning parameters:

  C      M  ROC        Sens       Spec     
  0.010  1  0.9086978  0.8220183  0.9779871
  0.010  2  0.9078770  0.8201835  0.9779871
  0.010  3  0.9158102  0.8256881  0.9793048
  0.255  1  0.9678269  0.9449541  0.9744629
  0.255  2  0.9690762  0.9412844  0.9753420
  0.255  3  0.9672854  0.9211009  0.9757806
  0.500  1  0.9767502  0.9633028  0.9740224
  0.500  2  0.9777352  0.9596330  0.9749015
  0.500  3  0.9751047  0.9321101  0.9753401

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.5 and M = 2.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     18.6      2.0
  positive      0.8     78.6
                            
 Accuracy (average) : 0.9719

[1] "TRAIN accuracy: 0.971946022727273"
[1] "TRAIN +precision: 0.990161001788909"
[1] "TRAIN -precision: 0.901724137931034"
[1] "TRAIN specifity: 0.959633027522936"
[1] "TRAIN sensitivity: 0.974900924702774"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       14       19
            positive       25      737
[1] "TEST accuracy: 0.944654088050314"
[1] "TEST +precision: 0.967191601049869"
[1] "TEST -precision: 0.424242424242424"
[1] "TEST specifity: 0.358974358974359"
[1] "TEST sensitivity: 0.974867724867725"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 9.0798405011495"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

2816 samples
 652 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 2253, 2252, 2253, 2253, 2253 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens       Spec     
  0.3  1          0.6               0.50        50      0.9748302  0.6146789  0.9933959
  0.3  1          0.6               0.50       100      0.9868416  0.7889908  0.9916329
  0.3  1          0.6               0.50       150      0.9896511  0.8678899  0.9907518
  0.3  1          0.6               0.75        50      0.9750357  0.6128440  0.9942780
  0.3  1          0.6               0.75       100      0.9856402  0.7889908  0.9933940
  0.3  1          0.6               0.75       150      0.9898759  0.8642202  0.9929535
  0.3  1          0.6               1.00        50      0.9741914  0.5743119  0.9951561
  0.3  1          0.6               1.00       100      0.9864536  0.7633028  0.9942751
  0.3  1          0.6               1.00       150      0.9908650  0.8587156  0.9942751
  0.3  1          0.8               0.50        50      0.9742092  0.5908257  0.9916338
  0.3  1          0.8               0.50       100      0.9862557  0.7889908  0.9911923
  0.3  1          0.8               0.50       150      0.9907568  0.8697248  0.9911914
  0.3  1          0.8               0.75        50      0.9740868  0.6036697  0.9942760
  0.3  1          0.8               0.75       100      0.9870961  0.8238532  0.9925149
  0.3  1          0.8               0.75       150      0.9914687  0.8788991  0.9916329
  0.3  1          0.8               1.00        50      0.9751848  0.5743119  0.9947156
  0.3  1          0.8               1.00       100      0.9869223  0.7614679  0.9938355
  0.3  1          0.8               1.00       150      0.9905662  0.8642202  0.9933940
  0.3  2          0.6               0.50        50      0.9861879  0.8220183  0.9898727
  0.3  2          0.6               0.50       100      0.9928320  0.9155963  0.9894302
  0.3  2          0.6               0.50       150      0.9936763  0.9357798  0.9885492
  0.3  2          0.6               0.75        50      0.9886810  0.8440367  0.9920734
  0.3  2          0.6               0.75       100      0.9933388  0.9119266  0.9911923
  0.3  2          0.6               0.75       150      0.9944281  0.9467890  0.9916329
  0.3  2          0.6               1.00        50      0.9881666  0.8422018  0.9938345
  0.3  2          0.6               1.00       100      0.9929520  0.9155963  0.9925129
  0.3  2          0.6               1.00       150      0.9949979  0.9376147  0.9925139
  0.3  2          0.8               0.50        50      0.9864418  0.8201835  0.9925129
  0.3  2          0.8               0.50       100      0.9936410  0.9229358  0.9925129
  0.3  2          0.8               0.50       150      0.9950026  0.9412844  0.9907508
  0.3  2          0.8               0.75        50      0.9880637  0.8550459  0.9907528
  0.3  2          0.8               0.75       100      0.9934463  0.9137615  0.9907518
  0.3  2          0.8               0.75       150      0.9952000  0.9559633  0.9911923
  0.3  2          0.8               1.00        50      0.9870355  0.8385321  0.9925149
  0.3  2          0.8               1.00       100      0.9937781  0.9119266  0.9925139
  0.3  2          0.8               1.00       150      0.9952809  0.9412844  0.9920744
  0.3  3          0.6               0.50        50      0.9922227  0.8935780  0.9898717
  0.3  3          0.6               0.50       100      0.9948329  0.9467890  0.9911923
  0.3  3          0.6               0.50       150      0.9951065  0.9614679  0.9881096
  0.3  3          0.6               0.75        50      0.9923110  0.8935780  0.9920724
  0.3  3          0.6               0.75       100      0.9952761  0.9669725  0.9907528
  0.3  3          0.6               0.75       150      0.9955224  0.9706422  0.9903122
  0.3  3          0.6               1.00        50      0.9921685  0.9082569  0.9929544
  0.3  3          0.6               1.00       100      0.9957532  0.9614679  0.9916329
  0.3  3          0.6               1.00       150      0.9961573  0.9706422  0.9916329
  0.3  3          0.8               0.50        50      0.9929696  0.9045872  0.9920744
  0.3  3          0.8               0.50       100      0.9948399  0.9486239  0.9903122
  0.3  3          0.8               0.50       150      0.9952437  0.9724771  0.9898717
  0.3  3          0.8               0.75        50      0.9925939  0.9174312  0.9907537
  0.3  3          0.8               0.75       100      0.9951148  0.9688073  0.9894312
  0.3  3          0.8               0.75       150      0.9955471  0.9779817  0.9894312
  0.3  3          0.8               1.00        50      0.9927394  0.9100917  0.9920734
  0.3  3          0.8               1.00       100      0.9957650  0.9633028  0.9925139
  0.3  3          0.8               1.00       150      0.9962983  0.9779817  0.9929544
  0.4  1          0.6               0.50        50      0.9815836  0.6899083  0.9920724
  0.4  1          0.6               0.50       100      0.9892889  0.8642202  0.9894312
  0.4  1          0.6               0.50       150      0.9923770  0.9064220  0.9894292
  0.4  1          0.6               0.75        50      0.9836657  0.6954128  0.9947175
  0.4  1          0.6               0.75       100      0.9897983  0.8403670  0.9938355
  0.4  1          0.6               0.75       150      0.9928775  0.9027523  0.9916319
  0.4  1          0.6               1.00        50      0.9805654  0.6807339  0.9955967
  0.4  1          0.6               1.00       100      0.9897102  0.8532110  0.9947156
  0.4  1          0.6               1.00       150      0.9928335  0.8935780  0.9938345
  0.4  1          0.8               0.50        50      0.9796900  0.6752294  0.9929544
  0.4  1          0.8               0.50       100      0.9873643  0.8587156  0.9898717
  0.4  1          0.8               0.50       150      0.9909424  0.9100917  0.9916329
  0.4  1          0.8               0.75        50      0.9806607  0.6844037  0.9907528
  0.4  1          0.8               0.75       100      0.9889913  0.8458716  0.9929544
  0.4  1          0.8               0.75       150      0.9922839  0.9082569  0.9911923
  0.4  1          0.8               1.00        50      0.9798381  0.6788991  0.9938355
  0.4  1          0.8               1.00       100      0.9894231  0.8422018  0.9942760
  0.4  1          0.8               1.00       150      0.9933951  0.8990826  0.9933940
  0.4  2          0.6               0.50        50      0.9887408  0.8862385  0.9894322
  0.4  2          0.6               0.50       100      0.9935841  0.9394495  0.9903113
  0.4  2          0.6               0.50       150      0.9945214  0.9651376  0.9881096
  0.4  2          0.6               0.75        50      0.9899160  0.8844037  0.9907508
  0.4  2          0.6               0.75       100      0.9941983  0.9376147  0.9894302
  0.4  2          0.6               0.75       150      0.9948724  0.9688073  0.9885492
  0.4  2          0.6               1.00        50      0.9903601  0.8715596  0.9911923
  0.4  2          0.6               1.00       100      0.9953253  0.9302752  0.9920744
  0.4  2          0.6               1.00       150      0.9953453  0.9669725  0.9925149
  0.4  2          0.8               0.50        50      0.9894259  0.8935780  0.9898698
  0.4  2          0.8               0.50       100      0.9931230  0.9486239  0.9885492
  0.4  2          0.8               0.50       150      0.9946302  0.9688073  0.9881106
  0.4  2          0.8               0.75        50      0.9917621  0.8825688  0.9925139
  0.4  2          0.8               0.75       100      0.9950543  0.9541284  0.9903122
  0.4  2          0.8               0.75       150      0.9953734  0.9669725  0.9889907
  0.4  2          0.8               1.00        50      0.9905660  0.8899083  0.9925139
  0.4  2          0.8               1.00       100      0.9950465  0.9394495  0.9907508
  0.4  2          0.8               1.00       150      0.9958182  0.9651376  0.9907518
  0.4  3          0.6               0.50        50      0.9954221  0.9155963  0.9898717
  0.4  3          0.6               0.50       100      0.9958305  0.9669725  0.9894331
  0.4  3          0.6               0.50       150      0.9954019  0.9798165  0.9889926
  0.4  3          0.6               0.75        50      0.9945043  0.9412844  0.9933940
  0.4  3          0.6               0.75       100      0.9954011  0.9651376  0.9929544
  0.4  3          0.6               0.75       150      0.9954255  0.9743119  0.9907518
  0.4  3          0.6               1.00        50      0.9939060  0.9467890  0.9929525
  0.4  3          0.6               1.00       100      0.9958295  0.9743119  0.9933930
  0.4  3          0.6               1.00       150      0.9964235  0.9779817  0.9929525
  0.4  3          0.8               0.50        50      0.9938825  0.9431193  0.9916319
  0.4  3          0.8               0.50       100      0.9956519  0.9651376  0.9916348
  0.4  3          0.8               0.50       150      0.9958947  0.9761468  0.9898737
  0.4  3          0.8               0.75        50      0.9934374  0.9339450  0.9907499
  0.4  3          0.8               0.75       100      0.9952924  0.9816514  0.9903113
  0.4  3          0.8               0.75       150      0.9953649  0.9853211  0.9889916
  0.4  3          0.8               1.00        50      0.9947592  0.9357798  0.9911914
  0.4  3          0.8               1.00       100      0.9964117  0.9743119  0.9911923
  0.4  3          0.8               1.00       150      0.9963754  0.9853211  0.9903113

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 150, max_depth = 3, eta = 0.4, gamma = 0, colsample_bytree = 0.6, min_child_weight = 1 and subsample = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     18.9      0.6
  positive      0.4     80.1
                            
 Accuracy (average) : 0.9901

[1] "TRAIN accuracy: 0.990056818181818"
[1] "TRAIN +precision: 0.994706660785179"
[1] "TRAIN -precision: 0.970856102003643"
[1] "TRAIN specifity: 0.977981651376147"
[1] "TRAIN sensitivity: 0.992954645530603"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       18        5
            positive       21      751
[1] "TEST accuracy: 0.967295597484277"
[1] "TEST +precision: 0.97279792746114"
[1] "TEST -precision: 0.782608695652174"
[1] "TEST specifity: 0.461538461538462"
[1] "TEST sensitivity: 0.993386243386243"

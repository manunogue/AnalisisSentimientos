[1] "DATASET NAME: HRB_Uni_IR_0"
[1] "TRAIN INSTANCES: 995"
[1] "TEST INSTANCES: 332"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 5.01028203964233"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

995 samples
652 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 796, 796, 796, 795, 797 
Resampling results:

  ROC        Sens       Spec     
  0.9305942  0.5983908  0.9728089

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative      8.9      2.3
  positive      6.0     82.7
                            
 Accuracy (average) : 0.9166

[1] "TRAIN accuracy: 0.916582914572864"
[1] "TRAIN +precision: 0.932049830124575"
[1] "TRAIN -precision: 0.794642857142857"
[1] "TRAIN specifity: 0.597315436241611"
[1] "TRAIN sensitivity: 0.972813238770686"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       31        5
            positive       14      282
[1] "TEST accuracy: 0.942771084337349"
[1] "TEST +precision: 0.952702702702703"
[1] "TEST -precision: 0.861111111111111"
[1] "TEST specifity: 0.688888888888889"
[1] "TEST sensitivity: 0.982578397212544"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 1.27422421773275"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

995 samples
652 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 796, 796, 796, 796, 796 
Resampling results across tuning parameters:

  C      M  ROC        Sens       Spec     
  0.010  1  0.6556489  0.3427586  0.9822555
  0.010  2  0.6585951  0.3360920  0.9834459
  0.010  3  0.6351014  0.2958621  0.9846432
  0.255  1  0.7107451  0.4567816  0.9479777
  0.255  2  0.7401443  0.4565517  0.9503446
  0.255  3  0.7517047  0.4298851  0.9491820
  0.500  1  0.7111717  0.4634483  0.9467943
  0.500  2  0.7212035  0.4565517  0.9491681
  0.500  3  0.7368003  0.4503448  0.9444553

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.255 and M = 3.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative      6.4      4.3
  positive      8.5     80.7
                            
 Accuracy (average) : 0.8714

[1] "TRAIN accuracy: 0.871356783919598"
[1] "TRAIN +precision: 0.904279279279279"
[1] "TRAIN -precision: 0.598130841121495"
[1] "TRAIN specifity: 0.429530201342282"
[1] "TRAIN sensitivity: 0.949172576832151"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       25       12
            positive       20      275
[1] "TEST accuracy: 0.903614457831325"
[1] "TEST +precision: 0.932203389830508"
[1] "TEST -precision: 0.675675675675676"
[1] "TEST specifity: 0.555555555555556"
[1] "TEST sensitivity: 0.958188153310105"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 3.41200491984685"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

995 samples
652 predictors
  2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 796, 796, 796, 796, 796 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens       Spec     
  0.3  1          0.6               0.50        50      0.9274817  0.4434483  0.9893630
  0.3  1          0.6               0.50       100      0.9447670  0.5303448  0.9846502
  0.3  1          0.6               0.50       150      0.9438284  0.5501149  0.9822833
  0.3  1          0.6               0.75        50      0.9395774  0.4298851  0.9882005
  0.3  1          0.6               0.75       100      0.9463925  0.5236782  0.9893770
  0.3  1          0.6               0.75       150      0.9527108  0.5908046  0.9893839
  0.3  1          0.6               1.00        50      0.9336457  0.3896552  0.9917369
  0.3  1          0.6               1.00       100      0.9515518  0.4834483  0.9893770
  0.3  1          0.6               1.00       150      0.9533986  0.5443678  0.9929133
  0.3  1          0.8               0.50        50      0.9251594  0.3685057  0.9858267
  0.3  1          0.8               0.50       100      0.9401877  0.5434483  0.9834598
  0.3  1          0.8               0.50       150      0.9370469  0.5639080  0.9834668
  0.3  1          0.8               0.75        50      0.9316739  0.4229885  0.9881935
  0.3  1          0.8               0.75       100      0.9476061  0.5572414  0.9870240
  0.3  1          0.8               0.75       150      0.9461609  0.5710345  0.9834737
  0.3  1          0.8               1.00        50      0.9303142  0.3896552  0.9917369
  0.3  1          0.8               1.00       100      0.9493055  0.4703448  0.9893770
  0.3  1          0.8               1.00       150      0.9535914  0.5372414  0.9929133
  0.3  2          0.6               0.50        50      0.9427103  0.5041379  0.9799443
  0.3  2          0.6               0.50       100      0.9471272  0.5770115  0.9787539
  0.3  2          0.6               0.50       150      0.9431250  0.5972414  0.9716533
  0.3  2          0.6               0.75        50      0.9504966  0.4841379  0.9881935
  0.3  2          0.6               0.75       100      0.9483249  0.5839080  0.9834807
  0.3  2          0.6               0.75       150      0.9470175  0.6305747  0.9811138
  0.3  2          0.6               1.00        50      0.9516591  0.5108046  0.9858406
  0.3  2          0.6               1.00       100      0.9536084  0.5712644  0.9823042
  0.3  2          0.6               1.00       150      0.9533491  0.6045977  0.9811208
  0.3  2          0.8               0.50        50      0.9340820  0.4974713  0.9775496
  0.3  2          0.8               0.50       100      0.9371854  0.5772414  0.9799165
  0.3  2          0.8               0.50       150      0.9301902  0.5903448  0.9775496
  0.3  2          0.8               0.75        50      0.9410269  0.5374713  0.9775635
  0.3  2          0.8               0.75       100      0.9483428  0.6110345  0.9811069
  0.3  2          0.8               0.75       150      0.9454959  0.6379310  0.9740063
  0.3  2          0.8               1.00        50      0.9490625  0.4908046  0.9858406
  0.3  2          0.8               1.00       100      0.9549299  0.5643678  0.9858475
  0.3  2          0.8               1.00       150      0.9516416  0.5977011  0.9799304
  0.3  3          0.6               0.50        50      0.9385350  0.5641379  0.9787470
  0.3  3          0.6               0.50       100      0.9367845  0.5905747  0.9763731
  0.3  3          0.6               0.50       150      0.9305609  0.6041379  0.9692865
  0.3  3          0.6               0.75        50      0.9486279  0.5639080  0.9799234
  0.3  3          0.6               0.75       100      0.9443295  0.5839080  0.9822903
  0.3  3          0.6               0.75       150      0.9422592  0.6177011  0.9834598
  0.3  3          0.6               1.00        50      0.9548156  0.5439080  0.9870240
  0.3  3          0.6               1.00       100      0.9526222  0.5910345  0.9811069
  0.3  3          0.6               1.00       150      0.9483767  0.6045977  0.9787470
  0.3  3          0.8               0.50        50      0.9312979  0.5639080  0.9751827
  0.3  3          0.8               0.50       100      0.9300219  0.5772414  0.9704560
  0.3  3          0.8               0.50       150      0.9265034  0.5972414  0.9716464
  0.3  3          0.8               0.75        50      0.9403103  0.5505747  0.9822903
  0.3  3          0.8               0.75       100      0.9394245  0.6114943  0.9787470
  0.3  3          0.8               0.75       150      0.9366237  0.5905747  0.9775635
  0.3  3          0.8               1.00        50      0.9543614  0.5103448  0.9893839
  0.3  3          0.8               1.00       100      0.9524261  0.6041379  0.9811069
  0.3  3          0.8               1.00       150      0.9489458  0.6043678  0.9799234
  0.4  1          0.6               0.50        50      0.9388247  0.4905747  0.9881796
  0.4  1          0.6               0.50       100      0.9419765  0.5367816  0.9775496
  0.4  1          0.6               0.50       150      0.9415648  0.5905747  0.9810929
  0.4  1          0.6               0.75        50      0.9402476  0.4503448  0.9870031
  0.4  1          0.6               0.75       100      0.9485771  0.5643678  0.9846641
  0.4  1          0.6               0.75       150      0.9485753  0.6179310  0.9834807
  0.4  1          0.6               1.00        50      0.9391295  0.4167816  0.9893700
  0.4  1          0.6               1.00       100      0.9551130  0.5105747  0.9917299
  0.4  1          0.6               1.00       150      0.9521737  0.5779310  0.9893700
  0.4  1          0.8               0.50        50      0.9360255  0.4298851  0.9787261
  0.4  1          0.8               0.50       100      0.9396899  0.5639080  0.9822694
  0.4  1          0.8               0.50       150      0.9388665  0.5772414  0.9775635
  0.4  1          0.8               0.75        50      0.9393353  0.4967816  0.9846572
  0.4  1          0.8               0.75       100      0.9557895  0.5636782  0.9846502
  0.4  1          0.8               0.75       150      0.9525615  0.6105747  0.9823042
  0.4  1          0.8               1.00        50      0.9384569  0.4232184  0.9893700
  0.4  1          0.8               1.00       100      0.9521975  0.5370115  0.9905534
  0.4  1          0.8               1.00       150      0.9523867  0.5843678  0.9870101
  0.4  2          0.6               0.50        50      0.9344510  0.5641379  0.9751827
  0.4  2          0.6               0.50       100      0.9339384  0.5836782  0.9775496
  0.4  2          0.6               0.50       150      0.9301528  0.6036782  0.9728159
  0.4  2          0.6               0.75        50      0.9443575  0.5779310  0.9799234
  0.4  2          0.6               0.75       100      0.9465179  0.6241379  0.9787330
  0.4  2          0.6               0.75       150      0.9420110  0.6036782  0.9775566
  0.4  2          0.6               1.00        50      0.9510777  0.5177011  0.9882074
  0.4  2          0.6               1.00       100      0.9497630  0.5643678  0.9787470
  0.4  2          0.6               1.00       150      0.9483473  0.6245977  0.9787400
  0.4  2          0.8               0.50        50      0.9359419  0.5241379  0.9787261
  0.4  2          0.8               0.50       100      0.9349414  0.6043678  0.9728089
  0.4  2          0.8               0.50       150      0.9352175  0.6305747  0.9716255
  0.4  2          0.8               0.75        50      0.9470422  0.5236782  0.9811069
  0.4  2          0.8               0.75       100      0.9501283  0.6039080  0.9834737
  0.4  2          0.8               0.75       150      0.9447556  0.5972414  0.9787470
  0.4  2          0.8               1.00        50      0.9551480  0.5443678  0.9858336
  0.4  2          0.8               1.00       100      0.9538832  0.6114943  0.9811138
  0.4  2          0.8               1.00       150      0.9512956  0.6110345  0.9787470
  0.4  3          0.6               0.50        50      0.9387894  0.5708046  0.9787330
  0.4  3          0.6               0.50       100      0.9316758  0.6241379  0.9751827
  0.4  3          0.6               0.50       150      0.9296427  0.6310345  0.9751758
  0.4  3          0.6               0.75        50      0.9475530  0.5232184  0.9787470
  0.4  3          0.6               0.75       100      0.9405427  0.5903448  0.9740202
  0.4  3          0.6               0.75       150      0.9371032  0.5967816  0.9751967
  0.4  3          0.6               1.00        50      0.9510620  0.5443678  0.9893700
  0.4  3          0.6               1.00       100      0.9469958  0.5777011  0.9810999
  0.4  3          0.6               1.00       150      0.9441410  0.6379310  0.9834598
  0.4  3          0.8               0.50        50      0.9284957  0.5505747  0.9775496
  0.4  3          0.8               0.50       100      0.9243773  0.5772414  0.9716394
  0.4  3          0.8               0.50       150      0.9252745  0.6108046  0.9716255
  0.4  3          0.8               0.75        50      0.9412497  0.5770115  0.9787470
  0.4  3          0.8               0.75       100      0.9368995  0.6308046  0.9775566
  0.4  3          0.8               0.75       150      0.9322234  0.6101149  0.9680961
  0.4  3          0.8               1.00        50      0.9532940  0.5710345  0.9822903
  0.4  3          0.8               1.00       100      0.9468225  0.6045977  0.9740063
  0.4  3          0.8               1.00       150      0.9430470  0.6179310  0.9728228

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 100, max_depth = 1, eta = 0.4, gamma = 0, colsample_bytree = 0.8, min_child_weight = 1 and subsample = 0.75.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative      8.4      1.3
  positive      6.5     83.7
                            
 Accuracy (average) : 0.9216

[1] "TRAIN accuracy: 0.921608040201005"
[1] "TRAIN +precision: 0.927616926503341"
[1] "TRAIN -precision: 0.865979381443299"
[1] "TRAIN specifity: 0.563758389261745"
[1] "TRAIN sensitivity: 0.984633569739953"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       28        2
            positive       17      285
[1] "TEST accuracy: 0.942771084337349"
[1] "TEST +precision: 0.943708609271523"
[1] "TEST -precision: 0.933333333333333"
[1] "TEST specifity: 0.622222222222222"
[1] "TEST sensitivity: 0.993031358885017"

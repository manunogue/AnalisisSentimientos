[1] "DATASET NAME: Tapeo_Uni_IR_5"
[1] "TRAIN INSTANCES: 1181"
[1] "TEST INSTANCES: 333"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 3.33036708831787"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

1181 samples
 672 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 945, 945, 945, 945, 944 
Resampling results:

  ROC  Sens  Spec     
  1    1     0.9979167

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     18.6      0.2
  positive      0.0     81.2
                            
 Accuracy (average) : 0.9983

[1] "TRAIN accuracy: 0.998306519898391"
[1] "TRAIN +precision: 1"
[1] "TRAIN -precision: 0.990990990990991"
[1] "TRAIN specifity: 1"
[1] "TRAIN sensitivity: 0.997918834547347"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        4        3
            positive        7      319
[1] "TEST accuracy: 0.96996996996997"
[1] "TEST +precision: 0.978527607361963"
[1] "TEST -precision: 0.571428571428571"
[1] "TEST specifity: 0.363636363636364"
[1] "TEST sensitivity: 0.990683229813665"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 1.68949703375498"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

1181 samples
 672 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 945, 945, 945, 944, 945 
Resampling results across tuning parameters:

  C      M  ROC        Sens       Spec     
  0.010  1  0.9377409  0.8863636  0.9625594
  0.010  2  0.9344177  0.8863636  0.9583981
  0.010  3  0.9360683  0.8863636  0.9552731
  0.255  1  0.9577553  0.9363636  0.9615177
  0.255  2  0.9568993  0.9363636  0.9573564
  0.255  3  0.9580021  0.9272727  0.9604652
  0.500  1  0.9698174  0.9727273  0.9604760
  0.500  2  0.9690442  0.9727273  0.9563148
  0.500  3  0.9580731  0.9272727  0.9604652

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.5 and M = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     18.1      3.2
  positive      0.5     78.2
                            
 Accuracy (average) : 0.9627

[1] "TRAIN accuracy: 0.962743437764606"
[1] "TRAIN +precision: 0.993541442411195"
[1] "TRAIN -precision: 0.849206349206349"
[1] "TRAIN specifity: 0.972727272727273"
[1] "TRAIN sensitivity: 0.960457856399584"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        3       12
            positive        8      310
[1] "TEST accuracy: 0.93993993993994"
[1] "TEST +precision: 0.974842767295597"
[1] "TEST -precision: 0.2"
[1] "TEST specifity: 0.272727272727273"
[1] "TEST sensitivity: 0.962732919254658"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 4.08300224939982"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

1181 samples
 672 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 944, 945, 945, 945, 945 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens       Spec     
  0.3  1          0.6               0.50        50      0.9845126  0.6590909  0.9916721
  0.3  1          0.6               0.50       100      0.9915537  0.8909091  0.9906304
  0.3  1          0.6               0.50       150      0.9943466  0.9681818  0.9937608
  0.3  1          0.6               0.75        50      0.9838642  0.6681818  0.9948025
  0.3  1          0.6               0.75       100      0.9928913  0.8909091  0.9927191
  0.3  1          0.6               0.75       150      0.9941812  0.9772727  0.9937554
  0.3  1          0.6               1.00        50      0.9834110  0.6136364  0.9947971
  0.3  1          0.6               1.00       100      0.9922991  0.8818182  0.9937554
  0.3  1          0.6               1.00       150      0.9943704  0.9590909  0.9927137
  0.3  1          0.8               0.50        50      0.9834493  0.6954545  0.9906358
  0.3  1          0.8               0.50       100      0.9925949  0.8863636  0.9927137
  0.3  1          0.8               0.50       150      0.9947730  0.9772727  0.9916721
  0.3  1          0.8               0.75        50      0.9838747  0.6590909  0.9937554
  0.3  1          0.8               0.75       100      0.9915534  0.9272727  0.9906358
  0.3  1          0.8               0.75       150      0.9930922  0.9772727  0.9948025
  0.3  1          0.8               1.00        50      0.9842864  0.6454545  0.9947971
  0.3  1          0.8               1.00       100      0.9927370  0.8681818  0.9927137
  0.3  1          0.8               1.00       150      0.9940154  0.9636364  0.9937554
  0.3  2          0.6               0.50        50      0.9926858  0.9590909  0.9927137
  0.3  2          0.6               0.50       100      0.9970173  0.9909091  0.9937554
  0.3  2          0.6               0.50       150      0.9984612  0.9909091  0.9906358
  0.3  2          0.6               0.75        50      0.9939176  0.9590909  0.9895941
  0.3  2          0.6               0.75       100      0.9973270  0.9909091  0.9906304
  0.3  2          0.6               0.75       150      0.9988403  0.9909091  0.9916721
  0.3  2          0.6               1.00        50      0.9939432  0.9727273  0.9937554
  0.3  2          0.6               1.00       100      0.9964755  0.9909091  0.9968804
  0.3  2          0.6               1.00       150      0.9982486  0.9909091  0.9968804
  0.3  2          0.8               0.50        50      0.9940617  0.9590909  0.9947971
  0.3  2          0.8               0.50       100      0.9969003  0.9909091  0.9958387
  0.3  2          0.8               0.50       150      0.9981771  0.9909091  0.9927191
  0.3  2          0.8               0.75        50      0.9943425  0.9500000  0.9916775
  0.3  2          0.8               0.75       100      0.9963585  0.9909091  0.9937554
  0.3  2          0.8               0.75       150      0.9979430  0.9909091  0.9927137
  0.3  2          0.8               1.00        50      0.9938497  0.9409091  0.9968804
  0.3  2          0.8               1.00       100      0.9969270  0.9909091  0.9979221
  0.3  2          0.8               1.00       150      0.9976613  0.9909091  0.9968804
  0.3  3          0.6               0.50        50      0.9960045  0.9681818  0.9927191
  0.3  3          0.6               0.50       100      0.9977314  0.9909091  0.9927137
  0.3  3          0.6               0.50       150      0.9987932  0.9909091  0.9896049
  0.3  3          0.6               0.75        50      0.9970179  0.9909091  0.9958387
  0.3  3          0.6               0.75       100      0.9983191  0.9909091  0.9947971
  0.3  3          0.6               0.75       150      0.9991477  0.9909091  0.9916829
  0.3  3          0.6               1.00        50      0.9982955  0.9909091  0.9968804
  0.3  3          0.6               1.00       100      0.9994792  0.9909091  0.9968804
  0.3  3          0.6               1.00       150      0.9998580  1.0000000  0.9947971
  0.3  3          0.8               0.50        50      0.9966384  0.9909091  0.9927137
  0.3  3          0.8               0.50       100      0.9989110  0.9909091  0.9916775
  0.3  3          0.8               0.50       150      0.9992898  0.9909091  0.9927137
  0.3  3          0.8               0.75        50      0.9962879  0.9909091  0.9947971
  0.3  3          0.8               0.75       100      0.9985134  0.9909091  0.9947971
  0.3  3          0.8               0.75       150      0.9993160  0.9909091  0.9937554
  0.3  3          0.8               1.00        50      0.9980824  0.9909091  0.9979221
  0.3  3          0.8               1.00       100      0.9996454  0.9909091  0.9958387
  0.3  3          0.8               1.00       150      0.9999290  0.9954545  0.9947971
  0.4  1          0.6               0.50        50      0.9866293  0.8318182  0.9916721
  0.4  1          0.6               0.50       100      0.9931631  0.9636364  0.9916775
  0.4  1          0.6               0.50       150      0.9944178  0.9909091  0.9927191
  0.4  1          0.6               0.75        50      0.9889266  0.7681818  0.9906304
  0.4  1          0.6               0.75       100      0.9932105  0.9681818  0.9927137
  0.4  1          0.6               0.75       150      0.9951755  0.9909091  0.9927137
  0.4  1          0.6               1.00        50      0.9900753  0.8090909  0.9906358
  0.4  1          0.6               1.00       100      0.9942759  0.9454545  0.9937554
  0.4  1          0.6               1.00       150      0.9952228  0.9909091  0.9947971
  0.4  1          0.8               0.50        50      0.9868816  0.7409091  0.9916775
  0.4  1          0.8               0.50       100      0.9928796  0.9590909  0.9895995
  0.4  1          0.8               0.50       150      0.9941099  0.9772727  0.9864691
  0.4  1          0.8               0.75        50      0.9880625  0.8045455  0.9906304
  0.4  1          0.8               0.75       100      0.9954363  0.9590909  0.9958387
  0.4  1          0.8               0.75       150      0.9961935  0.9909091  0.9947971
  0.4  1          0.8               1.00        50      0.9904295  0.7954545  0.9937554
  0.4  1          0.8               1.00       100      0.9939444  0.9500000  0.9947971
  0.4  1          0.8               1.00       150      0.9956963  0.9909091  0.9968804
  0.4  2          0.6               0.50        50      0.9953649  0.9681818  0.9958387
  0.4  2          0.6               0.50       100      0.9970456  0.9909091  0.9937554
  0.4  2          0.6               0.50       150      0.9975185  0.9909091  0.9916721
  0.4  2          0.6               0.75        50      0.9964490  0.9818182  0.9937662
  0.4  2          0.6               0.75       100      0.9983428  0.9909091  0.9937608
  0.4  2          0.6               0.75       150      0.9988636  0.9909091  0.9916829
  0.4  2          0.6               1.00        50      0.9945120  0.9909091  0.9968804
  0.4  2          0.6               1.00       100      0.9977078  0.9909091  0.9979221
  0.4  2          0.6               1.00       150      0.9983221  0.9909091  0.9958387
  0.4  2          0.8               0.50        50      0.9953146  0.9909091  0.9916721
  0.4  2          0.8               0.50       100      0.9979403  0.9909091  0.9927137
  0.4  2          0.8               0.50       150      0.9982955  0.9909091  0.9927137
  0.4  2          0.8               0.75        50      0.9959313  0.9909091  0.9947971
  0.4  2          0.8               0.75       100      0.9979180  0.9909091  0.9968804
  0.4  2          0.8               0.75       150      0.9987453  0.9909091  0.9906304
  0.4  2          0.8               1.00        50      0.9947963  0.9909091  0.9947971
  0.4  2          0.8               1.00       100      0.9978028  0.9909091  0.9958387
  0.4  2          0.8               1.00       150      0.9983932  0.9909091  0.9937554
  0.4  3          0.6               0.50        50      0.9963078  0.9818182  0.9895887
  0.4  3          0.6               0.50       100      0.9985559  0.9909091  0.9916775
  0.4  3          0.6               0.50       150      0.9986979  0.9909091  0.9916721
  0.4  3          0.6               0.75        50      0.9974238  0.9909091  0.9916721
  0.4  3          0.6               0.75       100      0.9995739  0.9909091  0.9906358
  0.4  3          0.6               0.75       150      0.9999053  1.0000000  0.9864745
  0.4  3          0.6               1.00        50      0.9985322  0.9909091  0.9958387
  0.4  3          0.6               1.00       100      0.9991477  0.9954545  0.9968804
  0.4  3          0.6               1.00       150      0.9995028  1.0000000  0.9947971
  0.4  3          0.8               0.50        50      0.9985085  0.9909091  0.9947971
  0.4  3          0.8               0.50       100      0.9994555  0.9909091  0.9916775
  0.4  3          0.8               0.50       150      0.9997633  0.9909091  0.9937608
  0.4  3          0.8               0.75        50      0.9982737  0.9909091  0.9927137
  0.4  3          0.8               0.75       100      0.9998343  0.9909091  0.9927191
  0.4  3          0.8               0.75       150      0.9999290  1.0000000  0.9947971
  0.4  3          0.8               1.00        50      0.9985132  0.9909091  0.9937554
  0.4  3          0.8               1.00       100      0.9994579  1.0000000  0.9937554
  0.4  3          0.8               1.00       150      0.9999763  1.0000000  0.9916775

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 150, max_depth = 3, eta = 0.4, gamma = 0, colsample_bytree = 0.8, min_child_weight = 1 and subsample = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     18.6      0.7
  positive      0.0     80.7
                            
 Accuracy (average) : 0.9932

[1] "TRAIN accuracy: 0.993226079593565"
[1] "TRAIN +precision: 1"
[1] "TRAIN -precision: 0.964912280701754"
[1] "TRAIN specifity: 1"
[1] "TRAIN sensitivity: 0.991675338189386"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        4        5
            positive        7      317
[1] "TEST accuracy: 0.963963963963964"
[1] "TEST +precision: 0.978395061728395"
[1] "TEST -precision: 0.444444444444444"
[1] "TEST specifity: 0.363636363636364"
[1] "TEST sensitivity: 0.984472049689441"

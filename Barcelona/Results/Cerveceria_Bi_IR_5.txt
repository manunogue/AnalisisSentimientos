[1] "DATASET NAME: Cerveceria_Bi_IR_5"
[1] "TRAIN INSTANCES: 3426"
[1] "TEST INSTANCES: 968"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 26.3993849754333"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

3426 samples
 690 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 2741, 2741, 2741, 2740, 2741 
Resampling results:

  ROC      Sens       Spec     
  0.96957  0.9294468  0.9615942

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     18.1      3.1
  positive      1.4     77.5
                            
 Accuracy (average) : 0.9553

[1] "TRAIN accuracy: 0.955341506129597"
[1] "TRAIN +precision: 0.982599037393558"
[1] "TRAIN -precision: 0.853793103448276"
[1] "TRAIN specifity: 0.929429429429429"
[1] "TRAIN sensitivity: 0.961594202898551"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       23       39
            positive       33      873
[1] "TEST accuracy: 0.925619834710744"
[1] "TEST +precision: 0.963576158940397"
[1] "TEST -precision: 0.370967741935484"
[1] "TEST specifity: 0.410714285714286"
[1] "TEST sensitivity: 0.957236842105263"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 6.2744643330574"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

3426 samples
 690 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 2740, 2741, 2741, 2741, 2741 
Resampling results across tuning parameters:

  C      M  ROC        Sens       Spec     
  0.010  1  0.6618946  0.2778252  0.9829710
  0.010  2  0.6507028  0.2642913  0.9836957
  0.010  3  0.6483313  0.2582202  0.9865942
  0.255  1  0.8961657  0.5060038  0.9815217
  0.255  2  0.8860982  0.4744473  0.9818841
  0.255  3  0.8755636  0.4504545  0.9847826
  0.500  1  0.9145033  0.5180339  0.9822464
  0.500  2  0.9067498  0.4879811  0.9891304
  0.500  3  0.8947609  0.4579508  0.9902174

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.5 and M = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     10.1      1.4
  positive      9.4     79.1
                           
 Accuracy (average) : 0.892

[1] "TRAIN accuracy: 0.892002335084647"
[1] "TRAIN +precision: 0.894129287598945"
[1] "TRAIN -precision: 0.875634517766497"
[1] "TRAIN specifity: 0.518018018018018"
[1] "TRAIN sensitivity: 0.982246376811594"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       10       19
            positive       46      893
[1] "TEST accuracy: 0.932851239669422"
[1] "TEST +precision: 0.951011714589989"
[1] "TEST -precision: 0.344827586206897"
[1] "TEST specifity: 0.178571428571429"
[1] "TEST sensitivity: 0.979166666666667"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 10.5285410006841"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

3426 samples
 690 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 2740, 2741, 2741, 2741, 2741 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens       Spec     
  0.3  1          0.6               0.50        50      0.8451144  0.1291213  0.9981884
  0.3  1          0.6               0.50       100      0.9059203  0.2913141  0.9938406
  0.3  1          0.6               0.50       150      0.9211171  0.4428908  0.9905797
  0.3  1          0.6               0.75        50      0.8549178  0.1486590  0.9981884
  0.3  1          0.6               0.75       100      0.9063315  0.3273594  0.9945652
  0.3  1          0.6               0.75       150      0.9284916  0.4458983  0.9891304
  0.3  1          0.6               1.00        50      0.8623039  0.1471328  0.9978261
  0.3  1          0.6               1.00       100      0.9094529  0.2793401  0.9956522
  0.3  1          0.6               1.00       150      0.9310943  0.4353496  0.9920290
  0.3  1          0.8               0.50        50      0.8529858  0.1216025  0.9981884
  0.3  1          0.8               0.50       100      0.9062390  0.3183369  0.9927536
  0.3  1          0.8               0.50       150      0.9286051  0.4503872  0.9902174
  0.3  1          0.8               0.75        50      0.8601039  0.1516440  0.9981884
  0.3  1          0.8               0.75       100      0.9109791  0.2823364  0.9945652
  0.3  1          0.8               0.75       150      0.9283016  0.4639547  0.9894928
  0.3  1          0.8               1.00        50      0.8628382  0.1471215  0.9981884
  0.3  1          0.8               1.00       100      0.9096986  0.2748289  0.9956522
  0.3  1          0.8               1.00       150      0.9321865  0.4323196  0.9913043
  0.3  2          0.6               0.50        50      0.9050212  0.3047918  0.9942029
  0.3  2          0.6               0.50       100      0.9353643  0.5120189  0.9884058
  0.3  2          0.6               0.50       150      0.9502358  0.5931096  0.9855072
  0.3  2          0.6               0.75        50      0.9119250  0.2927393  0.9942029
  0.3  2          0.6               0.75       100      0.9441337  0.5554932  0.9891304
  0.3  2          0.6               0.75       150      0.9583061  0.6306363  0.9884058
  0.3  2          0.6               1.00        50      0.9110246  0.2912804  0.9960145
  0.3  2          0.6               1.00       100      0.9489346  0.5164404  0.9902174
  0.3  2          0.6               1.00       150      0.9623463  0.6126136  0.9887681
  0.3  2          0.8               0.50        50      0.9022946  0.3257659  0.9931159
  0.3  2          0.8               0.50       100      0.9388216  0.5089664  0.9873188
  0.3  2          0.8               0.50       150      0.9525616  0.6095837  0.9833333
  0.3  2          0.8               0.75        50      0.9121028  0.2897318  0.9945652
  0.3  2          0.8               0.75       100      0.9453900  0.5389855  0.9887681
  0.3  2          0.8               0.75       150      0.9584542  0.6291101  0.9865942
  0.3  2          0.8               1.00        50      0.9096340  0.2913141  0.9960145
  0.3  2          0.8               1.00       100      0.9492268  0.5149254  0.9902174
  0.3  2          0.8               1.00       150      0.9625900  0.5990573  0.9884058
  0.3  3          0.6               0.50        50      0.9261852  0.4099203  0.9913043
  0.3  3          0.6               0.50       100      0.9543116  0.6351139  0.9844203
  0.3  3          0.6               0.50       150      0.9640560  0.6951633  0.9818841
  0.3  3          0.6               0.75        50      0.9346208  0.4263831  0.9894928
  0.3  3          0.6               0.75       100      0.9598056  0.6245876  0.9869565
  0.3  3          0.6               0.75       150      0.9693441  0.7027045  0.9844203
  0.3  3          0.6               1.00        50      0.9383953  0.4218494  0.9923913
  0.3  3          0.6               1.00       100      0.9660440  0.6005723  0.9898551
  0.3  3          0.6               1.00       150      0.9753005  0.6847380  0.9873188
  0.3  3          0.8               0.50        50      0.9260148  0.4443721  0.9891304
  0.3  3          0.8               0.50       100      0.9527198  0.6276063  0.9836957
  0.3  3          0.8               0.50       150      0.9617174  0.7011559  0.9768116
  0.3  3          0.8               0.75        50      0.9328053  0.4473460  0.9920290
  0.3  3          0.8               0.75       100      0.9585184  0.6456627  0.9844203
  0.3  3          0.8               0.75       150      0.9680134  0.7012457  0.9811594
  0.3  3          0.8               1.00        50      0.9398566  0.4428908  0.9898551
  0.3  3          0.8               1.00       100      0.9665819  0.5990686  0.9880435
  0.3  3          0.8               1.00       150      0.9762424  0.6891931  0.9858696
  0.4  1          0.6               0.50        50      0.8561852  0.2387611  0.9949275
  0.4  1          0.6               0.50       100      0.9103025  0.3963865  0.9894928
  0.4  1          0.6               0.50       150      0.9368233  0.5240938  0.9847826
  0.4  1          0.6               0.75        50      0.8758595  0.2116822  0.9963768
  0.4  1          0.6               0.75       100      0.9237704  0.4564246  0.9920290
  0.4  1          0.6               0.75       150      0.9425714  0.5450791  0.9876812
  0.4  1          0.6               1.00        50      0.8787031  0.2102345  0.9949275
  0.4  1          0.6               1.00       100      0.9234479  0.4278644  0.9920290
  0.4  1          0.6               1.00       150      0.9446345  0.5209853  0.9898551
  0.4  1          0.8               0.50        50      0.8684907  0.2401751  0.9942029
  0.4  1          0.8               0.50       100      0.9069446  0.4384244  0.9873188
  0.4  1          0.8               0.50       150      0.9361125  0.5059589  0.9851449
  0.4  1          0.8               0.75        50      0.8726536  0.2161935  0.9960145
  0.4  1          0.8               0.75       100      0.9231820  0.4473909  0.9909420
  0.4  1          0.8               0.75       150      0.9411296  0.5540343  0.9851449
  0.4  1          0.8               1.00        50      0.8782137  0.1951857  0.9967391
  0.4  1          0.8               1.00       100      0.9232321  0.4488834  0.9923913
  0.4  1          0.8               1.00       150      0.9463004  0.5044439  0.9905797
  0.4  2          0.6               0.50        50      0.9167421  0.3843452  0.9920290
  0.4  2          0.6               0.50       100      0.9474901  0.5916059  0.9862319
  0.4  2          0.6               0.50       150      0.9607050  0.6636741  0.9822464
  0.4  2          0.6               0.75        50      0.9231146  0.4294131  0.9905797
  0.4  2          0.6               0.75       100      0.9522468  0.5960835  0.9880435
  0.4  2          0.6               0.75       150      0.9652943  0.6727303  0.9844203
  0.4  2          0.6               1.00        50      0.9261594  0.4428571  0.9916667
  0.4  2          0.6               1.00       100      0.9570394  0.5780608  0.9887681
  0.4  2          0.6               1.00       150      0.9694989  0.6651554  0.9847826
  0.4  2          0.8               0.50        50      0.9070581  0.3978341  0.9905797
  0.4  2          0.8               0.50       100      0.9450645  0.5855572  0.9829710
  0.4  2          0.8               0.50       150      0.9557861  0.6667265  0.9797101
  0.4  2          0.8               0.75        50      0.9241141  0.4383908  0.9873188
  0.4  2          0.8               0.75       100      0.9545546  0.5975873  0.9833333
  0.4  2          0.8               0.75       150      0.9637331  0.6966783  0.9804348
  0.4  2          0.8               1.00        50      0.9270535  0.4549097  0.9916667
  0.4  2          0.8               1.00       100      0.9575236  0.5840422  0.9887681
  0.4  2          0.8               1.00       150      0.9697051  0.6711368  0.9858696
  0.4  3          0.6               0.50        50      0.9312996  0.4999776  0.9869565
  0.4  3          0.6               0.50       100      0.9570196  0.6681517  0.9793478
  0.4  3          0.6               0.50       150      0.9659309  0.7327348  0.9760870
  0.4  3          0.6               0.75        50      0.9429922  0.5014925  0.9876812
  0.4  3          0.6               0.75       100      0.9653327  0.6846482  0.9826087
  0.4  3          0.6               0.75       150      0.9734407  0.7492425  0.9793478
  0.4  3          0.6               1.00        50      0.9520004  0.5089889  0.9923913
  0.4  3          0.6               1.00       100      0.9735793  0.6712041  0.9898551
  0.4  3          0.6               1.00       150      0.9803178  0.7522613  0.9847826
  0.4  3          0.8               0.50        50      0.9335937  0.5044888  0.9891304
  0.4  3          0.8               0.50       100      0.9579284  0.6711592  0.9793478
  0.4  3          0.8               0.50       150      0.9664414  0.7387499  0.9771739
  0.4  3          0.8               0.75        50      0.9501813  0.5285153  0.9873188
  0.4  3          0.8               0.75       100      0.9669868  0.7042083  0.9836957
  0.4  3          0.8               0.75       150      0.9736798  0.7553024  0.9804348
  0.4  3          0.8               1.00        50      0.9533590  0.5194479  0.9898551
  0.4  3          0.8               1.00       100      0.9729158  0.6787005  0.9858696
  0.4  3          0.8               1.00       150      0.9803339  0.7582538  0.9840580

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 150, max_depth = 3, eta = 0.4, gamma = 0, colsample_bytree = 0.8, min_child_weight = 1 and subsample = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     14.7      1.3
  positive      4.7     79.3
                            
 Accuracy (average) : 0.9402

[1] "TRAIN accuracy: 0.940163455925277"
[1] "TRAIN +precision: 0.944038929440389"
[1] "TRAIN -precision: 0.919854280510018"
[1] "TRAIN specifity: 0.758258258258258"
[1] "TRAIN sensitivity: 0.984057971014493"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       21       10
            positive       35      902
[1] "TEST accuracy: 0.953512396694215"
[1] "TEST +precision: 0.96264674493063"
[1] "TEST -precision: 0.67741935483871"
[1] "TEST specifity: 0.375"
[1] "TEST sensitivity: 0.989035087719298"

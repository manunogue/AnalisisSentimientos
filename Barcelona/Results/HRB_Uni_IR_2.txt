[1] "DATASET NAME: HRB_Uni_IR_2"
[1] "TRAIN INSTANCES: 1343"
[1] "TEST INSTANCES: 332"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 9.41455507278442"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

1343 samples
 652 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1075, 1074, 1074, 1074, 1075 
Resampling results:

  ROC        Sens      Spec     
  0.9907197  0.971798  0.9740063

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     36.0      1.6
  positive      1.0     61.4
                            
 Accuracy (average) : 0.9732

[1] "TRAIN accuracy: 0.97319434102755"
[1] "TRAIN +precision: 0.983293556085919"
[1] "TRAIN -precision: 0.956435643564356"
[1] "TRAIN specifity: 0.971830985915493"
[1] "TRAIN sensitivity: 0.973995271867612"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       28        4
            positive       17      283
[1] "TEST accuracy: 0.936746987951807"
[1] "TEST +precision: 0.943333333333333"
[1] "TEST -precision: 0.875"
[1] "TEST specifity: 0.622222222222222"
[1] "TEST sensitivity: 0.986062717770035"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 1.83469971815745"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

1343 samples
 652 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1075, 1075, 1073, 1074, 1075 
Resampling results across tuning parameters:

  C      M  ROC        Sens       Spec     
  0.010  1  0.9050001  0.8612525  0.9101497
  0.010  2  0.9040752  0.8551919  0.9077828
  0.010  3  0.9078153  0.8411515  0.9077967
  0.255  1  0.9359461  0.9416364  0.9125235
  0.255  2  0.9304046  0.9153939  0.9113470
  0.255  3  0.9326561  0.8711919  0.9148834
  0.500  1  0.9407159  0.9436566  0.9125235
  0.500  2  0.9307061  0.9153939  0.9113470
  0.500  3  0.9333712  0.8751919  0.9148834

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.5 and M = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     34.9      5.5
  positive      2.1     57.5
                            
 Accuracy (average) : 0.9241

[1] "TRAIN accuracy: 0.924050632911392"
[1] "TRAIN +precision: 0.965"
[1] "TRAIN -precision: 0.863720073664825"
[1] "TRAIN specifity: 0.943661971830986"
[1] "TRAIN sensitivity: 0.912529550827423"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       26       33
            positive       19      254
[1] "TEST accuracy: 0.843373493975904"
[1] "TEST +precision: 0.93040293040293"
[1] "TEST -precision: 0.440677966101695"
[1] "TEST specifity: 0.577777777777778"
[1] "TEST sensitivity: 0.885017421602787"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 4.83496653238932"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

1343 samples
 652 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1075, 1073, 1075, 1074, 1075 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens       Spec     
  0.3  1          0.6               0.50        50      0.9604644  0.7487273  0.9598120
  0.3  1          0.6               0.50       100      0.9757627  0.8713939  0.9598120
  0.3  1          0.6               0.50       150      0.9839977  0.9074747  0.9562548
  0.3  1          0.6               0.75        50      0.9593198  0.7608283  0.9633623
  0.3  1          0.6               0.75       100      0.9781959  0.8553333  0.9621719
  0.3  1          0.6               0.75       150      0.9850559  0.8934545  0.9657292
  0.3  1          0.6               1.00        50      0.9599539  0.7305455  0.9633693
  0.3  1          0.6               1.00       100      0.9780184  0.8492525  0.9657431
  0.3  1          0.6               1.00       150      0.9850617  0.8914949  0.9680961
  0.3  1          0.8               0.50        50      0.9572198  0.7606869  0.9610024
  0.3  1          0.8               0.50       100      0.9767220  0.8532323  0.9598260
  0.3  1          0.8               0.50       150      0.9834186  0.8934949  0.9610024
  0.3  1          0.8               0.75        50      0.9613987  0.7667475  0.9657222
  0.3  1          0.8               0.75       100      0.9789684  0.8593131  0.9645458
  0.3  1          0.8               0.75       150      0.9865423  0.9035354  0.9657292
  0.3  1          0.8               1.00        50      0.9587610  0.7205051  0.9680891
  0.3  1          0.8               1.00       100      0.9789442  0.8573131  0.9669126
  0.3  1          0.8               1.00       150      0.9849237  0.8934747  0.9704629
  0.3  2          0.6               0.50        50      0.9798782  0.8592727  0.9609955
  0.3  2          0.6               0.50       100      0.9901288  0.9476970  0.9669057
  0.3  2          0.6               0.50       150      0.9919044  0.9637374  0.9680821
  0.3  2          0.6               0.75        50      0.9818868  0.8753939  0.9598260
  0.3  2          0.6               0.75       100      0.9903230  0.9436768  0.9681030
  0.3  2          0.6               0.75       150      0.9921278  0.9657778  0.9681030
  0.3  2          0.6               1.00        50      0.9817074  0.8835152  0.9680961
  0.3  2          0.6               1.00       100      0.9897089  0.9336364  0.9716464
  0.3  2          0.6               1.00       150      0.9910139  0.9617374  0.9716464
  0.3  2          0.8               0.50        50      0.9800375  0.8854141  0.9598120
  0.3  2          0.8               0.50       100      0.9886890  0.9376970  0.9657153
  0.3  2          0.8               0.50       150      0.9912097  0.9839192  0.9680821
  0.3  2          0.8               0.75        50      0.9816072  0.8632929  0.9669126
  0.3  2          0.8               0.75       100      0.9908628  0.9557576  0.9728228
  0.3  2          0.8               0.75       150      0.9913015  0.9597374  0.9704490
  0.3  2          0.8               1.00        50      0.9844505  0.8693333  0.9669196
  0.3  2          0.8               1.00       100      0.9907333  0.9557576  0.9669196
  0.3  2          0.8               1.00       150      0.9924473  0.9738586  0.9728298
  0.3  3          0.6               0.50        50      0.9851896  0.9095960  0.9479916
  0.3  3          0.6               0.50       100      0.9909028  0.9799394  0.9621859
  0.3  3          0.6               0.50       150      0.9908050  0.9839394  0.9598120
  0.3  3          0.6               0.75        50      0.9884275  0.9357374  0.9598120
  0.3  3          0.6               0.75       100      0.9921210  0.9859192  0.9680891
  0.3  3          0.6               0.75       150      0.9920981  0.9859596  0.9645458
  0.3  3          0.6               1.00        50      0.9891589  0.9316566  0.9621859
  0.3  3          0.6               1.00       100      0.9924914  0.9819192  0.9728298
  0.3  3          0.6               1.00       150      0.9932295  0.9839192  0.9716464
  0.3  3          0.8               0.50        50      0.9876332  0.9075152  0.9633693
  0.3  3          0.8               0.50       100      0.9913205  0.9798788  0.9633554
  0.3  3          0.8               0.50       150      0.9919966  0.9859394  0.9586286
  0.3  3          0.8               0.75        50      0.9893999  0.9457576  0.9669057
  0.3  3          0.8               0.75       100      0.9930545  0.9839192  0.9704490
  0.3  3          0.8               0.75       150      0.9929236  0.9859596  0.9704490
  0.3  3          0.8               1.00        50      0.9901754  0.9357172  0.9669057
  0.3  3          0.8               1.00       100      0.9929864  0.9818990  0.9728298
  0.3  3          0.8               1.00       150      0.9937963  0.9859394  0.9704629
  0.4  1          0.6               0.50        50      0.9680724  0.8129899  0.9598260
  0.4  1          0.6               0.50       100      0.9827202  0.8834343  0.9527254
  0.4  1          0.6               0.50       150      0.9854858  0.9194949  0.9586356
  0.4  1          0.6               0.75        50      0.9685571  0.8090505  0.9610094
  0.4  1          0.6               0.75       100      0.9817286  0.8833939  0.9609955
  0.4  1          0.6               0.75       150      0.9868663  0.9095152  0.9645388
  0.4  1          0.6               1.00        50      0.9661510  0.7849293  0.9680961
  0.4  1          0.6               1.00       100      0.9825579  0.8653737  0.9657362
  0.4  1          0.6               1.00       150      0.9864714  0.9195758  0.9692795
  0.4  1          0.8               0.50        50      0.9644104  0.7867475  0.9574452
  0.4  1          0.8               0.50       100      0.9820637  0.8713535  0.9527254
  0.4  1          0.8               0.50       150      0.9877019  0.9276162  0.9598399
  0.4  1          0.8               0.75        50      0.9663175  0.8129697  0.9609885
  0.4  1          0.8               0.75       100      0.9811561  0.8914949  0.9586425
  0.4  1          0.8               0.75       150      0.9878410  0.9356970  0.9633693
  0.4  1          0.8               1.00        50      0.9681474  0.8050303  0.9621859
  0.4  1          0.8               1.00       100      0.9823306  0.8794141  0.9704699
  0.4  1          0.8               1.00       150      0.9862527  0.9155556  0.9704699
  0.4  2          0.6               0.50        50      0.9809004  0.9076162  0.9621719
  0.4  2          0.6               0.50       100      0.9901431  0.9518182  0.9609885
  0.4  2          0.6               0.50       150      0.9904989  0.9617980  0.9550714
  0.4  2          0.6               0.75        50      0.9865991  0.9216364  0.9669057
  0.4  2          0.6               0.75       100      0.9912658  0.9657980  0.9763662
  0.4  2          0.6               0.75       150      0.9913204  0.9758788  0.9775566
  0.4  2          0.6               1.00        50      0.9860263  0.9014949  0.9645527
  0.4  2          0.6               1.00       100      0.9907914  0.9617778  0.9680961
  0.4  2          0.6               1.00       150      0.9925915  0.9818990  0.9669126
  0.4  2          0.8               0.50        50      0.9853095  0.9115960  0.9586286
  0.4  2          0.8               0.50       100      0.9911040  0.9557374  0.9586425
  0.4  2          0.8               0.50       150      0.9903727  0.9697980  0.9621859
  0.4  2          0.8               0.75        50      0.9873472  0.9156566  0.9657153
  0.4  2          0.8               0.75       100      0.9923165  0.9819192  0.9739923
  0.4  2          0.8               0.75       150      0.9925537  0.9839596  0.9728228
  0.4  2          0.8               1.00        50      0.9891447  0.9075960  0.9704560
  0.4  2          0.8               1.00       100      0.9921755  0.9658182  0.9692795
  0.4  2          0.8               1.00       150      0.9933866  0.9859394  0.9716464
  0.4  3          0.6               0.50        50      0.9882451  0.9456162  0.9609955
  0.4  3          0.6               0.50       100      0.9903441  0.9798990  0.9621789
  0.4  3          0.6               0.50       150      0.9910520  0.9758586  0.9586216
  0.4  3          0.6               0.75        50      0.9901430  0.9718788  0.9610024
  0.4  3          0.6               0.75       100      0.9919154  0.9799192  0.9645527
  0.4  3          0.6               0.75       150      0.9913958  0.9799192  0.9598051
  0.4  3          0.6               1.00        50      0.9911483  0.9658182  0.9657362
  0.4  3          0.6               1.00       100      0.9938230  0.9859192  0.9704629
  0.4  3          0.6               1.00       150      0.9934894  0.9859596  0.9704560
  0.4  3          0.8               0.50        50      0.9873574  0.9437172  0.9503585
  0.4  3          0.8               0.50       100      0.9909585  0.9698788  0.9633484
  0.4  3          0.8               0.50       150      0.9917907  0.9738384  0.9609816
  0.4  3          0.8               0.75        50      0.9911828  0.9637576  0.9680961
  0.4  3          0.8               0.75       100      0.9929269  0.9798990  0.9586356
  0.4  3          0.8               0.75       150      0.9924406  0.9839394  0.9610024
  0.4  3          0.8               1.00        50      0.9912742  0.9778586  0.9692795
  0.4  3          0.8               1.00       100      0.9937515  0.9838990  0.9704560
  0.4  3          0.8               1.00       150      0.9938612  0.9778788  0.9669057

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 150, max_depth = 3, eta = 0.4, gamma = 0, colsample_bytree = 0.8, min_child_weight = 1 and subsample = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     36.2      2.1
  positive      0.8     60.9
                           
 Accuracy (average) : 0.971

[1] "TRAIN accuracy: 0.970960536113179"
[1] "TRAIN +precision: 0.986731001206273"
[1] "TRAIN -precision: 0.945525291828794"
[1] "TRAIN specifity: 0.977867203219316"
[1] "TRAIN sensitivity: 0.966903073286052"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       32        7
            positive       13      280
[1] "TEST accuracy: 0.939759036144578"
[1] "TEST +precision: 0.955631399317406"
[1] "TEST -precision: 0.82051282051282"
[1] "TEST specifity: 0.711111111111111"
[1] "TEST sensitivity: 0.975609756097561"

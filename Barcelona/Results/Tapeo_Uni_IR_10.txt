[1] "DATASET NAME: Tapeo_Uni_IR_10"
[1] "TRAIN INSTANCES: 1089"
[1] "TEST INSTANCES: 333"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 3.80080413818359"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

1089 samples
 672 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 871, 872, 871, 871, 871 
Resampling results:

  ROC        Sens       Spec     
  0.9988354  0.9609231  0.9947917

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     11.3      0.5
  positive      0.5     87.8
                            
 Accuracy (average) : 0.9908

[1] "TRAIN accuracy: 0.990817263544536"
[1] "TRAIN +precision: 0.994797086368366"
[1] "TRAIN -precision: 0.9609375"
[1] "TRAIN specifity: 0.9609375"
[1] "TRAIN sensitivity: 0.994797086368366"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        5        4
            positive        6      318
[1] "TEST accuracy: 0.96996996996997"
[1] "TEST +precision: 0.981481481481482"
[1] "TEST -precision: 0.555555555555556"
[1] "TEST specifity: 0.454545454545455"
[1] "TEST sensitivity: 0.987577639751553"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 1.51665948629379"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

1089 samples
 672 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 871, 870, 872, 872, 871 
Resampling results across tuning parameters:

  C      M  ROC        Sens       Spec     
  0.010  1  0.8050016  0.5550769  0.9802299
  0.010  2  0.7827527  0.4756923  0.9812716
  0.010  3  0.7707373  0.4141538  0.9791937
  0.255  1  0.9467888  0.7978462  0.9781466
  0.255  2  0.9456366  0.7658462  0.9781466
  0.255  3  0.9279766  0.6873846  0.9781520
  0.500  1  0.9646051  0.8289231  0.9781466
  0.500  2  0.9637429  0.7969231  0.9781466
  0.500  3  0.9408794  0.7027692  0.9781520

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.5 and M = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative      9.7      1.9
  positive      2.0     86.3
                            
 Accuracy (average) : 0.9605

[1] "TRAIN accuracy: 0.960514233241506"
[1] "TRAIN +precision: 0.977130977130977"
[1] "TRAIN -precision: 0.834645669291339"
[1] "TRAIN specifity: 0.828125"
[1] "TRAIN sensitivity: 0.978147762747138"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        1        8
            positive       10      314
[1] "TEST accuracy: 0.945945945945946"
[1] "TEST +precision: 0.969135802469136"
[1] "TEST -precision: 0.111111111111111"
[1] "TEST specifity: 0.0909090909090909"
[1] "TEST sensitivity: 0.975155279503106"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 4.14165980021159"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

1089 samples
 672 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 870, 871, 871, 872, 872 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens       Spec     
  0.3  1          0.6               0.50        50      0.9727022  0.4686154  0.9937554
  0.3  1          0.6               0.50       100      0.9861685  0.7593846  0.9916721
  0.3  1          0.6               0.50       150      0.9873243  0.8366154  0.9927083
  0.3  1          0.6               0.75        50      0.9799842  0.4529231  0.9979167
  0.3  1          0.6               0.75       100      0.9830638  0.7507692  0.9947917
  0.3  1          0.6               0.75       150      0.9867774  0.8603077  0.9906304
  0.3  1          0.6               1.00        50      0.9775341  0.4049231  0.9979167
  0.3  1          0.6               1.00       100      0.9841832  0.7104615  0.9947917
  0.3  1          0.6               1.00       150      0.9858181  0.8523077  0.9927083
  0.3  1          0.8               0.50        50      0.9718314  0.4369231  0.9979167
  0.3  1          0.8               0.50       100      0.9820529  0.7664615  0.9937500
  0.3  1          0.8               0.50       150      0.9830515  0.8753846  0.9895887
  0.3  1          0.8               0.75        50      0.9720406  0.4529231  0.9968750
  0.3  1          0.8               0.75       100      0.9831547  0.7581538  0.9916667
  0.3  1          0.8               0.75       150      0.9832692  0.8756923  0.9927083
  0.3  1          0.8               1.00        50      0.9783035  0.4455385  0.9979167
  0.3  1          0.8               1.00       100      0.9829774  0.6873846  0.9958333
  0.3  1          0.8               1.00       150      0.9840231  0.8600000  0.9937500
  0.3  2          0.6               0.50        50      0.9849420  0.7356923  0.9968750
  0.3  2          0.6               0.50       100      0.9909283  0.8913846  0.9958333
  0.3  2          0.6               0.50       150      0.9928538  0.9144615  0.9916721
  0.3  2          0.6               0.75        50      0.9859498  0.8292308  0.9968750
  0.3  2          0.6               0.75       100      0.9907208  0.9067692  0.9916667
  0.3  2          0.6               0.75       150      0.9928258  0.9458462  0.9906250
  0.3  2          0.6               1.00        50      0.9872076  0.7975385  0.9968750
  0.3  2          0.6               1.00       100      0.9919381  0.8836923  0.9947917
  0.3  2          0.6               1.00       150      0.9935106  0.9458462  0.9937500
  0.3  2          0.8               0.50        50      0.9829685  0.7821538  0.9937500
  0.3  2          0.8               0.50       100      0.9894079  0.9144615  0.9937500
  0.3  2          0.8               0.50       150      0.9935343  0.9144615  0.9875000
  0.3  2          0.8               0.75        50      0.9857008  0.8212308  0.9968750
  0.3  2          0.8               0.75       100      0.9912779  0.8913846  0.9927137
  0.3  2          0.8               0.75       150      0.9920183  0.9304615  0.9916721
  0.3  2          0.8               1.00        50      0.9859005  0.8049231  0.9968750
  0.3  2          0.8               1.00       100      0.9896831  0.8913846  0.9958333
  0.3  2          0.8               1.00       150      0.9916685  0.9458462  0.9937500
  0.3  3          0.6               0.50        50      0.9877884  0.8676923  0.9947917
  0.3  3          0.6               0.50       100      0.9914488  0.9298462  0.9947917
  0.3  3          0.6               0.50       150      0.9922825  0.9298462  0.9916667
  0.3  3          0.6               0.75        50      0.9927705  0.8913846  0.9968750
  0.3  3          0.6               0.75       100      0.9946334  0.9298462  0.9937500
  0.3  3          0.6               0.75       150      0.9947188  0.9458462  0.9916667
  0.3  3          0.6               1.00        50      0.9900425  0.8913846  0.9979167
  0.3  3          0.6               1.00       100      0.9919231  0.9458462  0.9947971
  0.3  3          0.6               1.00       150      0.9926048  0.9615385  0.9947917
  0.3  3          0.8               0.50        50      0.9910581  0.8596923  0.9968750
  0.3  3          0.8               0.50       100      0.9927725  0.9304615  0.9916667
  0.3  3          0.8               0.50       150      0.9936162  0.9458462  0.9927083
  0.3  3          0.8               0.75        50      0.9903869  0.8836923  0.9937500
  0.3  3          0.8               0.75       100      0.9939147  0.9458462  0.9937554
  0.3  3          0.8               0.75       150      0.9949158  0.9458462  0.9927083
  0.3  3          0.8               1.00        50      0.9886359  0.8913846  0.9968750
  0.3  3          0.8               1.00       100      0.9928022  0.9458462  0.9937500
  0.3  3          0.8               1.00       150      0.9923619  0.9615385  0.9927083
  0.4  1          0.6               0.50        50      0.9779460  0.6409231  0.9958333
  0.4  1          0.6               0.50       100      0.9806280  0.8369231  0.9927083
  0.4  1          0.6               0.50       150      0.9833556  0.8913846  0.9885471
  0.4  1          0.6               0.75        50      0.9798631  0.5861538  0.9937500
  0.4  1          0.6               0.75       100      0.9847078  0.8523077  0.9927083
  0.4  1          0.6               0.75       150      0.9852885  0.8836923  0.9885525
  0.4  1          0.6               1.00        50      0.9798338  0.6092308  0.9968750
  0.4  1          0.6               1.00       100      0.9824763  0.8600000  0.9937500
  0.4  1          0.6               1.00       150      0.9865996  0.8836923  0.9927137
  0.4  1          0.8               0.50        50      0.9699581  0.6098462  0.9895995
  0.4  1          0.8               0.50       100      0.9878467  0.8673846  0.9875000
  0.4  1          0.8               0.50       150      0.9870597  0.9144615  0.9864691
  0.4  1          0.8               0.75        50      0.9789279  0.5633846  0.9958333
  0.4  1          0.8               0.75       100      0.9853105  0.8833846  0.9916721
  0.4  1          0.8               0.75       150      0.9880480  0.8913846  0.9906250
  0.4  1          0.8               1.00        50      0.9800152  0.5381538  0.9958333
  0.4  1          0.8               1.00       100      0.9824624  0.8129231  0.9927083
  0.4  1          0.8               1.00       150      0.9855771  0.8836923  0.9927083
  0.4  2          0.6               0.50        50      0.9868975  0.8676923  0.9927083
  0.4  2          0.6               0.50       100      0.9885111  0.9298462  0.9916667
  0.4  2          0.6               0.50       150      0.9904758  0.9144615  0.9885525
  0.4  2          0.6               0.75        50      0.9872181  0.8523077  0.9927137
  0.4  2          0.6               0.75       100      0.9920344  0.9304615  0.9927083
  0.4  2          0.6               0.75       150      0.9926040  0.9458462  0.9906250
  0.4  2          0.6               1.00        50      0.9900704  0.8680000  0.9947917
  0.4  2          0.6               1.00       100      0.9919704  0.9298462  0.9937500
  0.4  2          0.6               1.00       150      0.9923498  0.9458462  0.9916667
  0.4  2          0.8               0.50        50      0.9870116  0.8289231  0.9927137
  0.4  2          0.8               0.50       100      0.9890202  0.9458462  0.9895887
  0.4  2          0.8               0.50       150      0.9907702  0.9298462  0.9895833
  0.4  2          0.8               0.75        50      0.9876362  0.8680000  0.9947917
  0.4  2          0.8               0.75       100      0.9915331  0.9298462  0.9906358
  0.4  2          0.8               0.75       150      0.9934696  0.9458462  0.9927083
  0.4  2          0.8               1.00        50      0.9837067  0.8760000  0.9958333
  0.4  2          0.8               1.00       100      0.9876978  0.9458462  0.9947917
  0.4  2          0.8               1.00       150      0.9905568  0.9458462  0.9906250
  0.4  3          0.6               0.50        50      0.9852278  0.8987692  0.9927083
  0.4  3          0.6               0.50       100      0.9900441  0.9298462  0.9906250
  0.4  3          0.6               0.50       150      0.9911734  0.9298462  0.9906250
  0.4  3          0.6               0.75        50      0.9903070  0.8990769  0.9958333
  0.4  3          0.6               0.75       100      0.9917581  0.9458462  0.9927083
  0.4  3          0.6               0.75       150      0.9930040  0.9615385  0.9916667
  0.4  3          0.6               1.00        50      0.9900295  0.8913846  0.9968750
  0.4  3          0.6               1.00       100      0.9928456  0.9615385  0.9947917
  0.4  3          0.6               1.00       150      0.9925272  0.9615385  0.9947917
  0.4  3          0.8               0.50        50      0.9918957  0.8913846  0.9916775
  0.4  3          0.8               0.50       100      0.9921469  0.9144615  0.9906304
  0.4  3          0.8               0.50       150      0.9935916  0.9298462  0.9906358
  0.4  3          0.8               0.75        50      0.9929946  0.9298462  0.9947917
  0.4  3          0.8               0.75       100      0.9940043  0.9458462  0.9937500
  0.4  3          0.8               0.75       150      0.9953218  0.9615385  0.9947917
  0.4  3          0.8               1.00        50      0.9912297  0.9067692  0.9958333
  0.4  3          0.8               1.00       100      0.9930026  0.9538462  0.9937500
  0.4  3          0.8               1.00       150      0.9926061  0.9615385  0.9927083

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 150, max_depth = 3, eta = 0.4, gamma = 0, colsample_bytree = 0.8, min_child_weight = 1 and subsample = 0.75.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     11.3      0.5
  positive      0.5     87.8
                            
 Accuracy (average) : 0.9908

[1] "TRAIN accuracy: 0.990817263544536"
[1] "TRAIN +precision: 0.994797086368366"
[1] "TRAIN -precision: 0.9609375"
[1] "TRAIN specifity: 0.9609375"
[1] "TRAIN sensitivity: 0.994797086368366"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        4        4
            positive        7      318
[1] "TEST accuracy: 0.966966966966967"
[1] "TEST +precision: 0.978461538461538"
[1] "TEST -precision: 0.5"
[1] "TEST specifity: 0.363636363636364"
[1] "TEST sensitivity: 0.987577639751553"

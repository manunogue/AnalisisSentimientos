[1] "DATASET NAME: Tapeo_Uni_IR_2"
[1] "TRAIN INSTANCES: 1459"
[1] "TEST INSTANCES: 333"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 4.68728709220886"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

1459 samples
 672 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1168, 1166, 1167, 1167, 1168 
Resampling results:

  ROC        Sens  Spec     
  0.9989689  1     0.9989583

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     34.1      0.1
  positive      0.0     65.8
                            
 Accuracy (average) : 0.9993

[1] "TRAIN accuracy: 0.999314599040439"
[1] "TRAIN +precision: 1"
[1] "TRAIN -precision: 0.997995991983968"
[1] "TRAIN specifity: 1"
[1] "TRAIN sensitivity: 0.998959417273673"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        4        3
            positive        7      319
[1] "TEST accuracy: 0.96996996996997"
[1] "TEST +precision: 0.978527607361963"
[1] "TEST -precision: 0.571428571428571"
[1] "TEST specifity: 0.363636363636364"
[1] "TEST sensitivity: 0.990683229813665"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 1.97997439702352"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

1459 samples
 672 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1167, 1166, 1168, 1167, 1168 
Resampling results across tuning parameters:

  C      M  ROC        Sens  Spec     
  0.010  1  0.9841419  1     0.9635687
  0.010  2  0.9854897  1     0.9635687
  0.010  3  0.9902769  1     0.9614853
  0.255  1  0.9841513  1     0.9666937
  0.255  2  0.9856242  1     0.9666937
  0.255  3  0.9896132  1     0.9698133
  0.500  1  0.9831445  1     0.9666937
  0.500  2  0.9846225  1     0.9666937
  0.500  3  0.9899774  1     0.9698133

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.01 and M = 3.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     34.1      2.5
  positive      0.0     63.3
                            
 Accuracy (average) : 0.9746

[1] "TRAIN accuracy: 0.97464016449623"
[1] "TRAIN +precision: 1"
[1] "TRAIN -precision: 0.930841121495327"
[1] "TRAIN specifity: 1"
[1] "TRAIN sensitivity: 0.96149843912591"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        4       13
            positive        7      309
[1] "TEST accuracy: 0.93993993993994"
[1] "TEST +precision: 0.977848101265823"
[1] "TEST -precision: 0.235294117647059"
[1] "TEST specifity: 0.363636363636364"
[1] "TEST sensitivity: 0.959627329192547"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 5.34210834900538"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

1459 samples
 672 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1167, 1167, 1168, 1168, 1166 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens       Spec     
  0.3  1          0.6               0.50        50      0.9910821  0.8936970  0.9760525
  0.3  1          0.6               0.50       100      0.9968362  1.0000000  0.9760633
  0.3  1          0.6               0.50       150      0.9980249  1.0000000  0.9833549
  0.3  1          0.6               0.75        50      0.9908016  0.9538182  0.9687716
  0.3  1          0.6               0.75       100      0.9971289  1.0000000  0.9791883
  0.3  1          0.6               0.75       150      0.9982543  1.0000000  0.9833603
  0.3  1          0.6               1.00        50      0.9909409  0.9276566  0.9698133
  0.3  1          0.6               1.00       100      0.9976692  1.0000000  0.9864691
  0.3  1          0.6               1.00       150      0.9981609  1.0000000  0.9885525
  0.3  1          0.8               0.50        50      0.9905726  0.9216566  0.9708549
  0.3  1          0.8               0.50       100      0.9964415  1.0000000  0.9739745
  0.3  1          0.8               0.50       150      0.9975341  1.0000000  0.9781466
  0.3  1          0.8               0.75        50      0.9898954  0.9296970  0.9656574
  0.3  1          0.8               0.75       100      0.9972862  1.0000000  0.9812770
  0.3  1          0.8               0.75       150      0.9983395  1.0000000  0.9854437
  0.3  1          0.8               1.00        50      0.9913815  0.9296364  0.9760741
  0.3  1          0.8               1.00       100      0.9975322  1.0000000  0.9833495
  0.3  1          0.8               1.00       150      0.9984233  1.0000000  0.9916775
  0.3  2          0.6               0.50        50      0.9975574  1.0000000  0.9760687
  0.3  2          0.6               0.50       100      0.9987343  1.0000000  0.9885579
  0.3  2          0.6               0.50       150      0.9986283  1.0000000  0.9875162
  0.3  2          0.6               0.75        50      0.9988434  1.0000000  0.9802191
  0.3  2          0.6               0.75       100      0.9993224  1.0000000  0.9885579
  0.3  2          0.6               0.75       150      0.9998021  1.0000000  0.9895941
  0.3  2          0.6               1.00        50      0.9992388  1.0000000  0.9823187
  0.3  2          0.6               1.00       100      0.9991766  1.0000000  0.9937554
  0.3  2          0.6               1.00       150      0.9995208  1.0000000  0.9968750
  0.3  2          0.8               0.50        50      0.9987701  1.0000000  0.9708549
  0.3  2          0.8               0.50       100      0.9992077  1.0000000  0.9864853
  0.3  2          0.8               0.50       150      0.9988887  1.0000000  0.9895995
  0.3  2          0.8               0.75        50      0.9991961  1.0000000  0.9781412
  0.3  2          0.8               0.75       100      0.9993125  1.0000000  0.9927245
  0.3  2          0.8               0.75       150      0.9992492  1.0000000  0.9937608
  0.3  2          0.8               1.00        50      0.9988330  1.0000000  0.9781412
  0.3  2          0.8               1.00       100      0.9991979  1.0000000  0.9916775
  0.3  2          0.8               1.00       150      0.9992917  1.0000000  0.9937554
  0.3  3          0.6               0.50        50      0.9990490  1.0000000  0.9906304
  0.3  3          0.6               0.50       100      1.0000000  1.0000000  0.9875216
  0.3  3          0.6               0.50       150      0.9999053  1.0000000  0.9854383
  0.3  3          0.6               0.75        50      0.9997708  1.0000000  0.9906304
  0.3  3          0.6               0.75       100      1.0000000  1.0000000  0.9958333
  0.3  3          0.6               0.75       150      1.0000000  1.0000000  0.9937554
  0.3  3          0.6               1.00        50      1.0000000  1.0000000  0.9937608
  0.3  3          0.6               1.00       100      0.9999792  1.0000000  0.9937608
  0.3  3          0.6               1.00       150      1.0000000  1.0000000  0.9947971
  0.3  3          0.8               0.50        50      0.9994479  1.0000000  0.9885417
  0.3  3          0.8               0.50       100      0.9998646  1.0000000  0.9885579
  0.3  3          0.8               0.50       150      0.9998646  1.0000000  0.9875108
  0.3  3          0.8               0.75        50      0.9999792  1.0000000  0.9916667
  0.3  3          0.8               0.75       100      1.0000000  1.0000000  0.9958333
  0.3  3          0.8               0.75       150      1.0000000  1.0000000  0.9927191
  0.3  3          0.8               1.00        50      0.9991458  1.0000000  0.9927137
  0.3  3          0.8               1.00       100      0.9996458  1.0000000  0.9958333
  0.3  3          0.8               1.00       150      0.9999375  1.0000000  0.9947971
  0.4  1          0.6               0.50        50      0.9937114  0.9397172  0.9719020
  0.4  1          0.6               0.50       100      0.9980692  1.0000000  0.9802353
  0.4  1          0.6               0.50       150      0.9980039  1.0000000  0.9864799
  0.4  1          0.6               0.75        50      0.9937786  0.9497374  0.9750216
  0.4  1          0.6               0.75       100      0.9974931  1.0000000  0.9833495
  0.4  1          0.6               0.75       150      0.9980853  1.0000000  0.9875216
  0.4  1          0.6               1.00        50      0.9965644  0.9779192  0.9770941
  0.4  1          0.6               1.00       100      0.9982454  1.0000000  0.9875108
  0.4  1          0.6               1.00       150      0.9987053  1.0000000  0.9927191
  0.4  1          0.8               0.50        50      0.9937785  0.9718384  0.9739745
  0.4  1          0.8               0.50       100      0.9979423  1.0000000  0.9760687
  0.4  1          0.8               0.50       150      0.9980155  1.0000000  0.9895887
  0.4  1          0.8               0.75        50      0.9959319  0.9779192  0.9708549
  0.4  1          0.8               0.75       100      0.9982217  1.0000000  0.9864745
  0.4  1          0.8               0.75       150      0.9981911  1.0000000  0.9864853
  0.4  1          0.8               1.00        50      0.9956739  0.9598182  0.9771049
  0.4  1          0.8               1.00       100      0.9982023  1.0000000  0.9906412
  0.4  1          0.8               1.00       150      0.9985154  1.0000000  0.9895941
  0.4  2          0.6               0.50        50      0.9987710  1.0000000  0.9802299
  0.4  2          0.6               0.50       100      0.9989792  1.0000000  0.9885633
  0.4  2          0.6               0.50       150      0.9990411  1.0000000  0.9843966
  0.4  2          0.6               0.75        50      0.9999374  1.0000000  0.9906358
  0.4  2          0.6               0.75       100      0.9998333  1.0000000  0.9937608
  0.4  2          0.6               0.75       150      0.9998958  1.0000000  0.9906412
  0.4  2          0.6               1.00        50      0.9991667  1.0000000  0.9895941
  0.4  2          0.6               1.00       100      0.9992708  1.0000000  0.9958333
  0.4  2          0.6               1.00       150      0.9992083  1.0000000  0.9958333
  0.4  2          0.8               0.50        50      0.9986001  1.0000000  0.9812554
  0.4  2          0.8               0.50       100      0.9992911  1.0000000  0.9843858
  0.4  2          0.8               0.50       150      0.9993933  1.0000000  0.9771157
  0.4  2          0.8               0.75        50      0.9987141  1.0000000  0.9875054
  0.4  2          0.8               0.75       100      0.9990605  1.0000000  0.9895995
  0.4  2          0.8               0.75       150      0.9990146  1.0000000  0.9885579
  0.4  2          0.8               1.00        50      0.9988954  1.0000000  0.9885471
  0.4  2          0.8               1.00       100      0.9989792  1.0000000  0.9927137
  0.4  2          0.8               1.00       150      0.9989266  1.0000000  0.9937554
  0.4  3          0.6               0.50        50      0.9999474  1.0000000  0.9885525
  0.4  3          0.6               0.50       100      0.9999474  1.0000000  0.9885579
  0.4  3          0.6               0.50       150      1.0000000  1.0000000  0.9885579
  0.4  3          0.6               0.75        50      1.0000000  1.0000000  0.9896049
  0.4  3          0.6               0.75       100      1.0000000  1.0000000  0.9906466
  0.4  3          0.6               0.75       150      1.0000000  1.0000000  0.9896049
  0.4  3          0.6               1.00        50      1.0000000  1.0000000  0.9947917
  0.4  3          0.6               1.00       100      1.0000000  1.0000000  0.9958387
  0.4  3          0.6               1.00       150      1.0000000  1.0000000  0.9958333
  0.4  3          0.8               0.50        50      0.9998843  1.0000000  0.9895941
  0.4  3          0.8               0.50       100      0.9999053  1.0000000  0.9854383
  0.4  3          0.8               0.50       150      1.0000000  1.0000000  0.9864745
  0.4  3          0.8               0.75        50      1.0000000  1.0000000  0.9885471
  0.4  3          0.8               0.75       100      1.0000000  1.0000000  0.9927137
  0.4  3          0.8               0.75       150      1.0000000  1.0000000  0.9895887
  0.4  3          0.8               1.00        50      0.9995937  1.0000000  0.9916775
  0.4  3          0.8               1.00       100      0.9999375  1.0000000  0.9948025
  0.4  3          0.8               1.00       150      1.0000000  1.0000000  0.9916883

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 50, max_depth = 3, eta = 0.3, gamma = 0, colsample_bytree = 0.6, min_child_weight = 1 and subsample = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     34.1      0.4
  positive      0.0     65.5
                            
 Accuracy (average) : 0.9959

[1] "TRAIN accuracy: 0.995887594242632"
[1] "TRAIN +precision: 1"
[1] "TRAIN -precision: 0.988095238095238"
[1] "TRAIN specifity: 1"
[1] "TRAIN sensitivity: 0.993756503642039"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        5        6
            positive        6      316
[1] "TEST accuracy: 0.963963963963964"
[1] "TEST +precision: 0.981366459627329"
[1] "TEST -precision: 0.454545454545455"
[1] "TEST specifity: 0.454545454545455"
[1] "TEST sensitivity: 0.981366459627329"

[1] "DATASET NAME: Tapeo_Bi_IR_2"
[1] "TRAIN INSTANCES: 1457"
[1] "TEST INSTANCES: 333"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 3.44312596321106"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

1457 samples
 622 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1166, 1165, 1166, 1166, 1165 
Resampling results:

  ROC        Sens       Spec     
  0.9988641  0.9879394  0.9895615

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     33.8      0.7
  positive      0.4     65.1
                           
 Accuracy (average) : 0.989

[1] "TRAIN accuracy: 0.989018531228552"
[1] "TRAIN +precision: 0.993717277486911"
[1] "TRAIN -precision: 0.9800796812749"
[1] "TRAIN specifity: 0.987951807228916"
[1] "TRAIN sensitivity: 0.989572471324296"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        2        4
            positive        7      320
[1] "TEST accuracy: 0.966966966966967"
[1] "TEST +precision: 0.978593272171254"
[1] "TEST -precision: 0.333333333333333"
[1] "TEST specifity: 0.222222222222222"
[1] "TEST sensitivity: 0.987654320987654"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 1.68790848255157"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

1457 samples
 622 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1165, 1167, 1166, 1165, 1165 
Resampling results across tuning parameters:

  C      M  ROC        Sens       Spec     
  0.010  1  0.8346942  0.6249293  0.9749727
  0.010  2  0.8352098  0.6249293  0.9739311
  0.010  3  0.8376813  0.6249293  0.9854058
  0.255  1  0.8404725  0.6309293  0.9864474
  0.255  2  0.8408408  0.6309293  0.9854058
  0.255  3  0.8435063  0.6309293  0.9906141
  0.500  1  0.8483611  0.6309293  0.9916558
  0.500  2  0.8495185  0.6309293  0.9937391
  0.500  3  0.8500058  0.6309293  0.9937391

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.5 and M = 3.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     21.6      0.4
  positive     12.6     65.4
                            
 Accuracy (average) : 0.8696

[1] "TRAIN accuracy: 0.869595058339053"
[1] "TRAIN +precision: 0.838170624450308"
[1] "TRAIN -precision: 0.98125"
[1] "TRAIN specifity: 0.630522088353414"
[1] "TRAIN sensitivity: 0.993743482794578"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        2        2
            positive        7      322
[1] "TEST accuracy: 0.972972972972973"
[1] "TEST +precision: 0.978723404255319"
[1] "TEST -precision: 0.5"
[1] "TEST specifity: 0.222222222222222"
[1] "TEST sensitivity: 0.993827160493827"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 4.69697301785151"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

1457 samples
 622 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1165, 1165, 1166, 1167, 1165 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens       Spec     
  0.3  1          0.6               0.50        50      0.9598991  0.7147677  0.9822753
  0.3  1          0.6               0.50       100      0.9841079  0.8774747  0.9728839
  0.3  1          0.6               0.50       150      0.9886268  0.9799192  0.9739256
  0.3  1          0.6               0.75        50      0.9567026  0.7049697  0.9812336
  0.3  1          0.6               0.75       100      0.9857577  0.8633939  0.9781032
  0.3  1          0.6               0.75       150      0.9872034  0.9519192  0.9760144
  0.3  1          0.6               1.00        50      0.9589612  0.6807273  0.9853949
  0.3  1          0.6               1.00       100      0.9861446  0.8351919  0.9801811
  0.3  1          0.6               1.00       150      0.9888679  0.9599596  0.9801811
  0.3  1          0.8               0.50        50      0.9594077  0.6948283  0.9822699
  0.3  1          0.8               0.50       100      0.9781637  0.8733131  0.9749673
  0.3  1          0.8               0.50       150      0.9862516  0.9556970  0.9780923
  0.3  1          0.8               0.75        50      0.9573212  0.7067677  0.9791449
  0.3  1          0.8               0.75       100      0.9834839  0.8693939  0.9749673
  0.3  1          0.8               0.75       150      0.9872455  0.9559596  0.9739256
  0.3  1          0.8               1.00        50      0.9578800  0.6946667  0.9833170
  0.3  1          0.8               1.00       100      0.9851035  0.8452323  0.9812336
  0.3  1          0.8               1.00       150      0.9891005  0.9599596  0.9812282
  0.3  2          0.6               0.50        50      0.9839871  0.8695354  0.9791503
  0.3  2          0.6               0.50       100      0.9897533  0.9738384  0.9780923
  0.3  2          0.6               0.50       150      0.9907675  0.9859596  0.9791449
  0.3  2          0.6               0.75        50      0.9866584  0.8694747  0.9812336
  0.3  2          0.6               0.75       100      0.9915349  0.9859596  0.9781032
  0.3  2          0.6               0.75       150      0.9924217  0.9859596  0.9833115
  0.3  2          0.6               1.00        50      0.9869182  0.8533131  0.9812282
  0.3  2          0.6               1.00       100      0.9914014  0.9859596  0.9812336
  0.3  2          0.6               1.00       150      0.9944115  0.9859596  0.9812336
  0.3  2          0.8               0.50        50      0.9856955  0.8715152  0.9801865
  0.3  2          0.8               0.50       100      0.9915540  0.9859596  0.9760089
  0.3  2          0.8               0.50       150      0.9915504  0.9859596  0.9801756
  0.3  2          0.8               0.75        50      0.9864197  0.8473131  0.9770561
  0.3  2          0.8               0.75       100      0.9891270  0.9859596  0.9812282
  0.3  2          0.8               0.75       150      0.9903876  0.9859596  0.9791394
  0.3  2          0.8               1.00        50      0.9886923  0.8533131  0.9822644
  0.3  2          0.8               1.00       100      0.9917505  0.9859596  0.9843586
  0.3  2          0.8               1.00       150      0.9941193  0.9859596  0.9822808
  0.3  3          0.6               0.50        50      0.9879310  0.9236768  0.9843477
  0.3  3          0.6               0.50       100      0.9909704  0.9819192  0.9780923
  0.3  3          0.6               0.50       150      0.9908650  0.9859596  0.9781032
  0.3  3          0.6               0.75        50      0.9889636  0.9559192  0.9791394
  0.3  3          0.6               0.75       100      0.9922646  0.9859596  0.9822753
  0.3  3          0.6               0.75       150      0.9951842  0.9859596  0.9843586
  0.3  3          0.6               1.00        50      0.9917524  0.9859596  0.9822753
  0.3  3          0.6               1.00       100      0.9943701  0.9859596  0.9843586
  0.3  3          0.6               1.00       150      0.9962354  0.9859596  0.9822699
  0.3  3          0.8               0.50        50      0.9888261  0.9678182  0.9781032
  0.3  3          0.8               0.50       100      0.9907026  0.9859596  0.9801865
  0.3  3          0.8               0.50       150      0.9910073  0.9859596  0.9770452
  0.3  3          0.8               0.75        50      0.9916633  0.9859596  0.9801811
  0.3  3          0.8               0.75       100      0.9941504  0.9859596  0.9801920
  0.3  3          0.8               0.75       150      0.9951399  0.9859596  0.9791503
  0.3  3          0.8               1.00        50      0.9909200  0.9599596  0.9843586
  0.3  3          0.8               1.00       100      0.9945257  0.9859596  0.9854003
  0.3  3          0.8               1.00       150      0.9961315  0.9859596  0.9833170
  0.4  1          0.6               0.50        50      0.9706190  0.7570909  0.9781032
  0.4  1          0.6               0.50       100      0.9871865  0.9135758  0.9801811
  0.4  1          0.6               0.50       150      0.9878275  0.9778788  0.9770506
  0.4  1          0.6               0.75        50      0.9804639  0.7368485  0.9770615
  0.4  1          0.6               0.75       100      0.9874916  0.9617374  0.9739256
  0.4  1          0.6               0.75       150      0.9905252  0.9859596  0.9739256
  0.4  1          0.6               1.00        50      0.9805099  0.7509293  0.9791503
  0.4  1          0.6               1.00       100      0.9876326  0.9599596  0.9812282
  0.4  1          0.6               1.00       150      0.9903894  0.9599596  0.9812282
  0.4  1          0.8               0.50        50      0.9732791  0.8111919  0.9760144
  0.4  1          0.8               0.50       100      0.9860314  0.9176768  0.9760089
  0.4  1          0.8               0.50       150      0.9881896  0.9859596  0.9780977
  0.4  1          0.8               0.75        50      0.9813130  0.7448889  0.9812391
  0.4  1          0.8               0.75       100      0.9882083  0.9617980  0.9781032
  0.4  1          0.8               0.75       150      0.9907891  0.9859596  0.9791394
  0.4  1          0.8               1.00        50      0.9715997  0.7609899  0.9791503
  0.4  1          0.8               1.00       100      0.9880915  0.9599596  0.9822699
  0.4  1          0.8               1.00       150      0.9906298  0.9599596  0.9833170
  0.4  2          0.6               0.50        50      0.9882124  0.9016768  0.9833115
  0.4  2          0.6               0.50       100      0.9911395  0.9799596  0.9833115
  0.4  2          0.6               0.50       150      0.9897888  0.9799596  0.9812227
  0.4  2          0.6               0.75        50      0.9891306  0.9455960  0.9822753
  0.4  2          0.6               0.75       100      0.9917464  0.9859596  0.9833170
  0.4  2          0.6               0.75       150      0.9896992  0.9859596  0.9812391
  0.4  2          0.6               1.00        50      0.9903365  0.9599596  0.9833115
  0.4  2          0.6               1.00       100      0.9928152  0.9859596  0.9833115
  0.4  2          0.6               1.00       150      0.9943697  0.9859596  0.9864420
  0.4  2          0.8               0.50        50      0.9844414  0.9257374  0.9760199
  0.4  2          0.8               0.50       100      0.9881504  0.9778788  0.9791449
  0.4  2          0.8               0.50       150      0.9887999  0.9778788  0.9822753
  0.4  2          0.8               0.75        50      0.9872888  0.9357778  0.9749673
  0.4  2          0.8               0.75       100      0.9922388  0.9859596  0.9791339
  0.4  2          0.8               0.75       150      0.9940590  0.9859596  0.9801865
  0.4  2          0.8               1.00        50      0.9894564  0.9599596  0.9801865
  0.4  2          0.8               1.00       100      0.9933836  0.9859596  0.9843641
  0.4  2          0.8               1.00       150      0.9946096  0.9859596  0.9833224
  0.4  3          0.6               0.50        50      0.9869991  0.9678182  0.9801811
  0.4  3          0.6               0.50       100      0.9914469  0.9779192  0.9791394
  0.4  3          0.6               0.50       150      0.9901125  0.9779192  0.9760089
  0.4  3          0.6               0.75        50      0.9888247  0.9859596  0.9791449
  0.4  3          0.6               0.75       100      0.9923241  0.9859596  0.9791449
  0.4  3          0.6               0.75       150      0.9925435  0.9859596  0.9760089
  0.4  3          0.6               1.00        50      0.9925331  0.9859596  0.9822699
  0.4  3          0.6               1.00       100      0.9951304  0.9859596  0.9833170
  0.4  3          0.6               1.00       150      0.9956950  0.9859596  0.9822699
  0.4  3          0.8               0.50        50      0.9904452  0.9758586  0.9780923
  0.4  3          0.8               0.50       100      0.9906046  0.9819192  0.9822699
  0.4  3          0.8               0.50       150      0.9906566  0.9819192  0.9780977
  0.4  3          0.8               0.75        50      0.9907297  0.9859596  0.9770615
  0.4  3          0.8               0.75       100      0.9945949  0.9859596  0.9843641
  0.4  3          0.8               0.75       150      0.9937821  0.9859596  0.9801865
  0.4  3          0.8               1.00        50      0.9921416  0.9859596  0.9833170
  0.4  3          0.8               1.00       100      0.9948191  0.9859596  0.9833224
  0.4  3          0.8               1.00       150      0.9949767  0.9859596  0.9822699

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 150, max_depth = 3, eta = 0.3, gamma = 0, colsample_bytree = 0.6, min_child_weight = 1 and subsample = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     33.7      1.2
  positive      0.5     64.7
                            
 Accuracy (average) : 0.9835

[1] "TRAIN accuracy: 0.983527796842828"
[1] "TRAIN +precision: 0.992623814541623"
[1] "TRAIN -precision: 0.966535433070866"
[1] "TRAIN specifity: 0.985943775100402"
[1] "TRAIN sensitivity: 0.982273201251303"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        2        4
            positive        7      320
[1] "TEST accuracy: 0.966966966966967"
[1] "TEST +precision: 0.978593272171254"
[1] "TEST -precision: 0.333333333333333"
[1] "TEST specifity: 0.222222222222222"
[1] "TEST sensitivity: 0.987654320987654"

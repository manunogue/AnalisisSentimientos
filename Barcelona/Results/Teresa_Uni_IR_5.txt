[1] "DATASET NAME: Teresa_Uni_IR_5"
[1] "TRAIN INSTANCES: 1344"
[1] "TEST INSTANCES: 379"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 4.53047204017639"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

1344 samples
 698 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1075, 1075, 1075, 1075, 1076 
Resampling results:

  ROC        Sens       Spec     
  0.9945566  0.9764706  0.9963176

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     18.8      0.3
  positive      0.4     80.5
                            
 Accuracy (average) : 0.9926

[1] "TRAIN accuracy: 0.992559523809524"
[1] "TRAIN +precision: 0.994485294117647"
[1] "TRAIN -precision: 0.984375"
[1] "TRAIN specifity: 0.976744186046512"
[1] "TRAIN sensitivity: 0.996316758747698"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       10        2
            positive        8      359
[1] "TEST accuracy: 0.973614775725594"
[1] "TEST +precision: 0.978201634877384"
[1] "TEST -precision: 0.833333333333333"
[1] "TEST specifity: 0.555555555555556"
[1] "TEST sensitivity: 0.994459833795014"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 1.95684408346812"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

1344 samples
 698 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1075, 1076, 1076, 1074, 1075 
Resampling results across tuning parameters:

  C      M  ROC        Sens       Spec     
  0.010  1  0.9130810  0.8291855  0.9705492
  0.010  2  0.9122474  0.8175716  0.9705492
  0.010  3  0.9005032  0.7978130  0.9696275
  0.255  1  0.9629007  0.9417044  0.9650235
  0.255  2  0.9624857  0.9222474  0.9677884
  0.255  3  0.9657405  0.9220211  0.9677927
  0.500  1  0.9680066  0.9533183  0.9641018
  0.500  2  0.9675465  0.9338612  0.9668668
  0.500  3  0.9657405  0.9220211  0.9677927

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.5 and M = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     18.3      2.9
  positive      0.9     77.9
                            
 Accuracy (average) : 0.9621

[1] "TRAIN accuracy: 0.962053571428571"
[1] "TRAIN +precision: 0.988668555240793"
[1] "TRAIN -precision: 0.863157894736842"
[1] "TRAIN specifity: 0.953488372093023"
[1] "TRAIN sensitivity: 0.964088397790055"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       11        7
            positive        7      354
[1] "TEST accuracy: 0.963060686015831"
[1] "TEST +precision: 0.980609418282548"
[1] "TEST -precision: 0.611111111111111"
[1] "TEST specifity: 0.611111111111111"
[1] "TEST sensitivity: 0.980609418282548"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 5.10752686659495"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

1344 samples
 698 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1075, 1074, 1075, 1076, 1076 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens       Spec     
  0.3  1          0.6               0.50        50      0.9838052  0.7210407  0.9926267
  0.3  1          0.6               0.50       100      0.9919237  0.8763198  0.9898660
  0.3  1          0.6               0.50       150      0.9955228  0.9187783  0.9880227
  0.3  1          0.6               0.75        50      0.9840531  0.7018854  0.9917051
  0.3  1          0.6               0.75       100      0.9941753  0.8570136  0.9917093
  0.3  1          0.6               0.75       150      0.9960207  0.9460030  0.9898660
  0.3  1          0.6               1.00        50      0.9857439  0.7017345  0.9935484
  0.3  1          0.6               1.00       100      0.9934857  0.8570890  0.9907834
  0.3  1          0.6               1.00       150      0.9957797  0.9343891  0.9907834
  0.3  1          0.8               0.50        50      0.9848432  0.7366516  0.9926267
  0.3  1          0.8               0.50       100      0.9930621  0.8841629  0.9935484
  0.3  1          0.8               0.50       150      0.9959909  0.9536953  0.9907876
  0.3  1          0.8               0.75        50      0.9859998  0.6898944  0.9889401
  0.3  1          0.8               0.75       100      0.9931630  0.8530920  0.9898618
  0.3  1          0.8               0.75       150      0.9960801  0.9382353  0.9880227
  0.3  1          0.8               1.00        50      0.9852819  0.7017345  0.9926267
  0.3  1          0.8               1.00       100      0.9930865  0.8377828  0.9917051
  0.3  1          0.8               1.00       150      0.9955117  0.9266968  0.9907876
  0.3  2          0.6               0.50        50      0.9950454  0.8803167  0.9926310
  0.3  2          0.6               0.50       100      0.9986673  0.9767722  0.9898660
  0.3  2          0.6               0.50       150      0.9988970  0.9923077  0.9898660
  0.3  2          0.6               0.75        50      0.9954594  0.9074661  0.9907876
  0.3  2          0.6               0.75       100      0.9983962  0.9768477  0.9935526
  0.3  2          0.6               0.75       150      0.9986107  0.9923077  0.9917135
  0.3  2          0.6               1.00        50      0.9955510  0.9227753  0.9935526
  0.3  2          0.6               1.00       100      0.9990598  0.9730015  0.9926310
  0.3  2          0.6               1.00       150      0.9992524  0.9923077  0.9917093
  0.3  2          0.8               0.50        50      0.9950432  0.9380845  0.9907876
  0.3  2          0.8               0.50       100      0.9979455  0.9846154  0.9889443
  0.3  2          0.8               0.50       150      0.9987053  0.9923077  0.9907876
  0.3  2          0.8               0.75        50      0.9959063  0.9114630  0.9907876
  0.3  2          0.8               0.75       100      0.9992546  0.9730015  0.9926310
  0.3  2          0.8               0.75       150      0.9994311  0.9923077  0.9907876
  0.3  2          0.8               1.00        50      0.9966045  0.9266968  0.9953959
  0.3  2          0.8               1.00       100      0.9991295  0.9768477  0.9917093
  0.3  2          0.8               1.00       150      0.9992875  1.0000000  0.9917135
  0.3  3          0.6               0.50        50      0.9979907  0.9499246  0.9907919
  0.3  3          0.6               0.50       100      0.9985421  0.9923077  0.9889528
  0.3  3          0.6               0.50       150      0.9987946  0.9923077  0.9871095
  0.3  3          0.6               0.75        50      0.9989185  0.9730015  0.9944743
  0.3  3          0.6               0.75       100      0.9994503  0.9923077  0.9926310
  0.3  3          0.6               0.75       150      0.9995927  0.9923077  0.9907919
  0.3  3          0.6               1.00        50      0.9991314  0.9768477  0.9935526
  0.3  3          0.6               1.00       100      0.9995926  1.0000000  0.9953959
  0.3  3          0.6               1.00       150      0.9993799  1.0000000  0.9926310
  0.3  3          0.8               0.50        50      0.9986174  0.9730015  0.9935526
  0.3  3          0.8               0.50       100      0.9987064  0.9923077  0.9907961
  0.3  3          0.8               0.50       150      0.9987413  0.9923077  0.9898787
  0.3  3          0.8               0.75        50      0.9993626  0.9768477  0.9953917
  0.3  3          0.8               0.75       100      0.9992736  0.9923077  0.9935526
  0.3  3          0.8               0.75       150      0.9993445  0.9923077  0.9926352
  0.3  3          0.8               1.00        50      0.9989004  0.9730015  0.9926310
  0.3  3          0.8               1.00       100      0.9994504  1.0000000  0.9935526
  0.3  3          0.8               1.00       150      0.9993440  1.0000000  0.9935526
  0.4  1          0.6               0.50        50      0.9872631  0.7602564  0.9907834
  0.4  1          0.6               0.50       100      0.9938420  0.9033937  0.9880184
  0.4  1          0.6               0.50       150      0.9968296  0.9651584  0.9852577
  0.4  1          0.6               0.75        50      0.9897013  0.7871041  0.9898660
  0.4  1          0.6               0.75       100      0.9951968  0.9344646  0.9880269
  0.4  1          0.6               0.75       150      0.9970917  0.9650830  0.9871052
  0.4  1          0.6               1.00        50      0.9901188  0.7560332  0.9926267
  0.4  1          0.6               1.00       100      0.9953514  0.8955505  0.9917051
  0.4  1          0.6               1.00       150      0.9972059  0.9535445  0.9907834
  0.4  1          0.8               0.50        50      0.9886452  0.7834087  0.9861793
  0.4  1          0.8               0.50       100      0.9938018  0.9189291  0.9889443
  0.4  1          0.8               0.50       150      0.9964176  0.9458522  0.9852619
  0.4  1          0.8               0.75        50      0.9891314  0.7796380  0.9898618
  0.4  1          0.8               0.75       100      0.9949704  0.9191554  0.9880184
  0.4  1          0.8               0.75       150      0.9968324  0.9730015  0.9871010
  0.4  1          0.8               1.00        50      0.9895406  0.7678733  0.9917051
  0.4  1          0.8               1.00       100      0.9949520  0.9073152  0.9907834
  0.4  1          0.8               1.00       150      0.9970497  0.9653092  0.9907834
  0.4  2          0.6               0.50        50      0.9970369  0.9305430  0.9926310
  0.4  2          0.6               0.50       100      0.9984342  0.9884615  0.9917135
  0.4  2          0.6               0.50       150      0.9985787  0.9923077  0.9852661
  0.4  2          0.6               0.75        50      0.9974687  0.9497738  0.9898660
  0.4  2          0.6               0.75       100      0.9988061  1.0000000  0.9935526
  0.4  2          0.6               0.75       150      0.9988428  1.0000000  0.9907919
  0.4  2          0.6               1.00        50      0.9971813  0.9653092  0.9889443
  0.4  2          0.6               1.00       100      0.9992889  0.9884615  0.9926310
  0.4  2          0.6               1.00       150      0.9995369  1.0000000  0.9898702
  0.4  2          0.8               0.50        50      0.9969717  0.9537707  0.9926352
  0.4  2          0.8               0.50       100      0.9985418  0.9923077  0.9861836
  0.4  2          0.8               0.50       150      0.9987229  0.9923077  0.9871052
  0.4  2          0.8               0.75        50      0.9978008  0.9653092  0.9926310
  0.4  2          0.8               0.75       100      0.9992548  0.9923077  0.9907876
  0.4  2          0.8               0.75       150      0.9990594  0.9923077  0.9907919
  0.4  2          0.8               1.00        50      0.9983315  0.9653092  0.9935484
  0.4  2          0.8               1.00       100      0.9993960  0.9923077  0.9944700
  0.4  2          0.8               1.00       150      0.9993602  1.0000000  0.9917093
  0.4  3          0.6               0.50        50      0.9992027  0.9923077  0.9907876
  0.4  3          0.6               0.50       100      0.9991682  0.9923077  0.9880269
  0.4  3          0.6               0.50       150      0.9990809  0.9923077  0.9898744
  0.4  3          0.6               0.75        50      0.9994331  0.9730769  0.9944743
  0.4  3          0.6               0.75       100      0.9995041  1.0000000  0.9898702
  0.4  3          0.6               0.75       150      0.9992555  1.0000000  0.9926352
  0.4  3          0.6               1.00        50      0.9992382  0.9923077  0.9944743
  0.4  3          0.6               1.00       100      0.9992737  1.0000000  0.9898744
  0.4  3          0.6               1.00       150      0.9992558  1.0000000  0.9898744
  0.4  3          0.8               0.50        50      0.9990417  0.9768477  0.9926310
  0.4  3          0.8               0.50       100      0.9993948  0.9923077  0.9880227
  0.4  3          0.8               0.50       150      0.9994679  0.9923077  0.9907876
  0.4  3          0.8               0.75        50      0.9995217  0.9923077  0.9917093
  0.4  3          0.8               0.75       100      0.9994828  1.0000000  0.9898702
  0.4  3          0.8               0.75       150      0.9993396  1.0000000  0.9917135
  0.4  3          0.8               1.00        50      0.9995393  0.9923077  0.9935526
  0.4  3          0.8               1.00       100      0.9995395  1.0000000  0.9926352
  0.4  3          0.8               1.00       150      0.9992721  1.0000000  0.9926352

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 150, max_depth = 3, eta = 0.3, gamma = 0, colsample_bytree = 0.6, min_child_weight = 1 and subsample = 0.75.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     19.0      0.7
  positive      0.1     80.1
                            
 Accuracy (average) : 0.9911

[1] "TRAIN accuracy: 0.991071428571429"
[1] "TRAIN +precision: 0.998144712430427"
[1] "TRAIN -precision: 0.962406015037594"
[1] "TRAIN specifity: 0.992248062015504"
[1] "TRAIN sensitivity: 0.990791896869245"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       12        2
            positive        6      359
[1] "TEST accuracy: 0.978891820580475"
[1] "TEST +precision: 0.983561643835616"
[1] "TEST -precision: 0.857142857142857"
[1] "TEST specifity: 0.666666666666667"
[1] "TEST sensitivity: 0.994459833795014"

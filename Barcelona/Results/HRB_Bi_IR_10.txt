[1] "DATASET NAME: HRB_Bi_IR_10"
[1] "TRAIN INSTANCES: 1065"
[1] "TEST INSTANCES: 332"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 3.76675605773926"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

1065 samples
 825 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 852, 851, 852, 853, 852 
Resampling results:

  ROC        Sens       Spec     
  0.9282732  0.6739958  0.9704838

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     13.8      2.3
  positive      6.7     77.2
                            
 Accuracy (average) : 0.9099

[1] "TRAIN accuracy: 0.909859154929577"
[1] "TRAIN +precision: 0.920492721164614"
[1] "TRAIN -precision: 0.854651162790698"
[1] "TRAIN specifity: 0.674311926605505"
[1] "TRAIN sensitivity: 0.970484061393152"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       19        8
            positive       27      278
[1] "TEST accuracy: 0.894578313253012"
[1] "TEST +precision: 0.911475409836066"
[1] "TEST -precision: 0.703703703703704"
[1] "TEST specifity: 0.41304347826087"
[1] "TEST sensitivity: 0.972027972027972"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 1.99857073227564"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

1065 samples
 825 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 852, 852, 852, 852, 852 
Resampling results across tuning parameters:

  C      M  ROC        Sens         Spec     
  0.010  1  0.5069661  0.013636364  0.9905325
  0.010  2  0.5046530  0.009090909  0.9905325
  0.010  3  0.4999597  0.004545455  0.9952663
  0.255  1  0.6625968  0.257399577  0.9704977
  0.255  2  0.6633392  0.165750529  0.9740620
  0.255  3  0.6211981  0.092283298  0.9811277
  0.500  1  0.6995474  0.308033827  0.9657779
  0.500  2  0.6773440  0.229809725  0.9646084
  0.500  3  0.6896649  0.165750529  0.9717021

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.5 and M = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative      6.3      2.7
  positive     14.2     76.8
                           
 Accuracy (average) : 0.831

[1] "TRAIN accuracy: 0.830985915492958"
[1] "TRAIN +precision: 0.844169246646027"
[1] "TRAIN -precision: 0.697916666666667"
[1] "TRAIN specifity: 0.307339449541284"
[1] "TRAIN sensitivity: 0.965761511216057"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        1        9
            positive       45      277
[1] "TEST accuracy: 0.837349397590361"
[1] "TEST +precision: 0.860248447204969"
[1] "TEST -precision: 0.1"
[1] "TEST specifity: 0.0217391304347826"
[1] "TEST sensitivity: 0.968531468531469"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 4.61727954943975"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

1065 samples
 825 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 853, 852, 851, 852, 852 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens        Spec     
  0.3  1          0.6               0.50        50      0.7492566  0.11945032  0.9941107
  0.3  1          0.6               0.50       100      0.7851425  0.17452431  0.9882074
  0.3  1          0.6               0.50       150      0.8102782  0.21120507  0.9822973
  0.3  1          0.6               0.75        50      0.7608496  0.11004228  0.9929342
  0.3  1          0.6               0.75       100      0.8232845  0.19735729  0.9917577
  0.3  1          0.6               0.75       150      0.8341782  0.25708245  0.9893909
  0.3  1          0.6               1.00        50      0.7817105  0.09164905  0.9964706
  0.3  1          0.6               1.00       100      0.8378555  0.16057082  0.9941037
  0.3  1          0.6               1.00       150      0.8641082  0.25243129  0.9929203
  0.3  1          0.8               0.50        50      0.7479842  0.11014799  0.9917508
  0.3  1          0.8               0.50       100      0.7968369  0.18826638  0.9858475
  0.3  1          0.8               0.50       150      0.8100361  0.24334038  0.9822973
  0.3  1          0.8               0.75        50      0.7729645  0.09630021  0.9964706
  0.3  1          0.8               0.75       100      0.8154579  0.17441860  0.9929342
  0.3  1          0.8               0.75       150      0.8450524  0.22949260  0.9882005
  0.3  1          0.8               1.00        50      0.7736152  0.10095137  0.9952872
  0.3  1          0.8               1.00       100      0.8374544  0.16987315  0.9941037
  0.3  1          0.8               1.00       150      0.8604035  0.25232558  0.9929203
  0.3  2          0.6               0.50        50      0.8064409  0.15602537  0.9893839
  0.3  2          0.6               0.50       100      0.8330456  0.23403805  0.9810999
  0.3  2          0.6               0.50       150      0.8346885  0.27970402  0.9752036
  0.3  2          0.6               0.75        50      0.8062603  0.16966173  0.9929133
  0.3  2          0.6               0.75       100      0.8565154  0.26131078  0.9834807
  0.3  2          0.6               0.75       150      0.8674588  0.33012685  0.9775705
  0.3  2          0.6               1.00        50      0.8359706  0.18826638  0.9940968
  0.3  2          0.6               1.00       100      0.8750847  0.29841438  0.9905534
  0.3  2          0.6               1.00       150      0.8949535  0.36257928  0.9858267
  0.3  2          0.8               0.50        50      0.7899602  0.18340381  0.9858475
  0.3  2          0.8               0.50       100      0.8283358  0.24312896  0.9763871
  0.3  2          0.8               0.50       150      0.8383059  0.27980973  0.9716603
  0.3  2          0.8               0.75        50      0.8187734  0.17441860  0.9917369
  0.3  2          0.8               0.75       100      0.8603973  0.27970402  0.9858336
  0.3  2          0.8               0.75       150      0.8733185  0.35813953  0.9799234
  0.3  2          0.8               1.00        50      0.8427733  0.18340381  0.9952802
  0.3  2          0.8               1.00       100      0.8784803  0.28932347  0.9917438
  0.3  2          0.8               1.00       150      0.8969297  0.36691332  0.9846502
  0.3  3          0.6               0.50        50      0.8080134  0.21120507  0.9846572
  0.3  3          0.6               0.50       100      0.8355601  0.26606765  0.9799304
  0.3  3          0.6               0.50       150      0.8483262  0.30750529  0.9728437
  0.3  3          0.6               0.75        50      0.8510008  0.22050740  0.9905604
  0.3  3          0.6               0.75       100      0.8744866  0.33953488  0.9775774
  0.3  3          0.6               0.75       150      0.8757529  0.34862579  0.9669405
  0.3  3          0.6               1.00        50      0.8601781  0.26162791  0.9929203
  0.3  3          0.6               1.00       100      0.8928914  0.37156448  0.9858267
  0.3  3          0.6               1.00       150      0.8966038  0.40380550  0.9822833
  0.3  3          0.8               0.50        50      0.8129325  0.23858351  0.9846641
  0.3  3          0.8               0.50       100      0.8501127  0.29344609  0.9740271
  0.3  3          0.8               0.50       150      0.8516633  0.31162791  0.9763871
  0.3  3          0.8               0.75        50      0.8460311  0.22959831  0.9929273
  0.3  3          0.8               0.75       100      0.8719139  0.35792812  0.9811138
  0.3  3          0.8               0.75       150      0.8737545  0.37610994  0.9787470
  0.3  3          0.8               1.00        50      0.8637386  0.26617336  0.9952872
  0.3  3          0.8               1.00       100      0.8959094  0.35771670  0.9870171
  0.3  3          0.8               1.00       150      0.8975127  0.40338266  0.9834737
  0.4  1          0.6               0.50        50      0.7610622  0.13763214  0.9893978
  0.4  1          0.6               0.50       100      0.8006833  0.19746300  0.9775983
  0.4  1          0.6               0.50       150      0.8188419  0.24799154  0.9693213
  0.4  1          0.6               0.75        50      0.7807815  0.13319239  0.9941176
  0.4  1          0.6               0.75       100      0.8278915  0.23847780  0.9858475
  0.4  1          0.6               0.75       150      0.8590182  0.29386892  0.9846432
  0.4  1          0.6               1.00        50      0.7896044  0.13763214  0.9941037
  0.4  1          0.6               1.00       100      0.8506236  0.24334038  0.9941037
  0.4  1          0.6               1.00       150      0.8705600  0.28456660  0.9917438
  0.4  1          0.8               0.50        50      0.7537150  0.14725159  0.9905604
  0.4  1          0.8               0.50       100      0.7957350  0.22484144  0.9740341
  0.4  1          0.8               0.50       150      0.8173027  0.25232558  0.9752036
  0.4  1          0.8               0.75        50      0.7934908  0.13287526  0.9941107
  0.4  1          0.8               0.75       100      0.8356829  0.22008457  0.9882144
  0.4  1          0.8               0.75       150      0.8557373  0.28900634  0.9834807
  0.4  1          0.8               1.00        50      0.7934454  0.14196617  0.9952802
  0.4  1          0.8               1.00       100      0.8449356  0.23858351  0.9941037
  0.4  1          0.8               1.00       150      0.8702419  0.28456660  0.9929203
  0.4  2          0.6               0.50        50      0.7865975  0.23424947  0.9787330
  0.4  2          0.6               0.50       100      0.8175932  0.27970402  0.9693073
  0.4  2          0.6               0.50       150      0.8307203  0.28435518  0.9657710
  0.4  2          0.6               0.75        50      0.8352031  0.20634249  0.9882005
  0.4  2          0.6               0.75       100      0.8755465  0.33012685  0.9834737
  0.4  2          0.6               0.75       150      0.8789907  0.37145877  0.9787470
  0.4  2          0.6               1.00        50      0.8546936  0.23403805  0.9917369
  0.4  2          0.6               1.00       100      0.8909713  0.32145877  0.9917369
  0.4  2          0.6               1.00       150      0.9014025  0.38076110  0.9846432
  0.4  2          0.8               0.50        50      0.7946586  0.22494715  0.9787400
  0.4  2          0.8               0.50       100      0.8344652  0.27071882  0.9822903
  0.4  2          0.8               0.50       150      0.8371382  0.32558140  0.9681239
  0.4  2          0.8               0.75        50      0.8256721  0.23424947  0.9882074
  0.4  2          0.8               0.75       100      0.8627546  0.27526427  0.9811069
  0.4  2          0.8               0.75       150      0.8737888  0.37156448  0.9716742
  0.4  2          0.8               1.00        50      0.8564831  0.26649049  0.9941037
  0.4  2          0.8               1.00       100      0.8931175  0.34429175  0.9893700
  0.4  2          0.8               1.00       150      0.8989749  0.38097252  0.9811069
  0.4  3          0.6               0.50        50      0.8135451  0.25243129  0.9728507
  0.4  3          0.6               0.50       100      0.8309905  0.31183932  0.9645667
  0.4  3          0.6               0.50       150      0.8399189  0.33921776  0.9610164
  0.4  3          0.6               0.75        50      0.8520448  0.31205074  0.9799304
  0.4  3          0.6               0.75       100      0.8739902  0.34386892  0.9752245
  0.4  3          0.6               0.75       150      0.8730255  0.38530655  0.9716742
  0.4  3          0.6               1.00        50      0.8702048  0.29376321  0.9929273
  0.4  3          0.6               1.00       100      0.8934788  0.39936575  0.9834737
  0.4  3          0.6               1.00       150      0.8930215  0.41310782  0.9811138
  0.4  3          0.8               0.50        50      0.8281210  0.25243129  0.9751827
  0.4  3          0.8               0.50       100      0.8490491  0.32558140  0.9669196
  0.4  3          0.8               0.50       150      0.8488232  0.33488372  0.9586773
  0.4  3          0.8               0.75        50      0.8584743  0.27061311  0.9799304
  0.4  3          0.8               0.75       100      0.8722349  0.37632135  0.9740411
  0.4  3          0.8               0.75       150      0.8780525  0.40782241  0.9704908
  0.4  3          0.8               1.00        50      0.8731824  0.28953488  0.9893700
  0.4  3          0.8               1.00       100      0.8962970  0.37610994  0.9811069
  0.4  3          0.8               1.00       150      0.8925783  0.42653277  0.9752106

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 150, max_depth = 2, eta = 0.4, gamma = 0, colsample_bytree = 0.6, min_child_weight = 1 and subsample = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative      7.8      1.2
  positive     12.7     78.3
                           
 Accuracy (average) : 0.861

[1] "TRAIN accuracy: 0.861032863849765"
[1] "TRAIN +precision: 0.860681114551084"
[1] "TRAIN -precision: 0.864583333333333"
[1] "TRAIN specifity: 0.380733944954128"
[1] "TRAIN sensitivity: 0.984651711924439"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        9        7
            positive       37      279
[1] "TEST accuracy: 0.867469879518072"
[1] "TEST +precision: 0.882911392405063"
[1] "TEST -precision: 0.5625"
[1] "TEST specifity: 0.195652173913043"
[1] "TEST sensitivity: 0.975524475524476"

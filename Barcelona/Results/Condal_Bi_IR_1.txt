[1] "DATASET NAME: Condal_Bi_IR_1"
[1] "TRAIN INSTANCES: 4556"
[1] "TEST INSTANCES: 795"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 12.3295569419861"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

4556 samples
 720 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 3645, 3644, 3645, 3645, 3645 
Resampling results:

  ROC        Sens       Spec     
  0.9945985  0.9819983  0.9798034

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     49.1      1.0
  positive      0.9     49.0
                            
 Accuracy (average) : 0.9809

[1] "TRAIN accuracy: 0.980904302019315"
[1] "TRAIN +precision: 0.981962164540255"
[1] "TRAIN -precision: 0.979851073149365"
[1] "TRAIN specifity: 0.982001755926251"
[1] "TRAIN sensitivity: 0.979806848112379"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       17       10
            positive       29      739
[1] "TEST accuracy: 0.950943396226415"
[1] "TEST +precision: 0.962239583333333"
[1] "TEST -precision: 0.62962962962963"
[1] "TEST specifity: 0.369565217391304"
[1] "TEST sensitivity: 0.986648865153538"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 11.2191653331121"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

4556 samples
 720 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 3646, 3645, 3644, 3645, 3644 
Resampling results across tuning parameters:

  C      M  ROC        Sens       Spec     
  0.010  1  0.9723125  0.9183623  0.9596163
  0.010  2  0.9726965  0.9183623  0.9596163
  0.010  3  0.9725596  0.9166079  0.9635647
  0.255  1  0.9754199  0.9209938  0.9644486
  0.255  2  0.9763420  0.9209938  0.9644486
  0.255  3  0.9784124  0.9209938  0.9723463
  0.500  1  0.9777158  0.9209938  0.9741064
  0.500  2  0.9800408  0.9209938  0.9741064
  0.500  3  0.9814152  0.9209938  0.9749788

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.5 and M = 3.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     46.0      1.3
  positive      4.0     48.7
                           
 Accuracy (average) : 0.948

[1] "TRAIN accuracy: 0.947980684811238"
[1] "TRAIN +precision: 0.92503123698459"
[1] "TRAIN -precision: 0.973549883990719"
[1] "TRAIN specifity: 0.920983318700615"
[1] "TRAIN sensitivity: 0.974978050921861"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       12       19
            positive       34      730
[1] "TEST accuracy: 0.933333333333333"
[1] "TEST +precision: 0.955497382198953"
[1] "TEST -precision: 0.387096774193548"
[1] "TEST specifity: 0.260869565217391"
[1] "TEST sensitivity: 0.974632843791722"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 16.6232295036316"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

4556 samples
 720 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 3644, 3644, 3645, 3645, 3646 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens       Spec     
  0.3  1          0.6               0.50        50      0.8980369  0.9657663  0.5961153
  0.3  1          0.6               0.50       100      0.9513081  0.9086823  0.7922585
  0.3  1          0.6               0.50       150      0.9709209  0.8906960  0.9354588
  0.3  1          0.6               0.75        50      0.8979999  0.9762946  0.5785782
  0.3  1          0.6               0.75       100      0.9514673  0.8270166  0.8972026
  0.3  1          0.6               0.75       150      0.9700071  0.8858647  0.9341382
  0.3  1          0.6               1.00        50      0.8962091  0.9758531  0.5623299
  0.3  1          0.6               1.00       100      0.9502530  0.7932331  0.9578610
  0.3  1          0.6               1.00       150      0.9684798  0.8669954  0.9477598
  0.3  1          0.8               0.50        50      0.8907941  0.9688346  0.5781309
  0.3  1          0.8               0.50       100      0.9491230  0.8613110  0.8458897
  0.3  1          0.8               0.50       150      0.9708221  0.8871747  0.9306430
  0.3  1          0.8               0.75        50      0.9007161  0.9780451  0.5772460
  0.3  1          0.8               0.75       100      0.9532072  0.8450868  0.9007230
  0.3  1          0.8               0.75       150      0.9704583  0.8841074  0.9358994
  0.3  1          0.8               1.00        50      0.8990146  0.9820002  0.5671515
  0.3  1          0.8               1.00       100      0.9504403  0.7901591  0.9587382
  0.3  1          0.8               1.00       150      0.9681821  0.8656748  0.9473193
  0.3  2          0.6               0.50        50      0.9575080  0.8770195  0.8577328
  0.3  2          0.6               0.50       100      0.9845895  0.9468730  0.9394120
  0.3  2          0.6               0.50       150      0.9910236  0.9670763  0.9517072
  0.3  2          0.6               0.75        50      0.9571158  0.8727232  0.8453287
  0.3  2          0.6               0.75       100      0.9852792  0.9407297  0.9477540
  0.3  2          0.6               0.75       150      0.9905795  0.9697079  0.9521457
  0.3  2          0.6               1.00        50      0.9554421  0.9152641  0.7839156
  0.3  2          0.6               1.00       100      0.9848675  0.9271294  0.9556613
  0.3  2          0.6               1.00       150      0.9919113  0.9574079  0.9622441
  0.3  2          0.8               0.50        50      0.9554526  0.9548111  0.7546559
  0.3  2          0.8               0.50       100      0.9858035  0.9534461  0.9376605
  0.3  2          0.8               0.50       150      0.9906132  0.9732186  0.9460044
  0.3  2          0.8               0.75        50      0.9590290  0.9526181  0.7470908
  0.3  2          0.8               0.75       100      0.9860313  0.9398516  0.9512695
  0.3  2          0.8               0.75       150      0.9915421  0.9696983  0.9517014
  0.3  2          0.8               1.00        50      0.9572503  0.9507808  0.7558724
  0.3  2          0.8               1.00       100      0.9863403  0.9288905  0.9574147
  0.3  2          0.8               1.00       150      0.9919536  0.9644361  0.9613621
  0.3  3          0.6               0.50        50      0.9786407  0.9069279  0.9477559
  0.3  3          0.6               0.50       100      0.9921169  0.9705813  0.9530287
  0.3  3          0.6               0.50       150      0.9945101  0.9811201  0.9582919
  0.3  3          0.6               0.75        50      0.9818918  0.8977174  0.9578494
  0.3  3          0.6               0.75       100      0.9930167  0.9701456  0.9596115
  0.3  3          0.6               0.75       150      0.9949794  0.9819973  0.9688307
  0.3  3          0.6               1.00        50      0.9799253  0.9082447  0.9226865
  0.3  3          0.6               1.00       100      0.9933028  0.9670657  0.9622402
  0.3  3          0.6               1.00       150      0.9955330  0.9793657  0.9692664
  0.3  3          0.8               0.50        50      0.9818025  0.9284403  0.9350260
  0.3  3          0.8               0.50       100      0.9923232  0.9705822  0.9530210
  0.3  3          0.8               0.50       150      0.9944414  0.9793657  0.9565365
  0.3  3          0.8               0.75        50      0.9809249  0.9108579  0.9236476
  0.3  3          0.8               0.75       100      0.9935397  0.9727791  0.9657538
  0.3  3          0.8               0.75       150      0.9951666  0.9819973  0.9679506
  0.3  3          0.8               1.00        50      0.9810215  0.9538982  0.8476769
  0.3  3          0.8               1.00       100      0.9939596  0.9710208  0.9670725
  0.3  3          0.8               1.00       150      0.9957769  0.9819973  0.9727829
  0.4  1          0.6               0.50        50      0.9109344  0.9195903  0.6737507
  0.4  1          0.6               0.50       100      0.9597101  0.8467978  0.9376518
  0.4  1          0.6               0.50       150      0.9788745  0.9354492  0.9222894
  0.4  1          0.6               0.75        50      0.9128313  0.9789262  0.6013890
  0.4  1          0.6               0.75       100      0.9606888  0.8437314  0.9481945
  0.4  1          0.6               0.75       150      0.9785658  0.9341450  0.9402930
  0.4  1          0.6               1.00        50      0.9152290  0.7866811  0.8129506
  0.4  1          0.6               1.00       100      0.9603519  0.8345055  0.9539088
  0.4  1          0.6               1.00       150      0.9780787  0.9100087  0.9451215
  0.4  1          0.8               0.50        50      0.9149866  0.9784866  0.6110536
  0.4  1          0.8               0.50       100      0.9606345  0.8775072  0.8897484
  0.4  1          0.8               0.50       150      0.9795677  0.9416146  0.9297513
  0.4  1          0.8               0.75        50      0.9168699  0.9279265  0.6605957
  0.4  1          0.8               0.75       100      0.9598649  0.8393310  0.9508329
  0.4  1          0.8               0.75       150      0.9799396  0.9381049  0.9367804
  0.4  1          0.8               1.00        50      0.9158889  0.8542279  0.7467216
  0.4  1          0.8               1.00       100      0.9604647  0.8318710  0.9552236
  0.4  1          0.8               1.00       150      0.9782563  0.9100135  0.9459977
  0.4  2          0.6               0.50        50      0.9682954  0.8995200  0.8981849
  0.4  2          0.6               0.50       100      0.9885656  0.9688327  0.9459987
  0.4  2          0.6               0.50       150      0.9918830  0.9754078  0.9508300
  0.4  2          0.6               0.75        50      0.9685560  0.8932948  0.9003904
  0.4  2          0.6               0.75       100      0.9899102  0.9657567  0.9499470
  0.4  2          0.6               0.75       150      0.9932570  0.9754078  0.9569732
  0.4  2          0.6               1.00        50      0.9673306  0.8967901  0.8572990
  0.4  2          0.6               1.00       100      0.9902560  0.9503827  0.9613650
  0.4  2          0.6               1.00       150      0.9941211  0.9727704  0.9648757
  0.4  2          0.8               0.50        50      0.9678048  0.8713756  0.9380952
  0.4  2          0.8               0.50       100      0.9901862  0.9784847  0.9424870
  0.4  2          0.8               0.50       150      0.9926335  0.9789204  0.9534664
  0.4  2          0.8               0.75        50      0.9703590  0.9201176  0.8617052
  0.4  2          0.8               0.75       100      0.9908632  0.9714594  0.9530229
  0.4  2          0.8               0.75       150      0.9938830  0.9784847  0.9600492
  0.4  2          0.8               1.00        50      0.9687795  0.8998660  0.8669713
  0.4  2          0.8               1.00       100      0.9893238  0.9503962  0.9587314
  0.4  2          0.8               1.00       150      0.9941221  0.9784847  0.9644380
  0.4  3          0.6               0.50        50      0.9843619  0.9372267  0.9464450
  0.4  3          0.6               0.50       100      0.9929937  0.9806786  0.9569751
  0.4  3          0.6               0.50       150      0.9947627  0.9819973  0.9626817
  0.4  3          0.6               0.75        50      0.9866571  0.9407326  0.9455591
  0.4  3          0.6               0.75       100      0.9947416  0.9802391  0.9640004
  0.4  3          0.6               0.75       150      0.9953363  0.9819973  0.9670735
  0.4  3          0.6               1.00        50      0.9865093  0.9200954  0.9609206
  0.4  3          0.6               1.00       100      0.9952311  0.9776017  0.9688317
  0.4  3          0.6               1.00       150      0.9963694  0.9819973  0.9719038
  0.4  3          0.8               0.50        50      0.9850061  0.9473299  0.9455572
  0.4  3          0.8               0.50       100      0.9935803  0.9802429  0.9565385
  0.4  3          0.8               0.50       150      0.9947935  0.9819973  0.9587343
  0.4  3          0.8               0.75        50      0.9875275  0.9402959  0.9582909
  0.4  3          0.8               0.75       100      0.9952550  0.9819973  0.9675130
  0.4  3          0.8               0.75       150      0.9957593  0.9819973  0.9701456
  0.4  3          0.8               1.00        50      0.9872202  0.9161538  0.9618007
  0.4  3          0.8               1.00       100      0.9956577  0.9758473  0.9666339
  0.4  3          0.8               1.00       150      0.9969285  0.9819973  0.9732225

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 150, max_depth = 3, eta = 0.4, gamma = 0, colsample_bytree = 0.8, min_child_weight = 1 and subsample = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     49.1      1.3
  positive      0.9     48.7
                            
 Accuracy (average) : 0.9776

[1] "TRAIN accuracy: 0.977611940298508"
[1] "TRAIN +precision: 0.981842338352524"
[1] "TRAIN -precision: 0.973455178416014"
[1] "TRAIN specifity: 0.982001755926251"
[1] "TRAIN sensitivity: 0.973222124670764"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       20       20
            positive       26      729
[1] "TEST accuracy: 0.942138364779874"
[1] "TEST +precision: 0.965562913907285"
[1] "TEST -precision: 0.5"
[1] "TEST specifity: 0.434782608695652"
[1] "TEST sensitivity: 0.973297730307076"

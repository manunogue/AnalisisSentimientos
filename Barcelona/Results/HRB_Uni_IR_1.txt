[1] "DATASET NAME: HRB_Uni_IR_1"
[1] "TRAIN INSTANCES: 1692"
[1] "TEST INSTANCES: 332"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 11.2319369316101"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

1692 samples
 652 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1352, 1354, 1354, 1354, 1354 
Resampling results:

  ROC        Sens       Spec     
  0.9927403  0.9988235  0.9692795

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     49.9      1.5
  positive      0.1     48.5
                           
 Accuracy (average) : 0.984

[1] "TRAIN accuracy: 0.984042553191489"
[1] "TRAIN +precision: 0.99878197320341"
[1] "TRAIN -precision: 0.970149253731343"
[1] "TRAIN specifity: 0.998817966903073"
[1] "TRAIN sensitivity: 0.969267139479905"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       28        4
            positive       17      283
[1] "TEST accuracy: 0.936746987951807"
[1] "TEST +precision: 0.943333333333333"
[1] "TEST -precision: 0.875"
[1] "TEST specifity: 0.622222222222222"
[1] "TEST sensitivity: 0.986062717770035"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 2.5592346350352"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

1692 samples
 652 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1353, 1354, 1354, 1353, 1354 
Resampling results across tuning parameters:

  C      M  ROC        Sens       Spec     
  0.010  1  0.9407457  0.9503028  0.9149182
  0.010  2  0.9435208  0.9479360  0.9137348
  0.010  3  0.9408572  0.9479360  0.9054577
  0.255  1  0.9471324  0.9905395  0.9172642
  0.255  2  0.9536066  0.9869892  0.9172642
  0.255  3  0.9591852  0.9798886  0.9208354
  0.500  1  0.9483556  0.9905395  0.9172642
  0.500  2  0.9529869  0.9869892  0.9172642
  0.500  3  0.9573002  0.9810721  0.9172851

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.255 and M = 3.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative       49        4
  positive        1       46
                            
 Accuracy (average) : 0.9504

[1] "TRAIN accuracy: 0.950354609929078"
[1] "TRAIN +precision: 0.978643216080402"
[1] "TRAIN -precision: 0.925223214285714"
[1] "TRAIN specifity: 0.979905437352246"
[1] "TRAIN sensitivity: 0.92080378250591"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       29       13
            positive       16      274
[1] "TEST accuracy: 0.912650602409639"
[1] "TEST +precision: 0.944827586206897"
[1] "TEST -precision: 0.69047619047619"
[1] "TEST specifity: 0.644444444444444"
[1] "TEST sensitivity: 0.954703832752613"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 6.82223126490911"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

1692 samples
 652 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1354, 1354, 1353, 1353, 1354 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens       Spec     
  0.3  1          0.6               0.50        50      0.9659994  0.9066342  0.9042882
  0.3  1          0.6               0.50       100      0.9812779  0.9562826  0.9361852
  0.3  1          0.6               0.50       150      0.9876995  0.9657292  0.9444414
  0.3  1          0.6               0.75        50      0.9666683  0.9101914  0.9160808
  0.3  1          0.6               0.75       100      0.9815568  0.9574730  0.9409050
  0.3  1          0.6               0.75       150      0.9857395  0.9645527  0.9468082
  0.3  1          0.6               1.00        50      0.9667968  0.9172920  0.9160947
  0.3  1          0.6               1.00       100      0.9807855  0.9550922  0.9420814
  0.3  1          0.6               1.00       150      0.9873959  0.9562687  0.9456317
  0.3  1          0.8               0.50        50      0.9641958  0.9125653  0.9066481
  0.3  1          0.8               0.50       100      0.9796325  0.9574661  0.9290985
  0.3  1          0.8               0.50       150      0.9855343  0.9657501  0.9326279
  0.3  1          0.8               0.75        50      0.9673645  0.9102054  0.9255273
  0.3  1          0.8               0.75       100      0.9795215  0.9480125  0.9385660
  0.3  1          0.8               0.75       150      0.9862529  0.9657292  0.9456596
  0.3  1          0.8               1.00        50      0.9667208  0.9125444  0.9255552
  0.3  1          0.8               1.00       100      0.9813290  0.9527323  0.9373547
  0.3  1          0.8               1.00       150      0.9874697  0.9586425  0.9491820
  0.3  2          0.6               0.50        50      0.9835891  0.9610303  0.9420884
  0.3  2          0.6               0.50       100      0.9910227  0.9728089  0.9444692
  0.3  2          0.6               0.50       150      0.9924176  0.9905534  0.9503515
  0.3  2          0.6               0.75        50      0.9842937  0.9633623  0.9468013
  0.3  2          0.6               0.75       100      0.9905696  0.9751619  0.9515211
  0.3  2          0.6               0.75       150      0.9928504  0.9917299  0.9503376
  0.3  2          0.6               1.00        50      0.9857288  0.9586495  0.9397285
  0.3  2          0.6               1.00       100      0.9918869  0.9775426  0.9479986
  0.3  2          0.6               1.00       150      0.9940579  0.9822764  0.9574521
  0.3  2          0.8               0.50        50      0.9825790  0.9539297  0.9361573
  0.3  2          0.8               0.50       100      0.9913943  0.9775496  0.9420745
  0.3  2          0.8               0.50       150      0.9928946  0.9893770  0.9515280
  0.3  2          0.8               0.75        50      0.9840054  0.9586425  0.9326418
  0.3  2          0.8               0.75       100      0.9921567  0.9810999  0.9491542
  0.3  2          0.8               0.75       150      0.9939291  0.9870240  0.9515350
  0.3  2          0.8               1.00        50      0.9852847  0.9622137  0.9385312
  0.3  2          0.8               1.00       100      0.9927365  0.9728368  0.9479777
  0.3  2          0.8               1.00       150      0.9944934  0.9834598  0.9515211
  0.3  3          0.6               0.50        50      0.9898670  0.9669126  0.9314445
  0.3  3          0.6               0.50       100      0.9941594  0.9940898  0.9456317
  0.3  3          0.6               0.50       150      0.9940520  0.9976401  0.9456317
  0.3  3          0.6               0.75        50      0.9906565  0.9704699  0.9361504
  0.3  3          0.6               0.75       100      0.9951892  0.9917438  0.9515211
  0.3  3          0.6               0.75       150      0.9953987  0.9976331  0.9515211
  0.3  3          0.6               1.00        50      0.9917569  0.9740132  0.9562548
  0.3  3          0.6               1.00       100      0.9951242  0.9870101  0.9550644
  0.3  3          0.6               1.00       150      0.9957660  1.0000000  0.9574243
  0.3  3          0.8               0.50        50      0.9909803  0.9669266  0.9302680
  0.3  3          0.8               0.50       100      0.9949348  0.9893839  0.9397076
  0.3  3          0.8               0.50       150      0.9950053  0.9976401  0.9444414
  0.3  3          0.8               0.75        50      0.9910314  0.9728159  0.9420745
  0.3  3          0.8               0.75       100      0.9944169  0.9846432  0.9503446
  0.3  3          0.8               0.75       150      0.9952058  0.9976331  0.9479777
  0.3  3          0.8               1.00        50      0.9906233  0.9728368  0.9491542
  0.3  3          0.8               1.00       100      0.9938337  0.9822903  0.9515071
  0.3  3          0.8               1.00       150      0.9944127  0.9976471  0.9550574
  0.4  1          0.6               0.50        50      0.9690836  0.9267525  0.9090080
  0.4  1          0.6               0.50       100      0.9835224  0.9633832  0.9338253
  0.4  1          0.6               0.50       150      0.9885970  0.9692795  0.9456317
  0.4  1          0.6               0.75        50      0.9721941  0.9302680  0.9208562
  0.4  1          0.6               0.75       100      0.9850319  0.9562896  0.9444553
  0.4  1          0.6               0.75       150      0.9888206  0.9751758  0.9456526
  0.4  1          0.6               1.00        50      0.9734749  0.9314584  0.9302750
  0.4  1          0.6               1.00       100      0.9867428  0.9574521  0.9479777
  0.4  1          0.6               1.00       150      0.9901451  0.9669126  0.9491751
  0.4  1          0.8               0.50        50      0.9725867  0.9161086  0.9267177
  0.4  1          0.8               0.50       100      0.9835648  0.9550853  0.9373616
  0.4  1          0.8               0.50       150      0.9894636  0.9740063  0.9456317
  0.4  1          0.8               0.75        50      0.9712376  0.9326488  0.9219979
  0.4  1          0.8               0.75       100      0.9851377  0.9609955  0.9444483
  0.4  1          0.8               0.75       150      0.9888344  0.9728298  0.9479847
  0.4  1          0.8               1.00        50      0.9746336  0.9385451  0.9314514
  0.4  1          0.8               1.00       100      0.9859744  0.9562687  0.9467943
  0.4  1          0.8               1.00       150      0.9899004  0.9692795  0.9515350
  0.4  2          0.6               0.50        50      0.9860080  0.9645388  0.9385312
  0.4  2          0.6               0.50       100      0.9901254  0.9834459  0.9397215
  0.4  2          0.6               0.50       150      0.9915569  0.9941037  0.9432649
  0.4  2          0.6               0.75        50      0.9883291  0.9657501  0.9456109
  0.4  2          0.6               0.75       100      0.9934174  0.9822764  0.9503515
  0.4  2          0.6               0.75       150      0.9936464  0.9929273  0.9503376
  0.4  2          0.6               1.00        50      0.9891465  0.9657640  0.9444414
  0.4  2          0.6               1.00       100      0.9937878  0.9834737  0.9527115
  0.4  2          0.6               1.00       150      0.9947237  0.9929273  0.9574313
  0.4  2          0.8               0.50        50      0.9882081  0.9586495  0.9349948
  0.4  2          0.8               0.50       100      0.9934658  0.9905465  0.9444692
  0.4  2          0.8               0.50       150      0.9933884  0.9964497  0.9527115
  0.4  2          0.8               0.75        50      0.9885521  0.9621859  0.9456457
  0.4  2          0.8               0.75       100      0.9938132  0.9893630  0.9527254
  0.4  2          0.8               0.75       150      0.9943234  0.9929133  0.9550783
  0.4  2          0.8               1.00        50      0.9873809  0.9716464  0.9397215
  0.4  2          0.8               1.00       100      0.9923978  0.9822764  0.9503515
  0.4  2          0.8               1.00       150      0.9939339  0.9917438  0.9538670
  0.4  3          0.6               0.50        50      0.9911353  0.9799165  0.9444622
  0.4  3          0.6               0.50       100      0.9944369  0.9976331  0.9491612
  0.4  3          0.6               0.50       150      0.9940673  0.9976401  0.9456178
  0.4  3          0.6               0.75        50      0.9919499  0.9858127  0.9420675
  0.4  3          0.6               0.75       100      0.9932101  0.9952663  0.9491612
  0.4  3          0.6               0.75       150      0.9931901  0.9976331  0.9479708
  0.4  3          0.6               1.00        50      0.9931720  0.9787400  0.9527115
  0.4  3          0.6               1.00       100      0.9950077  0.9929273  0.9586147
  0.4  3          0.6               1.00       150      0.9952654  0.9976331  0.9550574
  0.4  3          0.8               0.50        50      0.9908048  0.9846363  0.9409189
  0.4  3          0.8               0.50       100      0.9936776  0.9917369  0.9479847
  0.4  3          0.8               0.50       150      0.9941815  0.9952732  0.9444414
  0.4  3          0.8               0.75        50      0.9927065  0.9834737  0.9408841
  0.4  3          0.8               0.75       100      0.9952256  0.9964706  0.9538810
  0.4  3          0.8               0.75       150      0.9952739  0.9976471  0.9550644
  0.4  3          0.8               1.00        50      0.9930952  0.9787470  0.9444414
  0.4  3          0.8               1.00       100      0.9952107  0.9929273  0.9479847
  0.4  3          0.8               1.00       150      0.9956344  0.9976331  0.9515211

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 150, max_depth = 3, eta = 0.3, gamma = 0, colsample_bytree = 0.6, min_child_weight = 1 and subsample = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     50.0      2.1
  positive      0.0     47.9
                            
 Accuracy (average) : 0.9787

[1] "TRAIN accuracy: 0.978723404255319"
[1] "TRAIN +precision: 1"
[1] "TRAIN -precision: 0.959183673469388"
[1] "TRAIN specifity: 1"
[1] "TRAIN sensitivity: 0.957446808510638"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       32        7
            positive       13      280
[1] "TEST accuracy: 0.939759036144578"
[1] "TEST +precision: 0.955631399317406"
[1] "TEST -precision: 0.82051282051282"
[1] "TEST specifity: 0.711111111111111"
[1] "TEST sensitivity: 0.975609756097561"

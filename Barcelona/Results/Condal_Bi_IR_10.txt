[1] "DATASET NAME: Condal_Bi_IR_10"
[1] "TRAIN INSTANCES: 2602"
[1] "TEST INSTANCES: 795"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 7.29771089553833"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

2602 samples
 720 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 2082, 2082, 2081, 2081, 2082 
Resampling results:

  ROC        Sens       Spec     
  0.9810065  0.8396635  0.9903412

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     10.5      0.8
  positive      2.0     86.7
                            
 Accuracy (average) : 0.9716

[1] "TRAIN accuracy: 0.971560338201384"
[1] "TRAIN +precision: 0.977469670710572"
[1] "TRAIN -precision: 0.925170068027211"
[1] "TRAIN specifity: 0.839506172839506"
[1] "TRAIN sensitivity: 0.990342405618964"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       15        6
            positive       31      743
[1] "TEST accuracy: 0.953459119496855"
[1] "TEST +precision: 0.959948320413437"
[1] "TEST -precision: 0.714285714285714"
[1] "TEST specifity: 0.326086956521739"
[1] "TEST sensitivity: 0.991989319092123"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 4.10313938458761"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

2602 samples
 720 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 2082, 2081, 2081, 2082, 2082 
Resampling results across tuning parameters:

  C      M  ROC        Sens        Spec     
  0.010  1  0.5938231  0.16365385  0.9956140
  0.010  2  0.5894390  0.16052885  0.9956140
  0.010  3  0.5595607  0.09879808  0.9995614
  0.255  1  0.7818006  0.37043269  0.9916599
  0.255  2  0.7580186  0.34259615  0.9947301
  0.255  3  0.7386883  0.29322115  0.9951706
  0.500  1  0.8529856  0.41966346  0.9907827
  0.500  2  0.8278517  0.36413462  0.9938539
  0.500  3  0.7939111  0.30860577  0.9956111

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.5 and M = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative      5.2      0.8
  positive      7.2     86.7
                            
 Accuracy (average) : 0.9197

[1] "TRAIN accuracy: 0.91967717140661"
[1] "TRAIN +precision: 0.923108384458078"
[1] "TRAIN -precision: 0.866242038216561"
[1] "TRAIN specifity: 0.419753086419753"
[1] "TRAIN sensitivity: 0.990781387181738"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        1        6
            positive       45      743
[1] "TEST accuracy: 0.935849056603774"
[1] "TEST +precision: 0.942893401015228"
[1] "TEST -precision: 0.142857142857143"
[1] "TEST specifity: 0.0217391304347826"
[1] "TEST sensitivity: 0.991989319092123"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 12.4337542851766"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

2602 samples
 720 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 2083, 2081, 2081, 2082, 2081 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens       Spec     
  0.3  1          0.6               0.50        50      0.8372661  0.1822596  0.9991218
  0.3  1          0.6               0.50       100      0.8842089  0.3242308  0.9982456
  0.3  1          0.6               0.50       150      0.9095338  0.4137981  0.9978061
  0.3  1          0.6               0.75        50      0.8366294  0.2008173  0.9995604
  0.3  1          0.6               0.75       100      0.8963693  0.3429808  0.9969269
  0.3  1          0.6               0.75       150      0.9122192  0.4447115  0.9978051
  0.3  1          0.6               1.00        50      0.8408026  0.2038942  1.0000000
  0.3  1          0.6               1.00       100      0.8926676  0.3242308  0.9986832
  0.3  1          0.6               1.00       150      0.9148500  0.3954327  0.9978051
  0.3  1          0.8               0.50        50      0.8219749  0.2070192  0.9986823
  0.3  1          0.8               0.50       100      0.8895238  0.3366346  0.9969269
  0.3  1          0.8               0.50       150      0.9089483  0.4199519  0.9964883
  0.3  1          0.8               0.75        50      0.8387287  0.2192788  0.9991218
  0.3  1          0.8               0.75       100      0.8869713  0.3365865  0.9973655
  0.3  1          0.8               0.75       150      0.9056298  0.4446635  0.9960488
  0.3  1          0.8               1.00        50      0.8427521  0.1977404  1.0000000
  0.3  1          0.8               1.00       100      0.8908296  0.3118750  0.9973665
  0.3  1          0.8               1.00       150      0.9132073  0.4138462  0.9978051
  0.3  2          0.6               0.50        50      0.8814465  0.2994712  0.9982437
  0.3  2          0.6               0.50       100      0.9155216  0.4694712  0.9964893
  0.3  2          0.6               0.50       150      0.9269025  0.5435577  0.9942934
  0.3  2          0.6               0.75        50      0.8878219  0.3459135  0.9991218
  0.3  2          0.6               0.75       100      0.9233650  0.5125962  0.9964874
  0.3  2          0.6               0.75       150      0.9379993  0.5712981  0.9951706
  0.3  2          0.6               1.00        50      0.8895960  0.3118750  0.9978061
  0.3  2          0.6               1.00       100      0.9329091  0.4817308  0.9973655
  0.3  2          0.6               1.00       150      0.9473008  0.5341827  0.9960488
  0.3  2          0.8               0.50        50      0.8872704  0.3118269  0.9978051
  0.3  2          0.8               0.50       100      0.9155890  0.4785577  0.9956111
  0.3  2          0.8               0.50       150      0.9274916  0.5311058  0.9916599
  0.3  2          0.8               0.75        50      0.8908773  0.3211538  0.9969260
  0.3  2          0.8               0.75       100      0.9255630  0.4939904  0.9973665
  0.3  2          0.8               0.75       150      0.9402084  0.5775000  0.9951716
  0.3  2          0.8               1.00        50      0.8939450  0.3180288  0.9991218
  0.3  2          0.8               1.00       100      0.9346053  0.4662500  0.9973655
  0.3  2          0.8               1.00       150      0.9467189  0.5434615  0.9964883
  0.3  3          0.6               0.50        50      0.9062959  0.3797115  0.9969279
  0.3  3          0.6               0.50       100      0.9268698  0.5250000  0.9951716
  0.3  3          0.6               0.50       150      0.9342363  0.5713462  0.9877058
  0.3  3          0.6               0.75        50      0.9120163  0.4292788  0.9978051
  0.3  3          0.6               0.75       100      0.9427206  0.5557692  0.9960497
  0.3  3          0.6               0.75       150      0.9513922  0.5928846  0.9938529
  0.3  3          0.6               1.00        50      0.9138617  0.4292788  0.9978051
  0.3  3          0.6               1.00       100      0.9470731  0.5588462  0.9969279
  0.3  3          0.6               1.00       150      0.9572068  0.5929327  0.9947311
  0.3  3          0.8               0.50        50      0.9056391  0.4291346  0.9969279
  0.3  3          0.8               0.50       100      0.9317393  0.5249038  0.9942944
  0.3  3          0.8               0.50       150      0.9350925  0.5805288  0.9921014
  0.3  3          0.8               0.75        50      0.9095845  0.4137500  0.9969260
  0.3  3          0.8               0.75       100      0.9397433  0.5558173  0.9951725
  0.3  3          0.8               0.75       150      0.9488108  0.5959615  0.9912194
  0.3  3          0.8               1.00        50      0.9184323  0.4353846  0.9978041
  0.3  3          0.8               1.00       100      0.9482858  0.5434135  0.9969279
  0.3  3          0.8               1.00       150      0.9598463  0.5928846  0.9942934
  0.4  1          0.6               0.50        50      0.8469772  0.2563462  0.9986832
  0.4  1          0.6               0.50       100      0.8873987  0.4169231  0.9951706
  0.4  1          0.6               0.50       150      0.9092108  0.4971635  0.9934172
  0.4  1          0.6               0.75        50      0.8545356  0.2440385  0.9982456
  0.4  1          0.6               0.75       100      0.9028351  0.4076442  0.9969279
  0.4  1          0.6               0.75       150      0.9213573  0.4940865  0.9964874
  0.4  1          0.6               1.00        50      0.8633544  0.2532212  0.9982447
  0.4  1          0.6               1.00       100      0.9010080  0.4076923  0.9973655
  0.4  1          0.6               1.00       150      0.9306513  0.4694231  0.9973655
  0.4  1          0.8               0.50        50      0.8438743  0.2500481  0.9978061
  0.4  1          0.8               0.50       100      0.8902410  0.4012500  0.9964864
  0.4  1          0.8               0.50       150      0.9098804  0.4601923  0.9942925
  0.4  1          0.8               0.75        50      0.8550770  0.2563462  0.9982437
  0.4  1          0.8               0.75       100      0.9069242  0.4076923  0.9969269
  0.4  1          0.8               0.75       150      0.9209615  0.4971635  0.9969269
  0.4  1          0.8               1.00        50      0.8606944  0.2470192  0.9973655
  0.4  1          0.8               1.00       100      0.9040706  0.4076923  0.9973655
  0.4  1          0.8               1.00       150      0.9275558  0.4692788  0.9973655
  0.4  2          0.6               0.50        50      0.8873283  0.3888942  0.9960497
  0.4  2          0.6               0.50       100      0.9245253  0.5250000  0.9951716
  0.4  2          0.6               0.50       150      0.9301049  0.5465865  0.9894640
  0.4  2          0.6               0.75        50      0.9022213  0.4231250  0.9964874
  0.4  2          0.6               0.75       100      0.9325902  0.5619712  0.9951697
  0.4  2          0.6               0.75       150      0.9453844  0.6021154  0.9920966
  0.4  2          0.6               1.00        50      0.9074803  0.4013942  0.9973655
  0.4  2          0.6               1.00       100      0.9408809  0.5403846  0.9969269
  0.4  2          0.6               1.00       150      0.9526835  0.5774519  0.9947320
  0.4  2          0.8               0.50        50      0.8986313  0.3952885  0.9969279
  0.4  2          0.8               0.50       100      0.9265658  0.5310577  0.9912233
  0.4  2          0.8               0.50       150      0.9338400  0.5774038  0.9907837
  0.4  2          0.8               0.75        50      0.9014192  0.4106731  0.9969260
  0.4  2          0.8               0.75       100      0.9373415  0.5497115  0.9942925
  0.4  2          0.8               0.75       150      0.9462193  0.5990865  0.9925371
  0.4  2          0.8               1.00        50      0.9051999  0.4046154  0.9982437
  0.4  2          0.8               1.00       100      0.9435625  0.5495673  0.9969269
  0.4  2          0.8               1.00       150      0.9547726  0.5959615  0.9956092
  0.4  3          0.6               0.50        50      0.9166176  0.4476442  0.9942944
  0.4  3          0.6               0.50       100      0.9351191  0.5465385  0.9894669
  0.4  3          0.6               0.50       150      0.9401586  0.6022596  0.9868305
  0.4  3          0.6               0.75        50      0.9206357  0.4878365  0.9969269
  0.4  3          0.6               0.75       100      0.9466118  0.5959615  0.9929757
  0.4  3          0.6               0.75       150      0.9515655  0.6175962  0.9912204
  0.4  3          0.6               1.00        50      0.9240480  0.4755769  0.9978051
  0.4  3          0.6               1.00       100      0.9551922  0.5865865  0.9947311
  0.4  3          0.6               1.00       150      0.9646670  0.6175962  0.9938558
  0.4  3          0.8               0.50        50      0.9178217  0.4725962  0.9951706
  0.4  3          0.8               0.50       100      0.9334047  0.5650481  0.9894631
  0.4  3          0.8               0.50       150      0.9365164  0.6113942  0.9850733
  0.4  3          0.8               0.75        50      0.9227644  0.4972115  0.9960488
  0.4  3          0.8               0.75       100      0.9482003  0.5929327  0.9925390
  0.4  3          0.8               0.75       150      0.9517733  0.6237500  0.9885859
  0.4  3          0.8               1.00        50      0.9309122  0.4941346  0.9964874
  0.4  3          0.8               1.00       100      0.9568294  0.5929327  0.9942915
  0.4  3          0.8               1.00       150      0.9644969  0.6206731  0.9920985

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 150, max_depth = 3, eta = 0.4, gamma = 0, colsample_bytree = 0.6, min_child_weight = 1 and subsample = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative      7.7      0.5
  positive      4.8     87.0
                           
 Accuracy (average) : 0.947

[1] "TRAIN accuracy: 0.946963873943121"
[1] "TRAIN +precision: 0.948073701842546"
[1] "TRAIN -precision: 0.934579439252336"
[1] "TRAIN specifity: 0.617283950617284"
[1] "TRAIN sensitivity: 0.993854258121159"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       11        4
            positive       35      745
[1] "TEST accuracy: 0.950943396226415"
[1] "TEST +precision: 0.955128205128205"
[1] "TEST -precision: 0.733333333333333"
[1] "TEST specifity: 0.239130434782609"
[1] "TEST sensitivity: 0.994659546061415"

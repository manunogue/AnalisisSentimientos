[1] "DATASET NAME: Cerveceria_Uni_IR_10"
[1] "TRAIN INSTANCES: 3162"
[1] "TEST INSTANCES: 968"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 26.292916059494"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

3162 samples
 654 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 2530, 2530, 2529, 2530, 2529 
Resampling results:

  ROC        Sens       Spec     
  0.9781632  0.9246253  0.9847273

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     12.0      1.3
  positive      1.0     85.6
                            
 Accuracy (average) : 0.9769

[1] "TRAIN accuracy: 0.976913345983555"
[1] "TRAIN +precision: 0.988682000730194"
[1] "TRAIN -precision: 0.900709219858156"
[1] "TRAIN specifity: 0.924757281553398"
[1] "TRAIN sensitivity: 0.984727272727273"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       21       14
            positive       25      908
[1] "TEST accuracy: 0.959710743801653"
[1] "TEST +precision: 0.973204715969989"
[1] "TEST -precision: 0.6"
[1] "TEST specifity: 0.456521739130435"
[1] "TEST sensitivity: 0.984815618221258"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 5.48071996370951"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

3162 samples
 654 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 2529, 2530, 2529, 2530, 2530 
Resampling results across tuning parameters:

  C      M  ROC        Sens       Spec     
  0.010  1  0.7699259  0.5558037  0.9803636
  0.010  2  0.7659751  0.5460476  0.9800000
  0.010  3  0.7615142  0.5168381  0.9807273
  0.255  1  0.9057415  0.7863650  0.9741818
  0.255  2  0.9016232  0.7693212  0.9709091
  0.255  3  0.8963905  0.7352336  0.9716364
  0.500  1  0.9209834  0.8227153  0.9734545
  0.500  2  0.9205407  0.7910961  0.9709091
  0.500  3  0.9105698  0.7643256  0.9690909

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.5 and M = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     10.7      2.3
  positive      2.3     84.7
                            
 Accuracy (average) : 0.9538

[1] "TRAIN accuracy: 0.953826691967109"
[1] "TRAIN +precision: 0.973454545454545"
[1] "TRAIN -precision: 0.822815533980582"
[1] "TRAIN specifity: 0.822815533980582"
[1] "TRAIN sensitivity: 0.973454545454545"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       18       17
            positive       28      905
[1] "TEST accuracy: 0.953512396694215"
[1] "TEST +precision: 0.969989281886388"
[1] "TEST -precision: 0.514285714285714"
[1] "TEST specifity: 0.391304347826087"
[1] "TEST sensitivity: 0.981561822125813"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 11.0283147335052"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

3162 samples
 654 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 2530, 2530, 2529, 2529, 2530 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens       Spec     
  0.3  1          0.6               0.50        50      0.9541144  0.4125478  0.9941818
  0.3  1          0.6               0.50       100      0.9701218  0.5823685  0.9916364
  0.3  1          0.6               0.50       150      0.9762659  0.6358213  0.9890909
  0.3  1          0.6               0.75        50      0.9528415  0.4344108  0.9941818
  0.3  1          0.6               0.75       100      0.9681752  0.5508963  0.9905455
  0.3  1          0.6               0.75       150      0.9718337  0.6600059  0.9905455
  0.3  1          0.6               1.00        50      0.9510138  0.4004408  0.9945455
  0.3  1          0.6               1.00       100      0.9687560  0.5386424  0.9923636
  0.3  1          0.6               1.00       150      0.9756300  0.6310314  0.9916364
  0.3  1          0.8               0.50        50      0.9563200  0.4245666  0.9920000
  0.3  1          0.8               0.50       100      0.9706497  0.5654129  0.9905455
  0.3  1          0.8               0.50       150      0.9766085  0.6430502  0.9890909
  0.3  1          0.8               0.75        50      0.9509503  0.4029092  0.9938182
  0.3  1          0.8               0.75       100      0.9675308  0.5458713  0.9912727
  0.3  1          0.8               0.75       150      0.9765507  0.6478695  0.9909091
  0.3  1          0.8               1.00        50      0.9504362  0.4053482  0.9952727
  0.3  1          0.8               1.00       100      0.9684204  0.5313547  0.9927273
  0.3  1          0.8               1.00       150      0.9750450  0.6285924  0.9920000
  0.3  2          0.6               0.50        50      0.9746303  0.5895974  0.9912727
  0.3  2          0.6               0.50       100      0.9828432  0.7255951  0.9912727
  0.3  2          0.6               0.50       150      0.9857686  0.7814575  0.9883636
  0.3  2          0.6               0.75        50      0.9719014  0.6210990  0.9923636
  0.3  2          0.6               0.75       100      0.9806555  0.7302674  0.9901818
  0.3  2          0.6               0.75       150      0.9825511  0.7862180  0.9872727
  0.3  2          0.6               1.00        50      0.9720414  0.5848369  0.9916364
  0.3  2          0.6               1.00       100      0.9793719  0.7231560  0.9901818
  0.3  2          0.6               1.00       150      0.9824173  0.7861299  0.9894545
  0.3  2          0.8               0.50        50      0.9752481  0.5944461  0.9909091
  0.3  2          0.8               0.50       100      0.9825483  0.7377020  0.9880000
  0.3  2          0.8               0.50       150      0.9845099  0.8129592  0.9883636
  0.3  2          0.8               0.75        50      0.9742352  0.6091978  0.9920000
  0.3  2          0.8               0.75       100      0.9820610  0.7547752  0.9909091
  0.3  2          0.8               0.75       150      0.9853679  0.8105201  0.9894545
  0.3  2          0.8               1.00        50      0.9723548  0.6041434  0.9930909
  0.3  2          0.8               1.00       100      0.9820526  0.7329121  0.9920000
  0.3  2          0.8               1.00       150      0.9847436  0.7837790  0.9905455
  0.3  3          0.6               0.50        50      0.9768912  0.7014105  0.9909091
  0.3  3          0.6               0.50       100      0.9841955  0.7960035  0.9905455
  0.3  3          0.6               0.50       150      0.9854197  0.8662944  0.9883636
  0.3  3          0.6               0.75        50      0.9799073  0.7134293  0.9912727
  0.3  3          0.6               0.75       100      0.9852685  0.8201881  0.9894545
  0.3  3          0.6               0.75       150      0.9874840  0.8735234  0.9898182
  0.3  3          0.6               1.00        50      0.9822282  0.7207758  0.9934545
  0.3  3          0.6               1.00       100      0.9859256  0.8226565  0.9890909
  0.3  3          0.6               1.00       150      0.9878724  0.8905671  0.9880000
  0.3  3          0.8               0.50        50      0.9782619  0.6965031  0.9898182
  0.3  3          0.8               0.50       100      0.9838804  0.8298560  0.9880000
  0.3  3          0.8               0.50       150      0.9866593  0.8759036  0.9872727
  0.3  3          0.8               0.75        50      0.9812313  0.7377314  0.9905455
  0.3  3          0.8               0.75       100      0.9867948  0.8250955  0.9912727
  0.3  3          0.8               0.75       150      0.9885079  0.8760212  0.9898182
  0.3  3          0.8               1.00        50      0.9814380  0.7159271  0.9920000
  0.3  3          0.8               1.00       100      0.9849553  0.8178666  0.9890909
  0.3  3          0.8               1.00       150      0.9868945  0.8881575  0.9901818
  0.4  1          0.6               0.50        50      0.9578667  0.5120188  0.9912727
  0.4  1          0.6               0.50       100      0.9716624  0.6502792  0.9909091
  0.4  1          0.6               0.50       150      0.9787968  0.7304731  0.9898182
  0.4  1          0.6               0.75        50      0.9604366  0.4902439  0.9905455
  0.4  1          0.6               0.75       100      0.9747180  0.6114899  0.9909091
  0.4  1          0.6               0.75       150      0.9766265  0.7037908  0.9880000
  0.4  1          0.6               1.00        50      0.9585698  0.4730826  0.9930909
  0.4  1          0.6               1.00       100      0.9738176  0.6115192  0.9905455
  0.4  1          0.6               1.00       150      0.9787176  0.6745813  0.9887273
  0.4  1          0.8               0.50        50      0.9581072  0.4828387  0.9938182
  0.4  1          0.8               0.50       100      0.9713925  0.6186894  0.9865455
  0.4  1          0.8               0.50       150      0.9800796  0.7036732  0.9865455
  0.4  1          0.8               0.75        50      0.9584973  0.4804584  0.9916364
  0.4  1          0.8               0.75       100      0.9743836  0.6526888  0.9898182
  0.4  1          0.8               0.75       150      0.9781538  0.7232442  0.9890909
  0.4  1          0.8               1.00        50      0.9595215  0.4852189  0.9920000
  0.4  1          0.8               1.00       100      0.9743379  0.6188069  0.9920000
  0.4  1          0.8               1.00       150      0.9787918  0.6818102  0.9890909
  0.4  2          0.6               0.50        50      0.9745926  0.6746694  0.9916364
  0.4  2          0.6               0.50       100      0.9815736  0.7935351  0.9898182
  0.4  2          0.6               0.50       150      0.9830889  0.8421393  0.9854545
  0.4  2          0.6               0.75        50      0.9743280  0.6868351  0.9912727
  0.4  2          0.6               0.75       100      0.9822140  0.7717308  0.9858182
  0.4  2          0.6               0.75       150      0.9829914  0.8298266  0.9850909
  0.4  2          0.6               1.00        50      0.9775687  0.6720541  0.9912727
  0.4  2          0.6               1.00       100      0.9846377  0.7789304  0.9916364
  0.4  2          0.6               1.00       150      0.9867936  0.8202175  0.9901818
  0.4  2          0.8               0.50        50      0.9720554  0.6770203  0.9861818
  0.4  2          0.8               0.50       100      0.9818782  0.7885395  0.9876364
  0.4  2          0.8               0.50       150      0.9852987  0.8370262  0.9883636
  0.4  2          0.8               0.75        50      0.9778626  0.6795181  0.9898182
  0.4  2          0.8               0.75       100      0.9833240  0.7886865  0.9883636
  0.4  2          0.8               0.75       150      0.9849313  0.8420805  0.9872727
  0.4  2          0.8               1.00        50      0.9783263  0.6793711  0.9905455
  0.4  2          0.8               1.00       100      0.9831168  0.7692330  0.9901818
  0.4  2          0.8               1.00       150      0.9850488  0.8299442  0.9887273
  0.4  3          0.6               0.50        50      0.9784178  0.7547164  0.9880000
  0.4  3          0.6               0.50       100      0.9840475  0.8518366  0.9880000
  0.4  3          0.6               0.50       150      0.9854197  0.8784602  0.9865455
  0.4  3          0.6               0.75        50      0.9799149  0.7619453  0.9920000
  0.4  3          0.6               0.75       100      0.9855358  0.8638554  0.9909091
  0.4  3          0.6               0.75       150      0.9867144  0.8977667  0.9887273
  0.4  3          0.6               1.00        50      0.9857666  0.7475463  0.9912727
  0.4  3          0.6               1.00       100      0.9883701  0.8663238  0.9898182
  0.4  3          0.6               1.00       150      0.9902851  0.8953864  0.9890909
  0.4  3          0.8               0.50        50      0.9811362  0.7668528  0.9898182
  0.4  3          0.8               0.50       100      0.9859796  0.8540993  0.9872727
  0.4  3          0.8               0.50       150      0.9857357  0.8904790  0.9869091
  0.4  3          0.8               0.75        50      0.9830788  0.7789891  0.9883636
  0.4  3          0.8               0.75       100      0.9865506  0.8735234  0.9880000
  0.4  3          0.8               0.75       150      0.9872268  0.8977373  0.9861818
  0.4  3          0.8               1.00        50      0.9828455  0.7740817  0.9934545
  0.4  3          0.8               1.00       100      0.9867944  0.8590361  0.9909091
  0.4  3          0.8               1.00       150      0.9879417  0.9050544  0.9894545

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 150, max_depth = 3, eta = 0.4, gamma = 0, colsample_bytree = 0.6, min_child_weight = 1 and subsample = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     11.7      0.9
  positive      1.4     86.0
                            
 Accuracy (average) : 0.9769

[1] "TRAIN accuracy: 0.976913345983555"
[1] "TRAIN +precision: 0.984437205935577"
[1] "TRAIN -precision: 0.924812030075188"
[1] "TRAIN specifity: 0.895631067961165"
[1] "TRAIN sensitivity: 0.989090909090909"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       25        6
            positive       21      916
[1] "TEST accuracy: 0.972107438016529"
[1] "TEST +precision: 0.977588046958378"
[1] "TEST -precision: 0.806451612903226"
[1] "TEST specifity: 0.543478260869565"
[1] "TEST sensitivity: 0.993492407809111"

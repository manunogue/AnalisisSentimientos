[1] "DATASET NAME: HRB_Uni_IR_5"
[1] "TRAIN INSTANCES: 1134"
[1] "TEST INSTANCES: 332"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 7.33495593070984"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

1134 samples
 652 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 908, 907, 908, 906, 907 
Resampling results:

  ROC        Sens       Spec     
  0.9815129  0.9203267  0.9692586

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     23.4      2.3
  positive      2.0     72.3
                            
 Accuracy (average) : 0.9568

[1] "TRAIN accuracy: 0.95679012345679"
[1] "TRAIN +precision: 0.972716488730724"
[1] "TRAIN -precision: 0.910652920962199"
[1] "TRAIN specifity: 0.920138888888889"
[1] "TRAIN sensitivity: 0.969267139479905"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       30        5
            positive       15      282
[1] "TEST accuracy: 0.939759036144578"
[1] "TEST +precision: 0.94949494949495"
[1] "TEST -precision: 0.857142857142857"
[1] "TEST specifity: 0.666666666666667"
[1] "TEST sensitivity: 0.982578397212544"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 1.4847488005956"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

1134 samples
 652 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 907, 907, 908, 907, 907 
Resampling results across tuning parameters:

  C      M  ROC        Sens       Spec     
  0.010  1  0.7881905  0.6426497  0.9397007
  0.010  2  0.7736147  0.6113128  0.9432370
  0.010  3  0.7684498  0.6007864  0.9408702
  0.255  1  0.8745654  0.8368421  0.9219840
  0.255  2  0.8723143  0.8022989  0.9279011
  0.255  3  0.8746314  0.7604356  0.9231535
  0.500  1  0.8760436  0.8403509  0.9208006
  0.500  2  0.8659472  0.8127042  0.9243509
  0.500  3  0.8766902  0.7742892  0.9219770

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.5 and M = 3.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     19.7      5.8
  positive      5.7     68.8
                            
 Accuracy (average) : 0.8845

[1] "TRAIN accuracy: 0.884479717813051"
[1] "TRAIN +precision: 0.923076923076923"
[1] "TRAIN -precision: 0.771626297577855"
[1] "TRAIN specifity: 0.774305555555555"
[1] "TRAIN sensitivity: 0.921985815602837"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       23       12
            positive       22      275
[1] "TEST accuracy: 0.897590361445783"
[1] "TEST +precision: 0.925925925925926"
[1] "TEST -precision: 0.657142857142857"
[1] "TEST specifity: 0.511111111111111"
[1] "TEST sensitivity: 0.958188153310105"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 4.32323873440425"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

1134 samples
 652 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 907, 908, 907, 907, 907 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens       Spec     
  0.3  1          0.6               0.50        50      0.9566989  0.6492438  0.9787400
  0.3  1          0.6               0.50       100      0.9715450  0.7640048  0.9763662
  0.3  1          0.6               0.50       150      0.9767966  0.8264368  0.9775496
  0.3  1          0.6               0.75        50      0.9599459  0.6563218  0.9811069
  0.3  1          0.6               0.75       100      0.9736737  0.7950998  0.9799304
  0.3  1          0.6               0.75       150      0.9782368  0.8195402  0.9787330
  0.3  1          0.6               1.00        50      0.9602534  0.6529946  0.9870101
  0.3  1          0.6               1.00       100      0.9727149  0.7675136  0.9834668
  0.3  1          0.6               1.00       150      0.9784963  0.8264368  0.9799165
  0.3  1          0.8               0.50        50      0.9596411  0.6632789  0.9751758
  0.3  1          0.8               0.50       100      0.9715534  0.7915306  0.9763592
  0.3  1          0.8               0.50       150      0.9754047  0.8369631  0.9680961
  0.3  1          0.8               0.75        50      0.9611407  0.6667877  0.9799234
  0.3  1          0.8               0.75       100      0.9720184  0.7467030  0.9787330
  0.3  1          0.8               0.75       150      0.9767010  0.8333333  0.9775566
  0.3  1          0.8               1.00        50      0.9605133  0.6425287  0.9858267
  0.3  1          0.8               1.00       100      0.9725883  0.7743497  0.9846432
  0.3  1          0.8               1.00       150      0.9785964  0.8194192  0.9810999
  0.3  2          0.6               0.50        50      0.9682527  0.7813067  0.9728437
  0.3  2          0.6               0.50       100      0.9770613  0.8646098  0.9704769
  0.3  2          0.6               0.50       150      0.9797652  0.8819722  0.9621859
  0.3  2          0.6               0.75        50      0.9713007  0.8055656  0.9763731
  0.3  2          0.6               0.75       100      0.9802848  0.8750151  0.9775496
  0.3  2          0.6               0.75       150      0.9831085  0.9027828  0.9716324
  0.3  2          0.6               1.00        50      0.9741960  0.8019359  0.9787400
  0.3  2          0.6               1.00       100      0.9805663  0.8575318  0.9775566
  0.3  2          0.6               1.00       150      0.9842618  0.8958863  0.9751827
  0.3  2          0.8               0.50        50      0.9733404  0.7778584  0.9751758
  0.3  2          0.8               0.50       100      0.9812402  0.8750756  0.9728159
  0.3  2          0.8               0.50       150      0.9814605  0.8924985  0.9704420
  0.3  2          0.8               0.75        50      0.9769781  0.7985481  0.9799165
  0.3  2          0.8               0.75       100      0.9829362  0.8679371  0.9751827
  0.3  2          0.8               0.75       150      0.9847798  0.8854809  0.9716255
  0.3  2          0.8               1.00        50      0.9752794  0.8124017  0.9799234
  0.3  2          0.8               1.00       100      0.9828707  0.8679976  0.9799025
  0.3  2          0.8               1.00       150      0.9848891  0.8785239  0.9751827
  0.3  3          0.6               0.50        50      0.9779915  0.8473079  0.9763871
  0.3  3          0.6               0.50       100      0.9821077  0.8785844  0.9704560
  0.3  3          0.6               0.50       150      0.9826387  0.8993950  0.9621789
  0.3  3          0.6               0.75        50      0.9786150  0.8264973  0.9740063
  0.3  3          0.6               0.75       100      0.9839507  0.8889292  0.9716394
  0.3  3          0.6               0.75       150      0.9841110  0.8958863  0.9692656
  0.3  3          0.6               1.00        50      0.9817819  0.8367211  0.9799165
  0.3  3          0.6               1.00       100      0.9863625  0.8957653  0.9716394
  0.3  3          0.6               1.00       150      0.9860016  0.9027223  0.9680891
  0.3  3          0.8               0.50        50      0.9796114  0.8297641  0.9692865
  0.3  3          0.8               0.50       100      0.9813156  0.8888082  0.9681030
  0.3  3          0.8               0.50       150      0.9799799  0.9027828  0.9633623
  0.3  3          0.8               0.75        50      0.9826707  0.8369026  0.9822903
  0.3  3          0.8               0.75       100      0.9855458  0.8783424  0.9775496
  0.3  3          0.8               0.75       150      0.9842715  0.8888082  0.9728159
  0.3  3          0.8               1.00        50      0.9799303  0.8575318  0.9775566
  0.3  3          0.8               1.00       100      0.9861835  0.8923170  0.9716464
  0.3  3          0.8               1.00       150      0.9852783  0.9027828  0.9704629
  0.4  1          0.6               0.50        50      0.9652923  0.6979431  0.9775635
  0.4  1          0.6               0.50       100      0.9730497  0.7985481  0.9704699
  0.4  1          0.6               0.50       150      0.9783606  0.8508167  0.9740202
  0.4  1          0.6               0.75        50      0.9671655  0.7325469  0.9799165
  0.4  1          0.6               0.75       100      0.9772469  0.8263763  0.9787400
  0.4  1          0.6               0.75       150      0.9807934  0.8436782  0.9787400
  0.4  1          0.6               1.00        50      0.9639752  0.7013309  0.9810999
  0.4  1          0.6               1.00       100      0.9764339  0.8125227  0.9822833
  0.4  1          0.6               1.00       150      0.9812006  0.8437992  0.9787330
  0.4  1          0.8               0.50        50      0.9646166  0.7046582  0.9728228
  0.4  1          0.8               0.50       100      0.9748186  0.7986086  0.9740063
  0.4  1          0.8               0.50       150      0.9763701  0.8507562  0.9716324
  0.4  1          0.8               0.75        50      0.9686718  0.7082880  0.9775426
  0.4  1          0.8               0.75       100      0.9771029  0.8125227  0.9775566
  0.4  1          0.8               0.75       150      0.9802081  0.8506957  0.9728228
  0.4  1          0.8               1.00        50      0.9657468  0.7255293  0.9834668
  0.4  1          0.8               1.00       100      0.9765794  0.8090744  0.9834668
  0.4  1          0.8               1.00       150      0.9798915  0.8472474  0.9775496
  0.4  2          0.6               0.50        50      0.9757301  0.8229885  0.9763731
  0.4  2          0.6               0.50       100      0.9804373  0.8609800  0.9763522
  0.4  2          0.6               0.50       150      0.9805536  0.8855414  0.9716255
  0.4  2          0.6               0.75        50      0.9793081  0.8262553  0.9716394
  0.4  2          0.6               0.75       100      0.9840664  0.8886872  0.9704629
  0.4  2          0.6               0.75       150      0.9836342  0.9096794  0.9716394
  0.4  2          0.6               1.00        50      0.9776146  0.8298851  0.9775566
  0.4  2          0.6               1.00       100      0.9835520  0.8992136  0.9751897
  0.4  2          0.6               1.00       150      0.9853279  0.9027828  0.9692725
  0.4  2          0.8               0.50        50      0.9749803  0.8125227  0.9692865
  0.4  2          0.8               0.50       100      0.9773866  0.8785844  0.9621928
  0.4  2          0.8               0.50       150      0.9788040  0.8855414  0.9657292
  0.4  2          0.8               0.75        50      0.9777952  0.8333333  0.9728298
  0.4  2          0.8               0.75       100      0.9835089  0.8853600  0.9728089
  0.4  2          0.8               0.75       150      0.9838057  0.9027223  0.9657083
  0.4  2          0.8               1.00        50      0.9781221  0.8297641  0.9751758
  0.4  2          0.8               1.00       100      0.9845808  0.8887477  0.9763731
  0.4  2          0.8               1.00       150      0.9859854  0.9026013  0.9739993
  0.4  3          0.6               0.50        50      0.9764454  0.8610405  0.9669335
  0.4  3          0.6               0.50       100      0.9789678  0.8819722  0.9645597
  0.4  3          0.6               0.50       150      0.9790315  0.8750151  0.9586356
  0.4  3          0.6               0.75        50      0.9833183  0.8543255  0.9787191
  0.4  3          0.6               0.75       100      0.9850901  0.9029643  0.9716324
  0.4  3          0.6               0.75       150      0.9845168  0.9133091  0.9680752
  0.4  3          0.6               1.00        50      0.9821333  0.8611010  0.9763801
  0.4  3          0.6               1.00       100      0.9848433  0.8957653  0.9704769
  0.4  3          0.6               1.00       150      0.9844874  0.9061706  0.9704699
  0.4  3          0.8               0.50        50      0.9776855  0.8646098  0.9669335
  0.4  3          0.8               0.50       100      0.9793104  0.8959468  0.9645527
  0.4  3          0.8               0.50       150      0.9793629  0.8993345  0.9633693
  0.4  3          0.8               0.75        50      0.9797775  0.8577737  0.9716464
  0.4  3          0.8               0.75       100      0.9821398  0.8992740  0.9728228
  0.4  3          0.8               0.75       150      0.9814550  0.8992136  0.9645458
  0.4  3          0.8               1.00        50      0.9830785  0.8679976  0.9739993
  0.4  3          0.8               1.00       100      0.9848430  0.9027828  0.9728228
  0.4  3          0.8               1.00       150      0.9836458  0.8957653  0.9692656

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 100, max_depth = 3, eta = 0.3, gamma = 0, colsample_bytree = 0.6, min_child_weight = 1 and subsample = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     22.8      2.1
  positive      2.6     72.5
                            
 Accuracy (average) : 0.9524

[1] "TRAIN accuracy: 0.952380952380952"
[1] "TRAIN +precision: 0.964788732394366"
[1] "TRAIN -precision: 0.914893617021277"
[1] "TRAIN specifity: 0.895833333333333"
[1] "TRAIN sensitivity: 0.971631205673759"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       31        5
            positive       14      282
[1] "TEST accuracy: 0.942771084337349"
[1] "TEST +precision: 0.952702702702703"
[1] "TEST -precision: 0.861111111111111"
[1] "TEST specifity: 0.688888888888889"
[1] "TEST sensitivity: 0.982578397212544"

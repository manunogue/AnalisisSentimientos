[1] "DATASET NAME: HRB_Uni_IR_10"
[1] "TRAIN INSTANCES: 1065"
[1] "TEST INSTANCES: 332"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 7.32041311264038"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

1065 samples
 652 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 852, 852, 852, 851, 853 
Resampling results:

  ROC        Sens       Spec     
  0.9677292  0.8269556  0.9668918

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     17.0      2.6
  positive      3.6     76.8
                           
 Accuracy (average) : 0.938

[1] "TRAIN accuracy: 0.938028169014084"
[1] "TRAIN +precision: 0.955607476635514"
[1] "TRAIN -precision: 0.866028708133971"
[1] "TRAIN specifity: 0.82648401826484"
[1] "TRAIN sensitivity: 0.966903073286052"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       32        6
            positive       13      281
[1] "TEST accuracy: 0.942771084337349"
[1] "TEST +precision: 0.95578231292517"
[1] "TEST -precision: 0.842105263157895"
[1] "TEST specifity: 0.711111111111111"
[1] "TEST sensitivity: 0.979094076655052"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 1.74961624940236"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

1065 samples
 652 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 853, 852, 852, 852, 851 
Resampling results across tuning parameters:

  C      M  ROC        Sens       Spec     
  0.010  1  0.7321246  0.5297040  0.9633623
  0.010  2  0.7297161  0.5343552  0.9609955
  0.010  3  0.7590325  0.5205074  0.9598329
  0.255  1  0.8502313  0.7214588  0.9361573
  0.255  2  0.8490351  0.7032770  0.9326210
  0.255  3  0.8585401  0.6754757  0.9302680
  0.500  1  0.8539413  0.7490486  0.9326001
  0.500  2  0.8808006  0.7305497  0.9278872
  0.500  3  0.8790220  0.7121564  0.9219840

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.5 and M = 2.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     15.0      5.7
  positive      5.5     73.7
                            
 Accuracy (average) : 0.8873

[1] "TRAIN accuracy: 0.887323943661972"
[1] "TRAIN +precision: 0.930094786729858"
[1] "TRAIN -precision: 0.723981900452489"
[1] "TRAIN specifity: 0.730593607305936"
[1] "TRAIN sensitivity: 0.92789598108747"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       23       10
            positive       22      277
[1] "TEST accuracy: 0.903614457831325"
[1] "TEST +precision: 0.926421404682274"
[1] "TEST -precision: 0.696969696969697"
[1] "TEST specifity: 0.511111111111111"
[1] "TEST sensitivity: 0.965156794425087"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 4.32469663222631"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

1065 samples
 652 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 853, 852, 851, 852, 852 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens       Spec     
  0.3  1          0.6               0.50        50      0.9424306  0.5659619  0.9822764
  0.3  1          0.6               0.50       100      0.9568815  0.6890063  0.9834528
  0.3  1          0.6               0.50       150      0.9595819  0.7208245  0.9775426
  0.3  1          0.6               0.75        50      0.9383026  0.5336152  0.9822624
  0.3  1          0.6               0.75       100      0.9591404  0.6660677  0.9810790
  0.3  1          0.6               0.75       150      0.9623313  0.7163848  0.9822694
  0.3  1          0.6               1.00        50      0.9432979  0.5062368  0.9858127
  0.3  1          0.6               1.00       100      0.9595834  0.6571882  0.9846293
  0.3  1          0.6               1.00       150      0.9651818  0.6982030  0.9822694
  0.3  1          0.8               0.50        50      0.9501436  0.5474630  0.9822764
  0.3  1          0.8               0.50       100      0.9625568  0.7071882  0.9799025
  0.3  1          0.8               0.50       150      0.9631952  0.7346723  0.9799025
  0.3  1          0.8               0.75        50      0.9387078  0.5199789  0.9858197
  0.3  1          0.8               0.75       100      0.9584191  0.6753700  0.9834459
  0.3  1          0.8               0.75       150      0.9651865  0.7301268  0.9775357
  0.3  1          0.8               1.00        50      0.9436911  0.5016913  0.9858127
  0.3  1          0.8               1.00       100      0.9588013  0.6435518  0.9846223
  0.3  1          0.8               1.00       150      0.9640932  0.6937632  0.9822624
  0.3  2          0.6               0.50        50      0.9536700  0.6344609  0.9728019
  0.3  2          0.6               0.50       100      0.9603405  0.7077167  0.9739923
  0.3  2          0.6               0.50       150      0.9585761  0.7072939  0.9704490
  0.3  2          0.6               0.75        50      0.9582262  0.6477801  0.9822624
  0.3  2          0.6               0.75       100      0.9632833  0.7119450  0.9787261
  0.3  2          0.6               0.75       150      0.9642696  0.7209302  0.9799165
  0.3  2          0.6               1.00        50      0.9528124  0.6483087  0.9763522
  0.3  2          0.6               1.00       100      0.9606046  0.7121564  0.9810929
  0.3  2          0.6               1.00       150      0.9620784  0.7530655  0.9763662
  0.3  2          0.8               0.50        50      0.9580371  0.6525370  0.9787121
  0.3  2          0.8               0.50       100      0.9609067  0.7482030  0.9787191
  0.3  2          0.8               0.50       150      0.9617270  0.7709302  0.9739993
  0.3  2          0.8               0.75        50      0.9580946  0.6799154  0.9739854
  0.3  2          0.8               0.75       100      0.9638872  0.7210359  0.9763522
  0.3  2          0.8               0.75       150      0.9626447  0.7483087  0.9763592
  0.3  2          0.8               1.00        50      0.9601531  0.6845666  0.9881726
  0.3  2          0.8               1.00       100      0.9619652  0.7075053  0.9834528
  0.3  2          0.8               1.00       150      0.9650620  0.7486258  0.9822764
  0.3  3          0.6               0.50        50      0.9620470  0.6937632  0.9775287
  0.3  3          0.6               0.50       100      0.9574403  0.7346723  0.9763522
  0.3  3          0.6               0.50       150      0.9548977  0.7346723  0.9739923
  0.3  3          0.6               0.75        50      0.9639116  0.6892178  0.9810999
  0.3  3          0.6               0.75       100      0.9644055  0.7438689  0.9763731
  0.3  3          0.6               0.75       150      0.9607221  0.7394292  0.9740132
  0.3  3          0.6               1.00        50      0.9625813  0.7256871  0.9775496
  0.3  3          0.6               1.00       100      0.9663359  0.7528541  0.9763731
  0.3  3          0.6               1.00       150      0.9616621  0.7573996  0.9751897
  0.3  3          0.8               0.50        50      0.9596339  0.7302326  0.9739923
  0.3  3          0.8               0.50       100      0.9584339  0.7530655  0.9692656
  0.3  3          0.8               0.50       150      0.9569913  0.7576110  0.9692725
  0.3  3          0.8               0.75        50      0.9647342  0.7210359  0.9799025
  0.3  3          0.8               0.75       100      0.9639440  0.7118393  0.9751897
  0.3  3          0.8               0.75       150      0.9597617  0.7209302  0.9763592
  0.3  3          0.8               1.00        50      0.9625880  0.7165962  0.9787261
  0.3  3          0.8               1.00       100      0.9648451  0.7347780  0.9775426
  0.3  3          0.8               1.00       150      0.9601555  0.7575053  0.9740132
  0.4  1          0.6               0.50        50      0.9469445  0.5699789  0.9763592
  0.4  1          0.6               0.50       100      0.9596587  0.7071882  0.9763522
  0.4  1          0.6               0.50       150      0.9557775  0.7208245  0.9728019
  0.4  1          0.6               0.75        50      0.9530199  0.6115222  0.9775426
  0.4  1          0.6               0.75       100      0.9657905  0.7028541  0.9799025
  0.4  1          0.6               0.75       150      0.9652868  0.7621564  0.9822624
  0.4  1          0.6               1.00        50      0.9537265  0.5840381  0.9834459
  0.4  1          0.6               1.00       100      0.9635198  0.6662791  0.9799025
  0.4  1          0.6               1.00       150      0.9670131  0.7437632  0.9810860
  0.4  1          0.8               0.50        50      0.9472605  0.5976744  0.9787121
  0.4  1          0.8               0.50       100      0.9602550  0.6799154  0.9716185
  0.4  1          0.8               0.50       150      0.9624803  0.7530655  0.9775357
  0.4  1          0.8               0.75        50      0.9496205  0.6115222  0.9799025
  0.4  1          0.8               0.75       100      0.9611681  0.7076110  0.9810790
  0.4  1          0.8               0.75       150      0.9665922  0.7437632  0.9810860
  0.4  1          0.8               1.00        50      0.9522987  0.5841438  0.9846293
  0.4  1          0.8               1.00       100      0.9626428  0.6800211  0.9810790
  0.4  1          0.8               1.00       150      0.9670949  0.7300211  0.9799025
  0.4  2          0.6               0.50        50      0.9626723  0.6938689  0.9834459
  0.4  2          0.6               0.50       100      0.9577011  0.7118393  0.9775357
  0.4  2          0.6               0.50       150      0.9593025  0.7483087  0.9751827
  0.4  2          0.6               0.75        50      0.9594264  0.6892178  0.9858127
  0.4  2          0.6               0.75       100      0.9584741  0.7347780  0.9834528
  0.4  2          0.6               0.75       150      0.9587628  0.7438689  0.9787330
  0.4  2          0.6               1.00        50      0.9620214  0.7027484  0.9799095
  0.4  2          0.6               1.00       100      0.9644052  0.7255814  0.9787261
  0.4  2          0.6               1.00       150      0.9634013  0.7484144  0.9787261
  0.4  2          0.8               0.50        50      0.9525771  0.6480973  0.9775496
  0.4  2          0.8               0.50       100      0.9566671  0.7255814  0.9763662
  0.4  2          0.8               0.50       150      0.9511658  0.7119450  0.9692725
  0.4  2          0.8               0.75        50      0.9618753  0.6983087  0.9810790
  0.4  2          0.8               0.75       100      0.9649496  0.7348837  0.9775426
  0.4  2          0.8               0.75       150      0.9634785  0.7575053  0.9739923
  0.4  2          0.8               1.00        50      0.9626088  0.7119450  0.9846363
  0.4  2          0.8               1.00       100      0.9662795  0.7302326  0.9822624
  0.4  2          0.8               1.00       150      0.9642858  0.7483087  0.9787191
  0.4  3          0.6               0.50        50      0.9596837  0.7346723  0.9763592
  0.4  3          0.6               0.50       100      0.9539689  0.7529598  0.9716464
  0.4  3          0.6               0.50       150      0.9494534  0.7391121  0.9704629
  0.4  3          0.6               0.75        50      0.9662492  0.6984144  0.9787261
  0.4  3          0.6               0.75       100      0.9630048  0.7620507  0.9704560
  0.4  3          0.6               0.75       150      0.9591619  0.7712474  0.9716394
  0.4  3          0.6               1.00        50      0.9602690  0.7347780  0.9822624
  0.4  3          0.6               1.00       100      0.9604969  0.7163848  0.9751688
  0.4  3          0.6               1.00       150      0.9572694  0.7345666  0.9692725
  0.4  3          0.8               0.50        50      0.9485581  0.6755814  0.9716324
  0.4  3          0.8               0.50       100      0.9460496  0.7393235  0.9728159
  0.4  3          0.8               0.50       150      0.9431205  0.7438689  0.9692795
  0.4  3          0.8               0.75        50      0.9567319  0.7028541  0.9751967
  0.4  3          0.8               0.75       100      0.9567640  0.7304440  0.9716394
  0.4  3          0.8               0.75       150      0.9520082  0.7393235  0.9704629
  0.4  3          0.8               1.00        50      0.9639402  0.7029598  0.9834459
  0.4  3          0.8               1.00       100      0.9622986  0.7390063  0.9775496
  0.4  3          0.8               1.00       150      0.9596616  0.7255814  0.9728228

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 150, max_depth = 1, eta = 0.4, gamma = 0, colsample_bytree = 0.8, min_child_weight = 1 and subsample = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     15.0      1.6
  positive      5.5     77.8
                            
 Accuracy (average) : 0.9286

[1] "TRAIN accuracy: 0.928638497652582"
[1] "TRAIN +precision: 0.933558558558559"
[1] "TRAIN -precision: 0.903954802259887"
[1] "TRAIN specifity: 0.730593607305936"
[1] "TRAIN sensitivity: 0.979905437352246"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       31        4
            positive       14      283
[1] "TEST accuracy: 0.94578313253012"
[1] "TEST +precision: 0.952861952861953"
[1] "TEST -precision: 0.885714285714286"
[1] "TEST specifity: 0.688888888888889"
[1] "TEST sensitivity: 0.986062717770035"

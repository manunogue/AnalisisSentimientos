[1] "DATASET NAME: Teresa_Bi_IR_1"
[1] "TRAIN INSTANCES: 2170"
[1] "TEST INSTANCES: 379"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 5.88538813591003"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

2170 samples
 671 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1736, 1736, 1736, 1736, 1736 
Resampling results:

  ROC        Sens       Spec     
  0.9973624  0.9824885  0.9889401

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     49.1      0.6
  positive      0.9     49.4
                            
 Accuracy (average) : 0.9857

[1] "TRAIN accuracy: 0.985714285714286"
[1] "TRAIN +precision: 0.982600732600733"
[1] "TRAIN -precision: 0.98886827458256"
[1] "TRAIN specifity: 0.982488479262673"
[1] "TRAIN sensitivity: 0.988940092165899"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        6        5
            positive       11      357
[1] "TEST accuracy: 0.95778364116095"
[1] "TEST +precision: 0.970108695652174"
[1] "TEST -precision: 0.545454545454545"
[1] "TEST specifity: 0.352941176470588"
[1] "TEST sensitivity: 0.986187845303867"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 3.05723303159078"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

2170 samples
 671 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1736, 1736, 1736, 1736, 1736 
Resampling results across tuning parameters:

  C      M  ROC        Sens       Spec     
  0.010  1  0.9384124  0.8285714  0.9852535
  0.010  2  0.9384272  0.8285714  0.9843318
  0.010  3  0.9392470  0.8285714  0.9861751
  0.255  1  0.9380025  0.8285714  0.9889401
  0.255  2  0.9380110  0.8285714  0.9880184
  0.255  3  0.9388201  0.8285714  0.9898618
  0.500  1  0.9487354  0.8285714  0.9907834
  0.500  2  0.9492387  0.8285714  0.9898618
  0.500  3  0.9506530  0.8285714  0.9917051

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.5 and M = 3.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     41.4      0.4
  positive      8.6     49.6
                            
 Accuracy (average) : 0.9101

[1] "TRAIN accuracy: 0.910138248847926"
[1] "TRAIN +precision: 0.852614896988906"
[1] "TRAIN -precision: 0.990088105726872"
[1] "TRAIN specifity: 0.828571428571429"
[1] "TRAIN sensitivity: 0.991705069124424"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        5        6
            positive       12      356
[1] "TEST accuracy: 0.952506596306069"
[1] "TEST +precision: 0.967391304347826"
[1] "TEST -precision: 0.454545454545455"
[1] "TEST specifity: 0.294117647058824"
[1] "TEST sensitivity: 0.983425414364641"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 6.60639050006866"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

2170 samples
 671 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1736, 1736, 1736, 1736, 1736 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens       Spec     
  0.3  1          0.6               0.50        50      0.9569963  0.8433180  0.9732719
  0.3  1          0.6               0.50       100      0.9847310  0.9161290  0.9806452
  0.3  1          0.6               0.50       150      0.9911508  0.9566820  0.9806452
  0.3  1          0.6               0.75        50      0.9513963  0.8626728  0.9741935
  0.3  1          0.6               0.75       100      0.9811294  0.9244240  0.9815668
  0.3  1          0.6               0.75       150      0.9888488  0.9483871  0.9788018
  0.3  1          0.6               1.00        50      0.9509758  0.8709677  0.9788018
  0.3  1          0.6               1.00       100      0.9820361  0.9271889  0.9797235
  0.3  1          0.6               1.00       150      0.9890887  0.9529954  0.9834101
  0.3  1          0.8               0.50        50      0.9456603  0.8414747  0.9493088
  0.3  1          0.8               0.50       100      0.9816815  0.9152074  0.9741935
  0.3  1          0.8               0.50       150      0.9899318  0.9576037  0.9769585
  0.3  1          0.8               0.75        50      0.9571747  0.8645161  0.9788018
  0.3  1          0.8               0.75       100      0.9842192  0.9225806  0.9824885
  0.3  1          0.8               0.75       150      0.9896876  0.9520737  0.9806452
  0.3  1          0.8               1.00        50      0.9529126  0.8774194  0.9824885
  0.3  1          0.8               1.00       100      0.9836501  0.9244240  0.9843318
  0.3  1          0.8               1.00       150      0.9891801  0.9493088  0.9843318
  0.3  2          0.6               0.50        50      0.9847693  0.9105991  0.9751152
  0.3  2          0.6               0.50       100      0.9926395  0.9658986  0.9824885
  0.3  2          0.6               0.50       150      0.9940474  0.9769585  0.9815668
  0.3  2          0.6               0.75        50      0.9869078  0.9216590  0.9852535
  0.3  2          0.6               0.75       100      0.9928306  0.9640553  0.9815668
  0.3  2          0.6               0.75       150      0.9937777  0.9824885  0.9843318
  0.3  2          0.6               1.00        50      0.9841194  0.9188940  0.9852535
  0.3  2          0.6               1.00       100      0.9933254  0.9594470  0.9834101
  0.3  2          0.6               1.00       150      0.9942662  0.9769585  0.9852535
  0.3  2          0.8               0.50        50      0.9841555  0.9188940  0.9714286
  0.3  2          0.8               0.50       100      0.9923252  0.9668203  0.9815668
  0.3  2          0.8               0.50       150      0.9934252  0.9824885  0.9815668
  0.3  2          0.8               0.75        50      0.9852428  0.9235023  0.9824885
  0.3  2          0.8               0.75       100      0.9916159  0.9658986  0.9788018
  0.3  2          0.8               0.75       150      0.9936057  0.9769585  0.9815668
  0.3  2          0.8               1.00        50      0.9855040  0.9161290  0.9834101
  0.3  2          0.8               1.00       100      0.9931661  0.9585253  0.9870968
  0.3  2          0.8               1.00       150      0.9950625  0.9732719  0.9861751
  0.3  3          0.6               0.50        50      0.9931810  0.9622120  0.9797235
  0.3  3          0.6               0.50       100      0.9941133  0.9824885  0.9806452
  0.3  3          0.6               0.50       150      0.9948162  0.9824885  0.9843318
  0.3  3          0.6               0.75        50      0.9915437  0.9502304  0.9834101
  0.3  3          0.6               0.75       100      0.9952409  0.9824885  0.9806452
  0.3  3          0.6               0.75       150      0.9961159  0.9824885  0.9861751
  0.3  3          0.6               1.00        50      0.9914205  0.9566820  0.9870968
  0.3  3          0.6               1.00       100      0.9946484  0.9769585  0.9861751
  0.3  3          0.6               1.00       150      0.9959566  0.9824885  0.9880184
  0.3  3          0.8               0.50        50      0.9901591  0.9502304  0.9778802
  0.3  3          0.8               0.50       100      0.9939880  0.9824885  0.9788018
  0.3  3          0.8               0.50       150      0.9948565  0.9824885  0.9824885
  0.3  3          0.8               0.75        50      0.9905201  0.9585253  0.9815668
  0.3  3          0.8               0.75       100      0.9936992  0.9824885  0.9861751
  0.3  3          0.8               0.75       150      0.9949818  0.9824885  0.9861751
  0.3  3          0.8               1.00        50      0.9914417  0.9502304  0.9843318
  0.3  3          0.8               1.00       100      0.9944892  0.9824885  0.9870968
  0.3  3          0.8               1.00       150      0.9960479  0.9824885  0.9880184
  0.4  1          0.6               0.50        50      0.9634755  0.8718894  0.9741935
  0.4  1          0.6               0.50       100      0.9871732  0.9373272  0.9788018
  0.4  1          0.6               0.50       150      0.9917921  0.9732719  0.9769585
  0.4  1          0.6               0.75        50      0.9684534  0.8940092  0.9760369
  0.4  1          0.6               0.75       100      0.9893521  0.9373272  0.9788018
  0.4  1          0.6               0.75       150      0.9921064  0.9732719  0.9824885
  0.4  1          0.6               1.00        50      0.9637644  0.8949309  0.9788018
  0.4  1          0.6               1.00       100      0.9891333  0.9391705  0.9852535
  0.4  1          0.6               1.00       150      0.9923953  0.9686636  0.9834101
  0.4  1          0.8               0.50        50      0.9653422  0.8921659  0.9649770
  0.4  1          0.8               0.50       100      0.9878549  0.9437788  0.9788018
  0.4  1          0.8               0.50       150      0.9911805  0.9732719  0.9788018
  0.4  1          0.8               0.75        50      0.9663955  0.8976959  0.9705069
  0.4  1          0.8               0.75       100      0.9886534  0.9345622  0.9824885
  0.4  1          0.8               0.75       150      0.9911550  0.9723502  0.9788018
  0.4  1          0.8               1.00        50      0.9641827  0.9059908  0.9797235
  0.4  1          0.8               1.00       100      0.9887936  0.9382488  0.9824885
  0.4  1          0.8               1.00       150      0.9913398  0.9686636  0.9843318
  0.4  2          0.6               0.50        50      0.9908195  0.9400922  0.9797235
  0.4  2          0.6               0.50       100      0.9923655  0.9741935  0.9797235
  0.4  2          0.6               0.50       150      0.9937926  0.9824885  0.9843318
  0.4  2          0.6               0.75        50      0.9914290  0.9428571  0.9843318
  0.4  2          0.6               0.75       100      0.9938627  0.9824885  0.9834101
  0.4  2          0.6               0.75       150      0.9948948  0.9824885  0.9834101
  0.4  2          0.6               1.00        50      0.9907261  0.9410138  0.9852535
  0.4  2          0.6               1.00       100      0.9943575  0.9741935  0.9852535
  0.4  2          0.6               1.00       150      0.9959757  0.9824885  0.9889401
  0.4  2          0.8               0.50        50      0.9884963  0.9465438  0.9778802
  0.4  2          0.8               0.50       100      0.9921425  0.9769585  0.9806452
  0.4  2          0.8               0.50       150      0.9941451  0.9824885  0.9834101
  0.4  2          0.8               0.75        50      0.9896133  0.9465438  0.9797235
  0.4  2          0.8               0.75       100      0.9927244  0.9824885  0.9824885
  0.4  2          0.8               0.75       150      0.9938988  0.9824885  0.9852535
  0.4  2          0.8               1.00        50      0.9904203  0.9400922  0.9861751
  0.4  2          0.8               1.00       100      0.9942407  0.9705069  0.9843318
  0.4  2          0.8               1.00       150      0.9954129  0.9824885  0.9861751
  0.4  3          0.6               0.50        50      0.9933020  0.9677419  0.9834101
  0.4  3          0.6               0.50       100      0.9943023  0.9824885  0.9843318
  0.4  3          0.6               0.50       150      0.9947907  0.9824885  0.9870968
  0.4  3          0.6               0.75        50      0.9926437  0.9732719  0.9834101
  0.4  3          0.6               0.75       100      0.9951454  0.9824885  0.9889401
  0.4  3          0.6               0.75       150      0.9957039  0.9824885  0.9889401
  0.4  3          0.6               1.00        50      0.9928986  0.9705069  0.9861751
  0.4  3          0.6               1.00       100      0.9952579  0.9824885  0.9880184
  0.4  3          0.6               1.00       150      0.9967211  0.9824885  0.9880184
  0.4  3          0.8               0.50        50      0.9937077  0.9705069  0.9834101
  0.4  3          0.8               0.50       100      0.9944403  0.9824885  0.9815668
  0.4  3          0.8               0.50       150      0.9952069  0.9824885  0.9824885
  0.4  3          0.8               0.75        50      0.9924186  0.9741935  0.9834101
  0.4  3          0.8               0.75       100      0.9943575  0.9824885  0.9870968
  0.4  3          0.8               0.75       150      0.9952749  0.9824885  0.9861751
  0.4  3          0.8               1.00        50      0.9938627  0.9695853  0.9815668
  0.4  3          0.8               1.00       100      0.9956784  0.9824885  0.9870968
  0.4  3          0.8               1.00       150      0.9966298  0.9824885  0.9870968

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 150, max_depth = 3, eta = 0.4, gamma = 0, colsample_bytree = 0.6, min_child_weight = 1 and subsample = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     49.1      0.6
  positive      0.9     49.4
                            
 Accuracy (average) : 0.9853

[1] "TRAIN accuracy: 0.985253456221198"
[1] "TRAIN +precision: 0.982584784601283"
[1] "TRAIN -precision: 0.987951807228916"
[1] "TRAIN specifity: 0.982488479262673"
[1] "TRAIN sensitivity: 0.988018433179723"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        5        8
            positive       12      354
[1] "TEST accuracy: 0.947229551451187"
[1] "TEST +precision: 0.967213114754098"
[1] "TEST -precision: 0.384615384615385"
[1] "TEST specifity: 0.294117647058824"
[1] "TEST sensitivity: 0.977900552486188"

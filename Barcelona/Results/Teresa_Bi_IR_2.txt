[1] "DATASET NAME: Teresa_Bi_IR_2"
[1] "TRAIN INSTANCES: 1653"
[1] "TEST INSTANCES: 379"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 4.20460104942322"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

1653 samples
 671 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1323, 1323, 1322, 1322, 1322 
Resampling results:

  ROC        Sens       Spec     
  0.9969394  0.9789008  0.9935484

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     33.6      0.4
  positive      0.7     65.2
                            
 Accuracy (average) : 0.9885

[1] "TRAIN accuracy: 0.988505747126437"
[1] "TRAIN +precision: 0.988990825688073"
[1] "TRAIN -precision: 0.987566607460036"
[1] "TRAIN specifity: 0.97887323943662"
[1] "TRAIN sensitivity: 0.993548387096774"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        6        5
            positive       11      357
[1] "TEST accuracy: 0.95778364116095"
[1] "TEST +precision: 0.970108695652174"
[1] "TEST -precision: 0.545454545454545"
[1] "TEST specifity: 0.352941176470588"
[1] "TEST sensitivity: 0.986187845303867"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 2.01350378592809"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

1653 samples
 671 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1322, 1323, 1323, 1322, 1322 
Resampling results across tuning parameters:

  C      M  ROC        Sens       Spec     
  0.010  1  0.8837925  0.6250272  0.9751152
  0.010  2  0.8858395  0.6250272  0.9751152
  0.010  3  0.8881950  0.6162552  0.9852535
  0.255  1  0.8931973  0.6373544  0.9806452
  0.255  2  0.8960408  0.6373544  0.9824885
  0.255  3  0.8929650  0.6285359  0.9861751
  0.500  1  0.8967435  0.6373544  0.9815668
  0.500  2  0.8992997  0.6373544  0.9834101
  0.500  3  0.8974140  0.6285359  0.9861751

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.5 and M = 2.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     21.9      1.1
  positive     12.5     64.5
                            
 Accuracy (average) : 0.8645

[1] "TRAIN accuracy: 0.864488808227465"
[1] "TRAIN +precision: 0.838177533385703"
[1] "TRAIN -precision: 0.952631578947368"
[1] "TRAIN specifity: 0.637323943661972"
[1] "TRAIN sensitivity: 0.983410138248848"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        4        4
            positive       13      358
[1] "TEST accuracy: 0.955145118733509"
[1] "TEST +precision: 0.964959568733154"
[1] "TEST -precision: 0.5"
[1] "TEST specifity: 0.235294117647059"
[1] "TEST sensitivity: 0.988950276243094"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 5.12883806626002"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

1653 samples
 671 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 1322, 1322, 1323, 1323, 1322 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens       Spec     
  0.3  1          0.6               0.50        50      0.9258066  0.5614501  0.9963134
  0.3  1          0.6               0.50       100      0.9677756  0.7128241  0.9926267
  0.3  1          0.6               0.50       150      0.9848779  0.8397609  0.9889401
  0.3  1          0.6               0.75        50      0.9290235  0.5527247  0.9917051
  0.3  1          0.6               0.75       100      0.9655547  0.7181959  0.9870968
  0.3  1          0.6               0.75       150      0.9847579  0.8925943  0.9880184
  0.3  1          0.6               1.00        50      0.9255247  0.5703462  0.9935484
  0.3  1          0.6               1.00       100      0.9668254  0.7428194  0.9880184
  0.3  1          0.6               1.00       150      0.9799840  0.8519950  0.9889401
  0.3  1          0.8               0.50        50      0.9299565  0.5333799  0.9917051
  0.3  1          0.8               0.50       100      0.9652583  0.7394038  0.9907834
  0.3  1          0.8               0.50       150      0.9841013  0.8362211  0.9852535
  0.3  1          0.8               0.75        50      0.9240292  0.5597112  0.9917051
  0.3  1          0.8               0.75       100      0.9626346  0.7622108  0.9870968
  0.3  1          0.8               0.75       150      0.9841838  0.8890390  0.9852535
  0.3  1          0.8               1.00        50      0.9248774  0.5632821  0.9907834
  0.3  1          0.8               1.00       100      0.9652000  0.7392951  0.9880184
  0.3  1          0.8               1.00       150      0.9821095  0.8484863  0.9880184
  0.3  2          0.6               0.50        50      0.9732154  0.7252445  0.9898618
  0.3  2          0.6               0.50       100      0.9837212  0.8836982  0.9880184
  0.3  2          0.6               0.50       150      0.9887852  0.9366247  0.9852535
  0.3  2          0.6               0.75        50      0.9721893  0.7200124  0.9898618
  0.3  2          0.6               0.75       100      0.9889868  0.9049526  0.9898618
  0.3  2          0.6               0.75       150      0.9919105  0.9331470  0.9917051
  0.3  2          0.6               1.00        50      0.9669364  0.7393728  0.9898618
  0.3  2          0.6               1.00       100      0.9890239  0.9031827  0.9889401
  0.3  2          0.6               1.00       150      0.9918367  0.9313461  0.9907834
  0.3  2          0.8               0.50        50      0.9670419  0.7516069  0.9889401
  0.3  2          0.8               0.50       100      0.9877634  0.8907934  0.9889401
  0.3  2          0.8               0.50       150      0.9909625  0.9277752  0.9880184
  0.3  2          0.8               0.75        50      0.9616057  0.7498991  0.9870968
  0.3  2          0.8               0.75       100      0.9863029  0.9066915  0.9870968
  0.3  2          0.8               0.75       150      0.9900818  0.9348704  0.9870968
  0.3  2          0.8               1.00        50      0.9670889  0.7358640  0.9907834
  0.3  2          0.8               1.00       100      0.9894539  0.9031827  0.9880184
  0.3  2          0.8               1.00       150      0.9916069  0.9331160  0.9889401
  0.3  3          0.6               0.50        50      0.9820759  0.8431455  0.9907834
  0.3  3          0.6               0.50       100      0.9883632  0.9189412  0.9907834
  0.3  3          0.6               0.50       150      0.9899251  0.9365782  0.9880184
  0.3  3          0.6               0.75        50      0.9839319  0.8485328  0.9889401
  0.3  3          0.6               0.75       100      0.9903523  0.9242664  0.9898618
  0.3  3          0.6               0.75       150      0.9918966  0.9489986  0.9907834
  0.3  3          0.6               1.00        50      0.9843168  0.8660456  0.9880184
  0.3  3          0.6               1.00       100      0.9903976  0.9366558  0.9898618
  0.3  3          0.6               1.00       150      0.9926066  0.9525540  0.9907834
  0.3  3          0.8               0.50        50      0.9844048  0.8537028  0.9907834
  0.3  3          0.8               0.50       100      0.9892910  0.9260208  0.9889401
  0.3  3          0.8               0.50       150      0.9912753  0.9401180  0.9880184
  0.3  3          0.8               0.75        50      0.9847556  0.8819748  0.9880184
  0.3  3          0.8               0.75       100      0.9920967  0.9472287  0.9870968
  0.3  3          0.8               0.75       150      0.9936006  0.9436889  0.9907834
  0.3  3          0.8               1.00        50      0.9854178  0.8643378  0.9880184
  0.3  3          0.8               1.00       100      0.9905351  0.9366247  0.9889401
  0.3  3          0.8               1.00       150      0.9930662  0.9490141  0.9889401
  0.4  1          0.6               0.50        50      0.9398475  0.6371371  0.9870968
  0.4  1          0.6               0.50       100      0.9764164  0.8220307  0.9870968
  0.4  1          0.6               0.50       150      0.9851928  0.8890390  0.9852535
  0.4  1          0.6               0.75        50      0.9400189  0.6319205  0.9880184
  0.4  1          0.6               0.75       100      0.9793556  0.8344046  0.9843318
  0.4  1          0.6               0.75       150      0.9871114  0.9137091  0.9870968
  0.4  1          0.6               1.00        50      0.9398203  0.6494799  0.9870968
  0.4  1          0.6               1.00       100      0.9786928  0.8256171  0.9880184
  0.4  1          0.6               1.00       150      0.9872804  0.8907934  0.9917051
  0.4  1          0.8               0.50        50      0.9390572  0.6266884  0.9861751
  0.4  1          0.8               0.50       100      0.9811211  0.8114889  0.9834101
  0.4  1          0.8               0.50       150      0.9866301  0.9066449  0.9861751
  0.4  1          0.8               0.75        50      0.9495742  0.6196398  0.9880184
  0.4  1          0.8               0.75       100      0.9807780  0.8608291  0.9870968
  0.4  1          0.8               0.75       150      0.9867218  0.9137091  0.9889401
  0.4  1          0.8               1.00        50      0.9437598  0.6441702  0.9861751
  0.4  1          0.8               1.00       100      0.9774797  0.8255706  0.9889401
  0.4  1          0.8               1.00       150      0.9872307  0.9031827  0.9898618
  0.4  2          0.6               0.50        50      0.9741229  0.8025617  0.9861751
  0.4  2          0.6               0.50       100      0.9869462  0.9136625  0.9861751
  0.4  2          0.6               0.50       150      0.9897866  0.9295141  0.9907834
  0.4  2          0.6               0.75        50      0.9829176  0.8467785  0.9898618
  0.4  2          0.6               0.75       100      0.9899715  0.9119702  0.9898618
  0.4  2          0.6               0.75       150      0.9929059  0.9436733  0.9889401
  0.4  2          0.6               1.00        50      0.9760738  0.8396367  0.9917051
  0.4  2          0.6               1.00       100      0.9887832  0.9102158  0.9880184
  0.4  2          0.6               1.00       150      0.9918540  0.9490141  0.9898618
  0.4  2          0.8               0.50        50      0.9759764  0.8432231  0.9870968
  0.4  2          0.8               0.50       100      0.9869262  0.9189412  0.9870968
  0.4  2          0.8               0.50       150      0.9894871  0.9401180  0.9870968
  0.4  2          0.8               0.75        50      0.9812643  0.8519485  0.9880184
  0.4  2          0.8               0.75       100      0.9906904  0.9207421  0.9861751
  0.4  2          0.8               0.75       150      0.9930509  0.9366092  0.9898618
  0.4  2          0.8               1.00        50      0.9790149  0.8150753  0.9889401
  0.4  2          0.8               1.00       100      0.9889498  0.9296072  0.9898618
  0.4  2          0.8               1.00       150      0.9924424  0.9437044  0.9907834
  0.4  3          0.6               0.50        50      0.9864166  0.8642757  0.9870968
  0.4  3          0.6               0.50       100      0.9910350  0.9366092  0.9843318
  0.4  3          0.6               0.50       150      0.9910100  0.9542773  0.9852535
  0.4  3          0.6               0.75        50      0.9856539  0.8978730  0.9870968
  0.4  3          0.6               0.75       100      0.9916221  0.9401646  0.9870968
  0.4  3          0.6               0.75       150      0.9922262  0.9578171  0.9880184
  0.4  3          0.6               1.00        50      0.9877903  0.9049371  0.9898618
  0.4  3          0.6               1.00       100      0.9922777  0.9507840  0.9917051
  0.4  3          0.6               1.00       150      0.9925052  0.9543083  0.9917051
  0.4  3          0.8               0.50        50      0.9862116  0.9048750  0.9889401
  0.4  3          0.8               0.50       100      0.9914194  0.9401180  0.9870968
  0.4  3          0.8               0.50       150      0.9913509  0.9507375  0.9852535
  0.4  3          0.8               0.75        50      0.9855597  0.9137246  0.9870968
  0.4  3          0.8               0.75       100      0.9918163  0.9436889  0.9880184
  0.4  3          0.8               0.75       150      0.9929032  0.9560627  0.9889401
  0.4  3          0.8               1.00        50      0.9898441  0.9067070  0.9917051
  0.4  3          0.8               1.00       100      0.9938109  0.9401646  0.9926267
  0.4  3          0.8               1.00       150      0.9943451  0.9525540  0.9917051

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 150, max_depth = 3, eta = 0.4, gamma = 0, colsample_bytree = 0.8, min_child_weight = 1 and subsample = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     32.7      0.5
  positive      1.6     65.1
                            
 Accuracy (average) : 0.9782

[1] "TRAIN accuracy: 0.978221415607985"
[1] "TRAIN +precision: 0.975521305530372"
[1] "TRAIN -precision: 0.983636363636364"
[1] "TRAIN specifity: 0.952464788732394"
[1] "TRAIN sensitivity: 0.991705069124424"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative        4        6
            positive       13      356
[1] "TEST accuracy: 0.949868073878628"
[1] "TEST +precision: 0.964769647696477"
[1] "TEST -precision: 0.4"
[1] "TEST specifity: 0.235294117647059"
[1] "TEST sensitivity: 0.983425414364641"

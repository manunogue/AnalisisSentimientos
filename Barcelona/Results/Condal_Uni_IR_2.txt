[1] "DATASET NAME: Condal_Uni_IR_2"
[1] "TRAIN INSTANCES: 3463"
[1] "TEST INSTANCES: 795"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 11.1125631332397"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

3463 samples
 652 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 2770, 2771, 2770, 2771, 2770 
Resampling results:

  ROC        Sens  Spec     
  0.9990295  1     0.9929564

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     34.4      0.5
  positive      0.0     65.1
                            
 Accuracy (average) : 0.9954

[1] "TRAIN accuracy: 0.995379728559053"
[1] "TRAIN +precision: 1"
[1] "TRAIN -precision: 0.986754966887417"
[1] "TRAIN specifity: 1"
[1] "TRAIN sensitivity: 0.992954645530603"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       20        4
            positive       19      752
[1] "TEST accuracy: 0.971069182389937"
[1] "TEST +precision: 0.975356679636835"
[1] "TEST -precision: 0.833333333333333"
[1] "TEST specifity: 0.512820512820513"
[1] "TEST sensitivity: 0.994708994708995"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 5.76859395106634"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

3463 samples
 652 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 2770, 2769, 2771, 2771, 2771 
Resampling results across tuning parameters:

  C      M  ROC        Sens       Spec     
  0.010  1  0.9780283  0.9823986  0.9625715
  0.010  2  0.9797331  0.9823986  0.9621320
  0.010  3  0.9814432  0.9823986  0.9608104
  0.255  1  0.9830673  0.9974860  0.9696190
  0.255  2  0.9847785  0.9974860  0.9691795
  0.255  3  0.9881906  0.9974860  0.9652157
  0.500  1  0.9835765  0.9974860  0.9700595
  0.500  2  0.9851665  0.9974860  0.9696200
  0.500  3  0.9883313  0.9974860  0.9652157

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.5 and M = 3.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     34.3      2.3
  positive      0.1     63.3
                            
 Accuracy (average) : 0.9763

[1] "TRAIN accuracy: 0.976321108865146"
[1] "TRAIN +precision: 0.998633257403189"
[1] "TRAIN -precision: 0.937697160883281"
[1] "TRAIN specifity: 0.99748322147651"
[1] "TRAIN sensitivity: 0.965213562307354"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       16       17
            positive       23      739
[1] "TEST accuracy: 0.949685534591195"
[1] "TEST +precision: 0.969816272965879"
[1] "TEST -precision: 0.484848484848485"
[1] "TEST specifity: 0.41025641025641"
[1] "TEST sensitivity: 0.977513227513228"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 10.9071273843447"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

3463 samples
 652 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 2770, 2771, 2771, 2770, 2770 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens       Spec     
  0.3  1          0.6               0.50        50      0.9766872  0.8012236  0.9757816
  0.3  1          0.6               0.50       100      0.9904676  0.9345733  0.9797444
  0.3  1          0.6               0.50       150      0.9954807  0.9689568  0.9793039
  0.3  1          0.6               0.75        50      0.9752176  0.8020710  0.9784247
  0.3  1          0.6               0.75       100      0.9908326  0.9345733  0.9810670
  0.3  1          0.6               0.75       150      0.9955303  0.9588903  0.9823856
  0.3  1          0.6               1.00        50      0.9767375  0.7995324  0.9793058
  0.3  1          0.6               1.00       100      0.9908579  0.9320699  0.9815065
  0.3  1          0.6               1.00       150      0.9957745  0.9689568  0.9845892
  0.3  1          0.8               0.50        50      0.9753588  0.8045990  0.9718168
  0.3  1          0.8               0.50       100      0.9911741  0.9312366  0.9766617
  0.3  1          0.8               0.50       150      0.9953605  0.9672831  0.9810640
  0.3  1          0.8               0.75        50      0.9772325  0.7927956  0.9797434
  0.3  1          0.8               0.75       100      0.9910108  0.9404311  0.9806255
  0.3  1          0.8               0.75       150      0.9951572  0.9630920  0.9819480
  0.3  1          0.8               1.00        50      0.9780310  0.8011955  0.9801869
  0.3  1          0.8               1.00       100      0.9907595  0.9370908  0.9819470
  0.3  1          0.8               1.00       150      0.9957739  0.9655919  0.9854693
  0.3  2          0.6               0.50        50      0.9931497  0.9563693  0.9748986
  0.3  2          0.6               0.50       100      0.9974796  0.9907739  0.9819451
  0.3  2          0.6               0.50       150      0.9981646  0.9991632  0.9867890
  0.3  2          0.6               0.75        50      0.9946369  0.9521782  0.9784257
  0.3  2          0.6               0.75       100      0.9979489  0.9857389  0.9854703
  0.3  2          0.6               0.75       150      0.9985605  0.9991632  0.9876720
  0.3  2          0.6               1.00        50      0.9940478  0.9563764  0.9784238
  0.3  2          0.6               1.00       100      0.9978453  0.9832179  0.9872334
  0.3  2          0.6               1.00       150      0.9982293  0.9983193  0.9872314
  0.3  2          0.8               0.50        50      0.9942488  0.9555431  0.9766646
  0.3  2          0.8               0.50       100      0.9978494  0.9916107  0.9837092
  0.3  2          0.8               0.50       150      0.9987032  0.9983228  0.9867929
  0.3  2          0.8               0.75        50      0.9946639  0.9505010  0.9749005
  0.3  2          0.8               0.75       100      0.9982298  0.9874161  0.9841487
  0.3  2          0.8               0.75       150      0.9986932  0.9966492  0.9885521
  0.3  2          0.8               1.00        50      0.9947205  0.9605886  0.9810679
  0.3  2          0.8               1.00       100      0.9977621  0.9857283  0.9872324
  0.3  2          0.8               1.00       150      0.9983957  0.9974825  0.9872314
  0.3  3          0.6               0.50        50      0.9978896  0.9882529  0.9837101
  0.3  3          0.6               0.50       100      0.9986100  1.0000000  0.9859099
  0.3  3          0.6               0.50       150      0.9989741  1.0000000  0.9889926
  0.3  3          0.6               0.75        50      0.9972619  0.9899195  0.9806274
  0.3  3          0.6               0.75       100      0.9982943  1.0000000  0.9889907
  0.3  3          0.6               0.75       150      0.9988116  1.0000000  0.9885501
  0.3  3          0.6               1.00        50      0.9983296  0.9865617  0.9863485
  0.3  3          0.6               1.00       100      0.9990499  0.9991632  0.9903122
  0.3  3          0.6               1.00       150      0.9992031  1.0000000  0.9903103
  0.3  3          0.8               0.50        50      0.9971692  0.9932773  0.9810650
  0.3  3          0.8               0.50       100      0.9986110  1.0000000  0.9881115
  0.3  3          0.8               0.50       150      0.9985148  1.0000000  0.9872314
  0.3  3          0.8               0.75        50      0.9977676  0.9916037  0.9815055
  0.3  3          0.8               0.75       100      0.9988710  1.0000000  0.9872305
  0.3  3          0.8               0.75       150      0.9988749  1.0000000  0.9911933
  0.3  3          0.8               1.00        50      0.9971405  0.9907598  0.9841487
  0.3  3          0.8               1.00       100      0.9985141  0.9983228  0.9876710
  0.3  3          0.8               1.00       150      0.9987430  1.0000000  0.9911943
  0.4  1          0.6               0.50        50      0.9833858  0.8641293  0.9801840
  0.4  1          0.6               0.50       100      0.9942635  0.9563975  0.9753420
  0.4  1          0.6               0.50       150      0.9964383  0.9723076  0.9801859
  0.4  1          0.6               0.75        50      0.9835039  0.8641187  0.9766588
  0.4  1          0.6               0.75       100      0.9954200  0.9588763  0.9832677
  0.4  1          0.6               0.75       150      0.9971787  0.9723076  0.9823866
  0.4  1          0.6               1.00        50      0.9830677  0.8616258  0.9806235
  0.4  1          0.6               1.00       100      0.9949440  0.9572237  0.9828262
  0.4  1          0.6               1.00       150      0.9968543  0.9706304  0.9859089
  0.4  1          0.8               0.50        50      0.9833673  0.8607679  0.9713773
  0.4  1          0.8               0.50       100      0.9941631  0.9672796  0.9766597
  0.4  1          0.8               0.50       150      0.9964883  0.9739918  0.9810640
  0.4  1          0.8               0.75        50      0.9825858  0.8632924  0.9797444
  0.4  1          0.8               0.75       100      0.9948951  0.9580500  0.9801840
  0.4  1          0.8               0.75       150      0.9968464  0.9697936  0.9845873
  0.4  1          0.8               1.00        50      0.9836312  0.8683485  0.9832657
  0.4  1          0.8               1.00       100      0.9945629  0.9580605  0.9815055
  0.4  1          0.8               1.00       150      0.9967550  0.9714743  0.9845873
  0.4  2          0.6               0.50        50      0.9954551  0.9656201  0.9740214
  0.4  2          0.6               0.50       100      0.9979485  0.9974825  0.9854693
  0.4  2          0.6               0.50       150      0.9981132  1.0000000  0.9889907
  0.4  2          0.6               0.75        50      0.9964990  0.9798530  0.9788624
  0.4  2          0.6               0.75       100      0.9983861  0.9983228  0.9872276
  0.4  2          0.6               0.75       150      0.9987372  0.9991597  0.9881096
  0.4  2          0.6               1.00        50      0.9966725  0.9681235  0.9810660
  0.4  2          0.6               1.00       100      0.9982908  0.9966457  0.9867900
  0.4  2          0.6               1.00       150      0.9985417  0.9991597  0.9867890
  0.4  2          0.8               0.50        50      0.9959133  0.9723006  0.9731384
  0.4  2          0.8               0.50       100      0.9978543  0.9949615  0.9815075
  0.4  2          0.8               0.50       150      0.9982646  0.9991632  0.9850298
  0.4  2          0.8               0.75        50      0.9964724  0.9840582  0.9775476
  0.4  2          0.8               0.75       100      0.9984951  0.9983228  0.9863514
  0.4  2          0.8               0.75       150      0.9985676  1.0000000  0.9881125
  0.4  2          0.8               1.00        50      0.9969161  0.9765057  0.9828271
  0.4  2          0.8               1.00       100      0.9984328  0.9924475  0.9889916
  0.4  2          0.8               1.00       150      0.9986820  1.0000000  0.9907528
  0.4  3          0.6               0.50        50      0.9979361  0.9907598  0.9854674
  0.4  3          0.6               0.50       100      0.9984842  1.0000000  0.9876691
  0.4  3          0.6               0.50       150      0.9985530  1.0000000  0.9859079
  0.4  3          0.6               0.75        50      0.9983057  0.9983228  0.9828262
  0.4  3          0.6               0.75       100      0.9990743  1.0000000  0.9898707
  0.4  3          0.6               0.75       150      0.9990060  1.0000000  0.9881115
  0.4  3          0.6               1.00        50      0.9981082  0.9932914  0.9854684
  0.4  3          0.6               1.00       100      0.9986730  1.0000000  0.9894331
  0.4  3          0.6               1.00       150      0.9987324  1.0000000  0.9903132
  0.4  3          0.8               0.50        50      0.9984012  0.9941212  0.9841507
  0.4  3          0.8               0.50       100      0.9991573  1.0000000  0.9889897
  0.4  3          0.8               0.50       150      0.9991332  1.0000000  0.9889897
  0.4  3          0.8               0.75        50      0.9983887  0.9899371  0.9850298
  0.4  3          0.8               0.75       100      0.9986969  1.0000000  0.9894302
  0.4  3          0.8               0.75       150      0.9984382  1.0000000  0.9881106
  0.4  3          0.8               1.00        50      0.9983959  0.9932843  0.9876700
  0.4  3          0.8               1.00       100      0.9988131  1.0000000  0.9903113
  0.4  3          0.8               1.00       150      0.9988489  1.0000000  0.9898707

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 150, max_depth = 3, eta = 0.3, gamma = 0, colsample_bytree = 0.6, min_child_weight = 1 and subsample = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     34.4      0.6
  positive      0.0     64.9
                            
 Accuracy (average) : 0.9936

[1] "TRAIN accuracy: 0.993647126768698"
[1] "TRAIN +precision: 1"
[1] "TRAIN -precision: 0.981878088962109"
[1] "TRAIN specifity: 1"
[1] "TRAIN sensitivity: 0.990312637604579"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       21        8
            positive       18      748
[1] "TEST accuracy: 0.967295597484277"
[1] "TEST +precision: 0.976501305483029"
[1] "TEST -precision: 0.724137931034483"
[1] "TEST specifity: 0.538461538461538"
[1] "TEST sensitivity: 0.989417989417989"

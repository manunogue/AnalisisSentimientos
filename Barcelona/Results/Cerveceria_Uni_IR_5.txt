[1] "DATASET NAME: Cerveceria_Uni_IR_5"
[1] "TRAIN INSTANCES: 3422"
[1] "TEST INSTANCES: 968"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 29.9518389701843"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

3422 samples
 654 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 2737, 2738, 2737, 2738, 2738 
Resampling results:

  ROC        Sens       Spec     
  0.9922527  0.9836263  0.9832727

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     19.3      1.3
  positive      0.3     79.0
                            
 Accuracy (average) : 0.9833

[1] "TRAIN accuracy: 0.983343074225599"
[1] "TRAIN +precision: 0.995948434622468"
[1] "TRAIN -precision: 0.934936350777935"
[1] "TRAIN specifity: 0.983630952380952"
[1] "TRAIN sensitivity: 0.983272727272727"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       22       14
            positive       24      908
[1] "TEST accuracy: 0.960743801652893"
[1] "TEST +precision: 0.974248927038627"
[1] "TEST -precision: 0.611111111111111"
[1] "TEST specifity: 0.478260869565217"
[1] "TEST sensitivity: 0.984815618221258"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 6.54038685162862"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

3422 samples
 654 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 2737, 2737, 2738, 2738, 2738 
Resampling results across tuning parameters:

  C      M  ROC        Sens       Spec     
  0.010  1  0.8838167  0.7826976  0.9687273
  0.010  2  0.8799912  0.7648203  0.9694545
  0.010  3  0.8735969  0.7559204  0.9683636
  0.255  1  0.9512901  0.9225981  0.9643636
  0.255  2  0.9540073  0.9121614  0.9643636
  0.255  3  0.9513911  0.9047098  0.9614545
  0.500  1  0.9599632  0.9374682  0.9647273
  0.500  2  0.9614969  0.9225539  0.9647273
  0.500  3  0.9574532  0.9106578  0.9610909

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.5 and M = 2.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     18.1      2.8
  positive      1.5     77.5
                            
 Accuracy (average) : 0.9565

[1] "TRAIN accuracy: 0.95645821157218"
[1] "TRAIN +precision: 0.980776340110906"
[1] "TRAIN -precision: 0.864714086471409"
[1] "TRAIN specifity: 0.922619047619048"
[1] "TRAIN sensitivity: 0.964727272727273"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       19       16
            positive       27      906
[1] "TEST accuracy: 0.955578512396694"
[1] "TEST +precision: 0.971061093247588"
[1] "TEST -precision: 0.542857142857143"
[1] "TEST specifity: 0.41304347826087"
[1] "TEST sensitivity: 0.982646420824295"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 10.4470070838928"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

3422 samples
 654 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 2737, 2738, 2738, 2737, 2738 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens       Spec     
  0.3  1          0.6               0.50        50      0.9576269  0.5608513  0.9909091
  0.3  1          0.6               0.50       100      0.9785671  0.7082366  0.9872727
  0.3  1          0.6               0.50       150      0.9844557  0.7677501  0.9872727
  0.3  1          0.6               0.75        50      0.9566282  0.5459812  0.9901818
  0.3  1          0.6               0.75       100      0.9782984  0.6829630  0.9880000
  0.3  1          0.6               0.75       150      0.9839760  0.7782753  0.9876364
  0.3  1          0.6               1.00        50      0.9604729  0.5340299  0.9920000
  0.3  1          0.6               1.00       100      0.9776059  0.6740299  0.9898182
  0.3  1          0.6               1.00       150      0.9837078  0.7603206  0.9880000
  0.3  1          0.8               0.50        50      0.9603175  0.5340188  0.9901818
  0.3  1          0.8               0.50       100      0.9786493  0.6739967  0.9869091
  0.3  1          0.8               0.50       150      0.9843436  0.7960531  0.9854545
  0.3  1          0.8               0.75        50      0.9600854  0.5444887  0.9898182
  0.3  1          0.8               0.75       100      0.9780594  0.6800111  0.9887273
  0.3  1          0.8               0.75       150      0.9846827  0.7856164  0.9869091
  0.3  1          0.8               1.00        50      0.9595612  0.5445218  0.9916364
  0.3  1          0.8               1.00       100      0.9773711  0.6710337  0.9898182
  0.3  1          0.8               1.00       150      0.9837630  0.7603538  0.9887273
  0.3  2          0.6               0.50        50      0.9820227  0.7245771  0.9865455
  0.3  2          0.6               0.50       100      0.9905575  0.8481260  0.9872727
  0.3  2          0.6               0.50       150      0.9938394  0.8987617  0.9850909
  0.3  2          0.6               0.75        50      0.9808989  0.7156219  0.9876364
  0.3  2          0.6               0.75       100      0.9908569  0.8421559  0.9869091
  0.3  2          0.6               0.75       150      0.9934655  0.9076396  0.9854545
  0.3  2          0.6               1.00        50      0.9817720  0.7335655  0.9890909
  0.3  2          0.6               1.00       100      0.9906937  0.8585406  0.9876364
  0.3  2          0.6               1.00       150      0.9935914  0.8987507  0.9869091
  0.3  2          0.8               0.50        50      0.9821391  0.7394693  0.9894545
  0.3  2          0.8               0.50       100      0.9915484  0.8615589  0.9858182
  0.3  2          0.8               0.50       150      0.9939889  0.9106247  0.9854545
  0.3  2          0.8               0.75        50      0.9839033  0.7334771  0.9894545
  0.3  2          0.8               0.75       100      0.9912828  0.8601106  0.9887273
  0.3  2          0.8               0.75       150      0.9936409  0.9062134  0.9861818
  0.3  2          0.8               1.00        50      0.9822927  0.7335655  0.9890909
  0.3  2          0.8               1.00       100      0.9908886  0.8466777  0.9883636
  0.3  2          0.8               1.00       150      0.9939998  0.9076838  0.9872727
  0.3  3          0.6               0.50        50      0.9885495  0.8273521  0.9887273
  0.3  3          0.6               0.50       100      0.9948029  0.9211056  0.9887273
  0.3  3          0.6               0.50       150      0.9961045  0.9553455  0.9880000
  0.3  3          0.6               0.75        50      0.9900676  0.8422001  0.9883636
  0.3  3          0.6               0.75       100      0.9950160  0.9255611  0.9880000
  0.3  3          0.6               0.75       150      0.9963615  0.9687562  0.9861818
  0.3  3          0.6               1.00        50      0.9915246  0.8302819  0.9890909
  0.3  3          0.6               1.00       100      0.9955498  0.9181537  0.9880000
  0.3  3          0.6               1.00       150      0.9968864  0.9538751  0.9880000
  0.3  3          0.8               0.50        50      0.9882362  0.8303040  0.9872727
  0.3  3          0.8               0.50       100      0.9944330  0.9166169  0.9847273
  0.3  3          0.8               0.50       150      0.9957986  0.9732007  0.9836364
  0.3  3          0.8               0.75        50      0.9914955  0.8362631  0.9894545
  0.3  3          0.8               0.75       100      0.9955752  0.9225539  0.9894545
  0.3  3          0.8               0.75       150      0.9966035  0.9672637  0.9872727
  0.3  3          0.8               1.00        50      0.9916993  0.8392150  0.9894545
  0.3  3          0.8               1.00       100      0.9954667  0.9240464  0.9883636
  0.3  3          0.8               1.00       150      0.9970419  0.9568159  0.9883636
  0.4  1          0.6               0.50        50      0.9647630  0.6383085  0.9858182
  0.4  1          0.6               0.50       100      0.9809619  0.7648535  0.9829091
  0.4  1          0.6               0.50       150      0.9871102  0.8272858  0.9840000
  0.4  1          0.6               0.75        50      0.9678118  0.6011277  0.9861818
  0.4  1          0.6               0.75       100      0.9830411  0.7380321  0.9880000
  0.4  1          0.6               0.75       150      0.9862898  0.8273079  0.9843636
  0.4  1          0.6               1.00        50      0.9664716  0.6144500  0.9890909
  0.4  1          0.6               1.00       100      0.9830557  0.7379878  0.9887273
  0.4  1          0.6               1.00       150      0.9864600  0.8168712  0.9880000
  0.4  1          0.8               0.50        50      0.9688985  0.6383085  0.9872727
  0.4  1          0.8               0.50       100      0.9838120  0.7782090  0.9858182
  0.4  1          0.8               0.50       150      0.9859373  0.8317745  0.9836364
  0.4  1          0.8               0.75        50      0.9657260  0.6085130  0.9901818
  0.4  1          0.8               0.75       100      0.9820677  0.7469431  0.9869091
  0.4  1          0.8               0.75       150      0.9865280  0.8228082  0.9847273
  0.4  1          0.8               1.00        50      0.9672917  0.6159867  0.9887273
  0.4  1          0.8               1.00       100      0.9834335  0.7395025  0.9883636
  0.4  1          0.8               1.00       150      0.9869873  0.8183527  0.9872727
  0.4  2          0.6               0.50        50      0.9845052  0.8109232  0.9854545
  0.4  2          0.6               0.50       100      0.9917698  0.8897955  0.9847273
  0.4  2          0.6               0.50       150      0.9941775  0.9270426  0.9829091
  0.4  2          0.6               0.75        50      0.9847145  0.7960641  0.9872727
  0.4  2          0.6               0.75       100      0.9925962  0.8942731  0.9847273
  0.4  2          0.6               0.75       150      0.9949245  0.9404422  0.9843636
  0.4  2          0.6               1.00        50      0.9872501  0.8049751  0.9898182
  0.4  2          0.6               1.00       100      0.9927927  0.8927805  0.9876364
  0.4  2          0.6               1.00       150      0.9951064  0.9255500  0.9872727
  0.4  2          0.8               0.50        50      0.9831257  0.8035379  0.9869091
  0.4  2          0.8               0.50       100      0.9925767  0.8838585  0.9843636
  0.4  2          0.8               0.50       150      0.9941362  0.9374682  0.9843636
  0.4  2          0.8               0.75        50      0.9862532  0.8049972  0.9865455
  0.4  2          0.8               0.75       100      0.9938126  0.9017358  0.9880000
  0.4  2          0.8               0.75       150      0.9957340  0.9420011  0.9872727
  0.4  2          0.8               1.00        50      0.9882354  0.8094527  0.9894545
  0.4  2          0.8               1.00       100      0.9937573  0.9002653  0.9876364
  0.4  2          0.8               1.00       150      0.9957119  0.9344942  0.9890909
  0.4  3          0.6               0.50        50      0.9922515  0.8674737  0.9854545
  0.4  3          0.6               0.50       100      0.9944914  0.9449530  0.9850909
  0.4  3          0.6               0.50       150      0.9956569  0.9687120  0.9825455
  0.4  3          0.6               0.75        50      0.9923967  0.8868546  0.9880000
  0.4  3          0.6               0.75       100      0.9958264  0.9597789  0.9883636
  0.4  3          0.6               0.75       150      0.9968014  0.9731786  0.9872727
  0.4  3          0.6               1.00        50      0.9930015  0.8853510  0.9905455
  0.4  3          0.6               1.00       100      0.9963019  0.9523494  0.9876364
  0.4  3          0.6               1.00       150      0.9972384  0.9791487  0.9876364
  0.4  3          0.8               0.50        50      0.9914869  0.8853400  0.9869091
  0.4  3          0.8               0.50       100      0.9957959  0.9448646  0.9865455
  0.4  3          0.8               0.50       150      0.9964495  0.9806412  0.9847273
  0.4  3          0.8               0.75        50      0.9930586  0.8913543  0.9883636
  0.4  3          0.8               0.75       100      0.9955566  0.9582974  0.9869091
  0.4  3          0.8               0.75       150      0.9966731  0.9776562  0.9861818
  0.4  3          0.8               1.00        50      0.9937810  0.8912991  0.9880000
  0.4  3          0.8               1.00       100      0.9966775  0.9553344  0.9869091
  0.4  3          0.8               1.00       150      0.9973614  0.9806412  0.9872727

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 150, max_depth = 3, eta = 0.4, gamma = 0, colsample_bytree = 0.8, min_child_weight = 1 and subsample = 1.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative     19.3      1.0
  positive      0.4     79.3
                           
 Accuracy (average) : 0.986

[1] "TRAIN accuracy: 0.985973115137347"
[1] "TRAIN +precision: 0.995234604105572"
[1] "TRAIN -precision: 0.949567723342939"
[1] "TRAIN specifity: 0.980654761904762"
[1] "TRAIN sensitivity: 0.987272727272727"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       28        8
            positive       18      914
[1] "TEST accuracy: 0.973140495867769"
[1] "TEST +precision: 0.98068669527897"
[1] "TEST -precision: 0.777777777777778"
[1] "TEST specifity: 0.608695652173913"
[1] "TEST sensitivity: 0.991323210412148"

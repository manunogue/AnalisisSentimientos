[1] "DATASET NAME: Cerveceria_Uni_IR_0"
[1] "TRAIN INSTANCES: 2902"
[1] "TEST INSTANCES: 968"
[1] "......................................................................................."
[1] "ALGORITHM: SVM"
[1] "TIME: 17.4829988479614"
[1] "MODEL SUMMARY: "
Support Vector Machines with Linear Kernel 

2902 samples
 654 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 2321, 2322, 2321, 2322, 2322 
Resampling results:

  ROC        Sens       Spec     
  0.8912747  0.4260215  0.9916364

Tuning parameter 'C' was held constant at a value of 1
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative      2.2      0.8
  positive      3.0     94.0
                            
 Accuracy (average) : 0.9621

[1] "TRAIN accuracy: 0.962095106822881"
[1] "TRAIN +precision: 0.96908315565032"
[1] "TRAIN -precision: 0.738636363636364"
[1] "TRAIN specifity: 0.427631578947368"
[1] "TRAIN sensitivity: 0.991636363636364"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       20       10
            positive       26      912
[1] "TEST accuracy: 0.962809917355372"
[1] "TEST +precision: 0.97228144989339"
[1] "TEST -precision: 0.666666666666667"
[1] "TEST specifity: 0.434782608695652"
[1] "TEST sensitivity: 0.989154013015184"
[1] "......................................................................................."
[1] "ALGORITHM: J48"
[1] "TIME: 4.38145518302917"
[1] "MODEL SUMMARY: "
C4.5-like Trees 

2902 samples
 654 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 2322, 2322, 2322, 2321, 2321 
Resampling results across tuning parameters:

  C      M  ROC        Sens        Spec     
  0.010  1  0.5186399  0.06516129  0.9978182
  0.010  2  0.5089683  0.03935484  0.9981818
  0.010  3  0.5089683  0.03935484  0.9981818
  0.255  1  0.5801267  0.15075269  0.9890909
  0.255  2  0.5842940  0.15075269  0.9894545
  0.255  3  0.6000485  0.18946237  0.9872727
  0.500  1  0.6146045  0.17698925  0.9781818
  0.500  2  0.6269824  0.17053763  0.9767273
  0.500  3  0.6381081  0.21569892  0.9818182

ROC was used to select the optimal model using the largest value.
The final values used for the model were C = 0.5 and M = 3.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative      1.1      1.7
  positive      4.1     93.0
                            
 Accuracy (average) : 0.9418

[1] "TRAIN accuracy: 0.941764300482426"
[1] "TRAIN +precision: 0.957786449095424"
[1] "TRAIN -precision: 0.397590361445783"
[1] "TRAIN specifity: 0.217105263157895"
[1] "TRAIN sensitivity: 0.981818181818182"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       10        8
            positive       36      914
[1] "TEST accuracy: 0.954545454545455"
[1] "TEST +precision: 0.962105263157895"
[1] "TEST -precision: 0.555555555555556"
[1] "TEST specifity: 0.217391304347826"
[1] "TEST sensitivity: 0.991323210412148"
[1] "......................................................................................."
[1] "ALGORITHM: XGBoost"
[1] "TIME: 8.48718289931615"
[1] "MODEL SUMMARY: "
eXtreme Gradient Boosting 

2902 samples
 654 predictor
   2 classes: 'negative', 'positive' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 2322, 2321, 2322, 2322, 2321 
Resampling results across tuning parameters:

  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens       Spec     
  0.3  1          0.6               0.50        50      0.9207462  0.2705376  0.9963636
  0.3  1          0.6               0.50       100      0.9336893  0.3496774  0.9952727
  0.3  1          0.6               0.50       150      0.9416508  0.3888172  0.9941818
  0.3  1          0.6               0.75        50      0.9126704  0.2769892  0.9974545
  0.3  1          0.6               0.75       100      0.9400600  0.3754839  0.9978182
  0.3  1          0.6               0.75       150      0.9417584  0.4090323  0.9967273
  0.3  1          0.6               1.00        50      0.9258651  0.2638710  0.9978182
  0.3  1          0.6               1.00       100      0.9403705  0.3430108  0.9978182
  0.3  1          0.6               1.00       150      0.9405234  0.3888172  0.9970909
  0.3  1          0.8               0.50        50      0.9254143  0.3027957  0.9978182
  0.3  1          0.8               0.50       100      0.9324246  0.3490323  0.9956364
  0.3  1          0.8               0.50       150      0.9320985  0.4021505  0.9938182
  0.3  1          0.8               0.75        50      0.9283105  0.2769892  0.9970909
  0.3  1          0.8               0.75       100      0.9394227  0.3952688  0.9967273
  0.3  1          0.8               0.75       150      0.9432027  0.4281720  0.9952727
  0.3  1          0.8               1.00        50      0.9241752  0.2505376  0.9985455
  0.3  1          0.8               1.00       100      0.9393206  0.3427957  0.9978182
  0.3  1          0.8               1.00       150      0.9407472  0.3821505  0.9967273
  0.3  2          0.6               0.50        50      0.9269973  0.3169892  0.9967273
  0.3  2          0.6               0.50       100      0.9232641  0.4154839  0.9927273
  0.3  2          0.6               0.50       150      0.9287918  0.4286022  0.9912727
  0.3  2          0.6               0.75        50      0.9340264  0.3363441  0.9974545
  0.3  2          0.6               0.75       100      0.9402068  0.3956989  0.9956364
  0.3  2          0.6               0.75       150      0.9356696  0.4415054  0.9934545
  0.3  2          0.6               1.00        50      0.9343763  0.3494624  0.9970909
  0.3  2          0.6               1.00       100      0.9390467  0.3888172  0.9963636
  0.3  2          0.6               1.00       150      0.9394936  0.4150538  0.9963636
  0.3  2          0.8               0.50        50      0.9346196  0.3427957  0.9956364
  0.3  2          0.8               0.50       100      0.9404653  0.4215054  0.9934545
  0.3  2          0.8               0.50       150      0.9424645  0.4419355  0.9920000
  0.3  2          0.8               0.75        50      0.9350109  0.3301075  0.9956364
  0.3  2          0.8               0.75       100      0.9396833  0.4083871  0.9941818
  0.3  2          0.8               0.75       150      0.9379711  0.4410753  0.9916364
  0.3  2          0.8               1.00        50      0.9324567  0.3432258  0.9967273
  0.3  2          0.8               1.00       100      0.9353924  0.3888172  0.9967273
  0.3  2          0.8               1.00       150      0.9350151  0.4152688  0.9949091
  0.3  3          0.6               0.50        50      0.9223973  0.3621505  0.9952727
  0.3  3          0.6               0.50       100      0.9246088  0.4083871  0.9923636
  0.3  3          0.6               0.50       150      0.9197337  0.4277419  0.9916364
  0.3  3          0.6               0.75        50      0.9374770  0.3823656  0.9949091
  0.3  3          0.6               0.75       100      0.9416989  0.4346237  0.9945455
  0.3  3          0.6               0.75       150      0.9383703  0.4544086  0.9934545
  0.3  3          0.6               1.00        50      0.9372375  0.3367742  0.9956364
  0.3  3          0.6               1.00       100      0.9411226  0.3823656  0.9952727
  0.3  3          0.6               1.00       150      0.9392293  0.4023656  0.9945455
  0.3  3          0.8               0.50        50      0.9222989  0.4215054  0.9927273
  0.3  3          0.8               0.50       100      0.9180258  0.4281720  0.9920000
  0.3  3          0.8               0.50       150      0.9157822  0.4610753  0.9905455
  0.3  3          0.8               0.75        50      0.9390944  0.4217204  0.9945455
  0.3  3          0.8               0.75       100      0.9398260  0.4610753  0.9927273
  0.3  3          0.8               0.75       150      0.9328477  0.4610753  0.9930909
  0.3  3          0.8               1.00        50      0.9319990  0.3494624  0.9963636
  0.3  3          0.8               1.00       100      0.9377380  0.4286022  0.9949091
  0.3  3          0.8               1.00       150      0.9359707  0.4481720  0.9941818
  0.4  1          0.6               0.50        50      0.9195509  0.2972043  0.9970909
  0.4  1          0.6               0.50       100      0.9288536  0.4412903  0.9945455
  0.4  1          0.6               0.50       150      0.9340825  0.4417204  0.9927273
  0.4  1          0.6               0.75        50      0.9315728  0.3294624  0.9974545
  0.4  1          0.6               0.75       100      0.9416305  0.4081720  0.9956364
  0.4  1          0.6               0.75       150      0.9469357  0.4219355  0.9945455
  0.4  1          0.6               1.00        50      0.9270297  0.2838710  0.9978182
  0.4  1          0.6               1.00       100      0.9410297  0.3823656  0.9970909
  0.4  1          0.6               1.00       150      0.9408825  0.4021505  0.9960000
  0.4  1          0.8               0.50        50      0.9259408  0.3494624  0.9970909
  0.4  1          0.8               0.50       100      0.9363922  0.3892473  0.9941818
  0.4  1          0.8               0.50       150      0.9449775  0.4483871  0.9923636
  0.4  1          0.8               0.75        50      0.9322757  0.3034409  0.9978182
  0.4  1          0.8               0.75       100      0.9381222  0.3888172  0.9952727
  0.4  1          0.8               0.75       150      0.9360649  0.4283871  0.9930909
  0.4  1          0.8               1.00        50      0.9321349  0.2836559  0.9974545
  0.4  1          0.8               1.00       100      0.9410976  0.3761290  0.9970909
  0.4  1          0.8               1.00       150      0.9414530  0.4088172  0.9967273
  0.4  2          0.6               0.50        50      0.9231069  0.4090323  0.9934545
  0.4  2          0.6               0.50       100      0.9233799  0.4486022  0.9916364
  0.4  2          0.6               0.50       150      0.9221521  0.4488172  0.9890909
  0.4  2          0.6               0.75        50      0.9344815  0.4023656  0.9941818
  0.4  2          0.6               0.75       100      0.9361638  0.4548387  0.9934545
  0.4  2          0.6               0.75       150      0.9344070  0.4873118  0.9938182
  0.4  2          0.6               1.00        50      0.9370332  0.3761290  0.9952727
  0.4  2          0.6               1.00       100      0.9380123  0.4417204  0.9938182
  0.4  2          0.6               1.00       150      0.9398643  0.4415054  0.9934545
  0.4  2          0.8               0.50        50      0.9173302  0.4273118  0.9945455
  0.4  2          0.8               0.50       100      0.9201544  0.4217204  0.9916364
  0.4  2          0.8               0.50       150      0.9152293  0.4348387  0.9912727
  0.4  2          0.8               0.75        50      0.9275865  0.4025806  0.9949091
  0.4  2          0.8               0.75       100      0.9315316  0.4612903  0.9934545
  0.4  2          0.8               0.75       150      0.9370428  0.4548387  0.9916364
  0.4  2          0.8               1.00        50      0.9331799  0.3761290  0.9963636
  0.4  2          0.8               1.00       100      0.9324661  0.4156989  0.9949091
  0.4  2          0.8               1.00       150      0.9344461  0.4288172  0.9941818
  0.4  3          0.6               0.50        50      0.9294278  0.4612903  0.9952727
  0.4  3          0.6               0.50       100      0.9240837  0.4217204  0.9923636
  0.4  3          0.6               0.50       150      0.9139543  0.3886022  0.9901818
  0.4  3          0.6               0.75        50      0.9256739  0.4023656  0.9927273
  0.4  3          0.6               0.75       100      0.9288418  0.4481720  0.9912727
  0.4  3          0.6               0.75       150      0.9248414  0.4617204  0.9912727
  0.4  3          0.6               1.00        50      0.9251388  0.3632258  0.9963636
  0.4  3          0.6               1.00       100      0.9317130  0.4283871  0.9952727
  0.4  3          0.6               1.00       150      0.9326158  0.4548387  0.9927273
  0.4  3          0.8               0.50        50      0.9307750  0.4417204  0.9930909
  0.4  3          0.8               0.50       100      0.9187413  0.4350538  0.9894545
  0.4  3          0.8               0.50       150      0.9154588  0.4415054  0.9869091
  0.4  3          0.8               0.75        50      0.9265212  0.4346237  0.9938182
  0.4  3          0.8               0.75       100      0.9289513  0.4673118  0.9930909
  0.4  3          0.8               0.75       150      0.9197685  0.4741935  0.9905455
  0.4  3          0.8               1.00        50      0.9300782  0.3627957  0.9952727
  0.4  3          0.8               1.00       100      0.9332461  0.4021505  0.9952727
  0.4  3          0.8               1.00       150      0.9324762  0.4221505  0.9930909

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'min_child_weight' was held constant at a value of 1
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 150, max_depth = 1, eta = 0.4, gamma = 0, colsample_bytree = 0.6, min_child_weight = 1 and subsample = 0.75.
[1] "CONFUSION MATRIX: "
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction negative positive
  negative      2.2      0.5
  positive      3.0     94.2
                            
 Accuracy (average) : 0.9645

[1] "TRAIN accuracy: 0.964507236388698"
[1] "TRAIN +precision: 0.968827488487425"
[1] "TRAIN -precision: 0.810126582278481"
[1] "TRAIN specifity: 0.421052631578947"
[1] "TRAIN sensitivity: 0.994545454545455"
[1] "***************************************************************************************"
[1] "***************************************** TEST ****************************************"
[1] "***************************************************************************************"
[1] "TEST ConfMatrix : "
                    label
sentiment_prediction negative positive
            negative       19        2
            positive       27      920
[1] "TEST accuracy: 0.97004132231405"
[1] "TEST +precision: 0.971488912354805"
[1] "TEST -precision: 0.904761904761905"
[1] "TEST specifity: 0.41304347826087"
[1] "TEST sensitivity: 0.997830802603037"
